{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A-oAodTQhTEW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VaU5YBU9dITP"
   },
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def draw_sequence(length, digit_low, digit_high, counter_low, counter_high):\n",
    "    digits, counter, important_digits = [], [], []\n",
    "\n",
    "    while len(digits) < length:\n",
    "        max_count = np.random.randint(counter_low + 1, counter_high + 1)\n",
    "        counter_i = np.arange(max_count, counter_low - 1, -1)\n",
    "        counter.extend(counter_i)\n",
    "\n",
    "        digits_i = np.random.randint(digit_low, digit_high + 1, len(counter_i))\n",
    "        if len(digits_i) + len(digits) <= length:\n",
    "            important_digits.append(digits_i[-1])\n",
    "        digits.extend(digits_i)\n",
    "\n",
    "    digits = digits[:length]\n",
    "    counter = counter[:length]\n",
    "    label = sum(important_digits)\n",
    "\n",
    "    return digits, counter, label\n",
    "\n",
    "sequence_length = 10\n",
    "digit_low = 0\n",
    "digit_high = 2\n",
    "counter_low = 0\n",
    "counter_high = 2\n",
    "\n",
    "draw_sequence(sequence_length, digit_low, digit_high, counter_low, counter_high)\n",
    "train_size = 50000\n",
    "test_size = 10000\n",
    "\n",
    "def generate_dataset(size):\n",
    "    digits_list = []\n",
    "    counter_list = []\n",
    "    labels = []\n",
    "    for _ in range(size):\n",
    "        digits, counter, label = draw_sequence(sequence_length, digit_low, digit_high, counter_low, counter_high)\n",
    "        digits_list.append(digits)\n",
    "        counter_list.append(counter)\n",
    "        labels.append(label)\n",
    "    return np.array(digits_list), np.array(counter_list), np.array(labels)\n",
    "\n",
    "X_train_digits, X_train_counter, y_train = generate_dataset(train_size)\n",
    "X_test_digits, X_test_counter, y_test = generate_dataset(test_size)\n",
    "\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_digits.npy', X_train_digits)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_counter.npy', X_train_counter)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_labels.npy', y_train)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_digits.npy', X_test_digits)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_counter.npy', X_test_counter)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_digits.npy')\n",
    "X_train_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_counter.npy')\n",
    "y_train = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_labels.npy')\n",
    "X_test_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_digits.npy')\n",
    "X_test_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_counter.npy')\n",
    "y_test = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(np.max(y_train) + 1)[y_train]\n",
    "y_test = np.eye(np.max(y_test) + 1)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train_digits, X_train_counter], axis=1)\n",
    "X_test = np.concatenate([X_test_digits, X_test_counter], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb \n",
    "masksper = 512\n",
    "d = X_train.shape[1]\n",
    "X_class = np.concatenate([X_train]*masksper, 0)\n",
    "Y_class = np.concatenate([y_train]*masksper, 0)\n",
    "B = np.concatenate(\n",
    "[np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class.shape[0])],\n",
    "1)\n",
    "B = np.float32(B.T)\n",
    "X_class = np.concatenate((X_class*B, B), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "from xgboost import XGBClassifier\n",
    "est = XGBClassifier(n_estimators=256, device='gpu')\n",
    "est.fit(X_class, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to path\n",
    "path = '/work/users/d/d/ddinh/aaco/models/'\n",
    "est.save_model(path + 'synthetic_raw.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bval = np.concatenate(\n",
    "  [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(8)], 1, keepdims=True) for _ in range(X_test.shape[0])],\n",
    "  1)\n",
    "Bval = np.float32(Bval.T)\n",
    "Xvalmasked = np.concatenate((X_test*Bval, Bval), 1)\n",
    "\n",
    "val_preds = est.predict_proba(Xvalmasked)\n",
    "\n",
    "print(np.mean(np.round(val_preds)==y_test))\n",
    "\n",
    "# mean number of masks Bval\n",
    "print(np.mean(np.sum(Bval, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_digits.shape = (50000, 10)\n",
      "X_train_counter.shape = (50000, 10)\n",
      "y_train.shape = (50000,)\n",
      "X_test_digits.shape = (10000, 10)\n",
      "X_test_counter.shape = (10000, 10)\n",
      "y_test.shape = (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load training data\n",
    "X_train_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_digits.npy')\n",
    "X_train_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_counter.npy')\n",
    "y_train = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_labels.npy')\n",
    "\n",
    "# Load test data\n",
    "X_test_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_digits.npy')\n",
    "X_test_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_counter.npy')\n",
    "y_test = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_labels.npy')\n",
    "\n",
    "print(\"X_train_digits.shape =\", X_train_digits.shape)     # (N, 10)\n",
    "print(\"X_train_counter.shape =\", X_train_counter.shape)   # (N, 10)\n",
    "print(\"y_train.shape =\", y_train.shape)                   # (N,)\n",
    "\n",
    "print(\"X_test_digits.shape =\", X_test_digits.shape)       # (N, 10)\n",
    "print(\"X_test_counter.shape =\", X_test_counter.shape)     # (N, 10)\n",
    "print(\"y_test.shape =\", y_test.shape)                     # (N,)\n",
    "\n",
    "N_train = X_train_digits.shape[0]  # Number of training samples\n",
    "N_test = X_test_digits.shape[0]    # Number of test samples\n",
    "seq_length = X_train_digits.shape[1]  # Should be 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mnist_images(X_digits, digit2images):\n",
    "    N = X_digits.shape[0]\n",
    "    seq_length = X_digits.shape[1]\n",
    "    \n",
    "    X_images = np.zeros((N, seq_length, 28, 28), dtype=np.float32)\n",
    "    \n",
    "    for i in range(N):\n",
    "        for t in range(seq_length):\n",
    "            digit = X_digits[i, t]\n",
    "            img_tensor = random.choice(digit2images[digit])\n",
    "            img_np = img_tensor.squeeze().numpy()  \n",
    "            X_images[i, t] = img_np\n",
    "    \n",
    "    return X_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "mnist_train = datasets.MNIST(\n",
    "    root='./mnist_data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "digit2images = {d: [] for d in range(10)}\n",
    "\n",
    "for i in range(len(mnist_train)):\n",
    "    img, label = mnist_train[i]      \n",
    "    digit2images[label].append(img)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_digits_images.shape = (50000, 10, 28, 28)\n",
      "X_train_counter_images.shape = (50000, 10, 28, 28)\n",
      "X_test_digits_images.shape = (10000, 10, 28, 28)\n",
      "X_test_counter_images.shape = (10000, 10, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train_digits_images = convert_to_mnist_images(X_train_digits, digit2images)\n",
    "X_train_counter_images = convert_to_mnist_images(X_train_counter, digit2images)\n",
    "\n",
    "print(\"X_train_digits_images.shape =\", X_train_digits_images.shape)   # (N_train, 10, 28, 28)\n",
    "print(\"X_train_counter_images.shape =\", X_train_counter_images.shape)\n",
    "\n",
    "X_test_digits_images = convert_to_mnist_images(X_test_digits, digit2images)\n",
    "X_test_counter_images = convert_to_mnist_images(X_test_counter, digit2images)\n",
    "\n",
    "print(\"X_test_digits_images.shape =\", X_test_digits_images.shape)     # (N_test, 10, 28, 28)\n",
    "print(\"X_test_counter_images.shape =\", X_test_counter_images.shape)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/train_digits_images.npy', X_train_digits_images)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/train_counter_images.npy', X_train_counter_images)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/train_labels.npy', y_train)\n",
    "\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/test_digits_images.npy', X_test_digits_images)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/test_counter_images.npy', X_test_counter_images)\n",
    "np.save('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/test_labels.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_digits_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_time_to_channels(X):\n",
    "    \"\"\"\n",
    "    X: shape (N, T, H, W) -> (N, H, W, T)\n",
    "    \"\"\"\n",
    "    X = X.transpose((0, 2, 3, 1))  \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class RandomMask(Layer):\n",
    "    def __init__(self):\n",
    "        super(RandomMask, self).__init__()\n",
    "        self.dim = 10  # Number of time steps (channels)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Get the dynamic batch size\n",
    "        batch_size = tf.shape(inputs)[0]  # Dynamically get the batch size\n",
    "\n",
    "        # Generate a random slice index for each sample in the batch\n",
    "        slice_indices = tf.random.uniform(\n",
    "            shape=(batch_size,), minval=1, maxval=self.dim + 1, dtype=tf.int32\n",
    "        )\n",
    "\n",
    "        # Create a batch of random permutation matrices (one for each sample)\n",
    "        eye = tf.eye(self.dim)  # Identity matrix (dim x dim)\n",
    "        permuted_eyes = tf.random.shuffle(tf.tile(eye[None, :, :], [batch_size, 1, 1]))  # (batch_size, dim, dim)\n",
    "\n",
    "        # Create the masks by summing over the random slices\n",
    "        masks = tf.map_fn(\n",
    "            lambda x: tf.reduce_sum(x[0][:, :x[1]], axis=1, keepdims=True),\n",
    "            (permuted_eyes, slice_indices),\n",
    "            fn_output_signature=tf.float32\n",
    "        )  # Shape: (batch_size, dim, 1)\n",
    "\n",
    "        # Reshape and broadcast masks to match the input shape\n",
    "        masks = tf.transpose(masks, perm=[0, 2, 1])  # Shape: (batch_size, 1, 1, dim)\n",
    "        masks = tf.expand_dims(masks, axis=1)  # Shape: (batch_size, 1, 1, dim)\n",
    "        masks = tf.broadcast_to(masks, tf.shape(inputs))  # Shape: (batch_size, height, width, dim)\n",
    "\n",
    "        # Apply the mask to the inputs\n",
    "        return inputs * masks\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "def resnet_block(x, filters, kernel_size=3, strides=1):\n",
    "    \"\"\"\n",
    "    A simple ResNet block with:\n",
    "    Conv -> BN -> ReLU -> Conv -> BN -> Add (skip) -> ReLU\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if strides != 1:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=strides, padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_resnet_branch(input_shape=(28,28,10), mask_rate=0.3):\n",
    "    \"\"\"\n",
    "    Builds a small ResNet-like model that accepts input of shape (H=28, W=28, channels=10).\n",
    "    Returns: (input_tensor, output_tensor)\n",
    "    \"\"\"\n",
    "    input_tensor = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Optionally mask the input at training time\n",
    "    x = RandomMask()(input_tensor)\n",
    "\n",
    "    # Initial projection\n",
    "    x = layers.Conv2D(32, 3, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # ResNet blocks\n",
    "    x = resnet_block(x, filters=32, kernel_size=3, strides=1)  # block 1\n",
    "    x = resnet_block(x, filters=64, kernel_size=3, strides=2)  # block 2, downsample\n",
    "    x = resnet_block(x, filters=64, kernel_size=3, strides=1)  # block 3\n",
    "    x = resnet_block(x, filters=128, kernel_size=3, strides=2) # block 4, downsample\n",
    "\n",
    "    # Flatten\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # or Flatten() + Dense, etc.\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    return input_tensor, x\n",
    "\n",
    "def build_multimodal_resnet(input_shape=(28,28,10),\n",
    "                            mask_rate=0.3,\n",
    "                            num_classes=np.max(y_train)+1):\n",
    "    \"\"\"\n",
    "    - input_shape: e.g. (28,28,10)\n",
    "    - mask_rate: probability for random masking\n",
    "    - regression: if True, final Dense(1, 'linear')\n",
    "                  else classification with Dense(num_classes, 'softmax').\n",
    "\n",
    "    Returns: a compiled Keras Model\n",
    "    \"\"\"\n",
    "    # Digits branch\n",
    "    digits_input, digits_features = build_resnet_branch(input_shape, mask_rate)\n",
    "\n",
    "    # Counter branch\n",
    "    counter_input, counter_features = build_resnet_branch(input_shape, mask_rate)\n",
    "\n",
    "    # Combine\n",
    "    combined = layers.Concatenate()([digits_features, counter_features])\n",
    "    x = layers.Dense(128, activation='relu')(combined)\n",
    "\n",
    "\n",
    "    # Classification\n",
    "    if num_classes is None:\n",
    "        raise ValueError(\"Must specify num_classes for classification\")\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=[digits_input, counter_input], outputs=output)\n",
    "    model.compile(optimizer='adam', \n",
    "                    loss='sparse_categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train digits shape: (40000, 28, 28, 10)\n",
      "Train counter shape: (40000, 28, 28, 10)\n",
      "Val digits shape: (10000, 28, 28, 10)\n",
      "Val counter shape: (10000, 28, 28, 10)\n",
      "y_train_split shape: (40000,)\n",
      "y_val_split shape: (10000,)\n",
      "Epoch 1/30\n",
      "\u001b[1m  1/157\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13:27\u001b[0m 28s/step - accuracy: 0.0234 - loss: 2.6393"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train_digits_raw = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/train_digits_images.npy')\n",
    "X_train_counter_raw = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/train_counter_images.npy')\n",
    "y_train = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/train_labels.npy')\n",
    "\n",
    "X_test_digits_raw  = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/test_digits_images.npy')\n",
    "X_test_counter_raw = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/test_counter_images.npy')\n",
    "y_test = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_images/test_labels.npy')\n",
    "\n",
    "X_train_digits  = reshape_time_to_channels(X_train_digits_raw)    \n",
    "X_train_counter = reshape_time_to_channels(X_train_counter_raw)\n",
    "X_test_digits   = reshape_time_to_channels(X_test_digits_raw)\n",
    "X_test_counter  = reshape_time_to_channels(X_test_counter_raw)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_digits, X_val_digits, X_train_counter, X_val_counter, y_train_split, y_val_split = \\\n",
    "    train_test_split(X_train_digits, X_train_counter, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train digits shape:\", X_train_digits.shape)       # ~80% of original\n",
    "print(\"Train counter shape:\", X_train_counter.shape)\n",
    "print(\"Val digits shape:\", X_val_digits.shape)           # ~20% of original\n",
    "print(\"Val counter shape:\", X_val_counter.shape)\n",
    "print(\"y_train_split shape:\", y_train_split.shape)\n",
    "print(\"y_val_split shape:\", y_val_split.shape)\n",
    "\n",
    "\n",
    "model = build_multimodal_resnet(\n",
    "    input_shape=(28,28,10),\n",
    "    mask_rate=0.3,        # random masking rate\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='/work/users/d/d/ddinh/aaco/models/synthetic_images.keras',\n",
    "    monitor='val_loss',          # Monitor validation loss\n",
    "    mode='min',                  # Minimize the monitored value\n",
    "    save_best_only=True,         # Save only the best model\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    x=[X_train_digits, X_train_counter],\n",
    "    y=y_train_split,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_data=([X_val_digits, X_val_counter], y_val_split),\n",
    "    callbacks=[checkpoint_cb],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating on Test set:\")\n",
    "test_loss = model.evaluate([X_test_digits, X_test_counter], y_test, verbose=1)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
