{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('/work/users/d/d/ddinh/aaco/src')\n",
    "from load_dataset import load_adni_data\n",
    "from cvar_sensing.utils import prepare_time_series, batch_interp_nd\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    ds = load_adni_data(file_path=file_path)\n",
    "    x = ds.x\n",
    "    y = ds.y\n",
    "    mask_nan = np.isnan(x)\n",
    "    x[mask_nan] = 0\n",
    "    \n",
    "    mask_nan_y = np.isnan(y)\n",
    "    y[mask_nan_y] = 0\n",
    "    return x, y\n",
    "\n",
    "train_x, train_y = load_data(\"/work/users/d/d/ddinh/aaco/input_data/train_data.npz\")\n",
    "val_x, val_y = load_data(\"/work/users/d/d/ddinh/aaco/input_data/val_data.npz\")\n",
    "test_x, test_y = load_data(\"/work/users/d/d/ddinh/aaco/input_data/test_data.npz\")\n",
    "\n",
    "num_ts = train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fill_imputation(x, y):\n",
    "    N, T, M = x.shape\n",
    "    _, _, C = y.shape\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Forward fill X on a per-modality basis\n",
    "        for t in range(1, T):\n",
    "            for m in range(M):\n",
    "                if x[i, t, m] == 0:\n",
    "                    x[i, t, m] = x[i, t-1, m]\n",
    "\n",
    "        # Forward fill Y if all classes at time t are zero\n",
    "        for t in range(1, T):\n",
    "            if np.all(y[i, t] == 0):\n",
    "                y[i, t] = y[i, t-1]\n",
    "                \n",
    "    return x, y\n",
    "\n",
    "train_x, train_y = forward_fill_imputation(train_x, train_y)\n",
    "val_x, val_y = forward_fill_imputation(val_x, val_y)\n",
    "test_x, test_y = forward_fill_imputation(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.transpose(train_x, (0, 2, 1)).reshape(-1, train_x.shape[1] * train_x.shape[2])\n",
    "val_x = np.transpose(val_x, (0, 2, 1)).reshape(-1, val_x.shape[1] * val_x.shape[2])\n",
    "test_x = np.transpose(test_x, (0, 2, 1)).reshape(-1, test_x.shape[1] * test_x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.models.load_model('/work/users/d/d/ddinh/aaco/models/mlp.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_base_embedding_model(\n",
    "    input_shape=(None,),  \n",
    "    embedding_dim=32\n",
    "):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Dense(32)(inputs)\n",
    "    x = layers.Dense(32)(x)\n",
    "    x = layers.Dense(32)(x)\n",
    "    \n",
    "    # Embedding head\n",
    "    embedding = layers.Dense(embedding_dim, activation=None)(x)\n",
    "    \n",
    "    base_model = keras.Model(inputs=inputs, outputs=embedding, name=\"BaseEmbeddingModel\")\n",
    "    return base_model\n",
    "\n",
    "\n",
    "def create_siamese_model(num_features, embedding_dim=32):\n",
    "    base_model = create_base_embedding_model(\n",
    "        input_shape=(num_features,),\n",
    "        embedding_dim=embedding_dim\n",
    "    )\n",
    "\n",
    "    input_a = keras.Input(shape=(num_features,), name='input_a')\n",
    "    input_b = keras.Input(shape=(num_features,), name='input_b')\n",
    "    \n",
    "    emb_a = base_model(input_a)\n",
    "    emb_b = base_model(input_b)\n",
    "    \n",
    "    siamese_model = keras.Model(inputs=[input_a, input_b],\n",
    "                                outputs=[emb_a, emb_b],\n",
    "                                name=\"SiameseModel\")\n",
    "    return siamese_model\n",
    "\n",
    "def x_masked(x, d=48, num_timestamps=12, num_modalities=4):\n",
    "    masks = np.concatenate(\n",
    "        [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(int(d*(3/4)))], 1, keepdims=True) for _ in range(x.shape[0])], 1\n",
    "        )\n",
    "    masks = np.float32(masks.T)\n",
    "    \n",
    "    # ensure there is no 1 at the end of time step \n",
    "    for i in range(num_modalities):\n",
    "        masks[:, num_timestamps * i + num_timestamps - 1] = 0\n",
    "        \n",
    "    masks_zero = np.sum(masks, axis=1) == 0\n",
    "    \n",
    "    \n",
    "    while np.sum(masks_zero) > 0:\n",
    "        # masks[masks_zero] = np.float32(np.concatenate(\n",
    "        #     [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(np.sum(masks_zero))], 1\n",
    "        #     )).T\n",
    "        masks[masks_zero] = np.float32(np.concatenate(\n",
    "            [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(int(d*(3/4)))], 1, keepdims=True) for _ in range(np.sum(masks_zero))], 1\n",
    "            )).T\n",
    "        for i in range(num_modalities):\n",
    "            masks[:, num_timestamps * i + num_timestamps - 1] = 0\n",
    "            \n",
    "        masks_zero = np.sum(masks, axis=1) == 0\n",
    "        \n",
    "    return np.copy(x), tf.cast(masks, tf.float32)\n",
    "\n",
    "def compute_accumulated_loss(y_pred, y_true, loss_function, timesteps=12):\n",
    "    mask = (y_true.sum(dim=-1) != 0)\n",
    "    \n",
    "    per_step_loss = []\n",
    "    for t in range(timesteps):\n",
    "        pred_t = y_pred[:, t, :] \n",
    "        target_t = y_true[:, t, :]      \n",
    "        loss_t = loss_function(pred_t, target_t)\n",
    "        per_step_loss.append(loss_t)\n",
    "\n",
    "    per_step_loss = torch.stack(per_step_loss, dim=1)\n",
    "    \n",
    "    if per_step_loss.dim() == 3:\n",
    "        per_step_loss = per_step_loss.mean(dim=-1) \n",
    "    per_step_loss = per_step_loss * mask  \n",
    "\n",
    "    valid_counts = mask.sum(dim=-1).clamp(min=1) \n",
    "    accumulated_loss = per_step_loss.sum(dim=-1) / valid_counts\n",
    "\n",
    "    return accumulated_loss\n",
    "\n",
    "def compute_loss_timestep(y_true, x_data, mask, classifier, acquisition_cost, loss_function, num_timestamps=12, num_modalities=4):\n",
    "    y_pred = np.zeros(y_true.shape)\n",
    "    for timestamp in range(num_timestamps):\n",
    "        x_input = np.zeros(x_data.shape)\n",
    "        mask_input = np.zeros(mask.shape)\n",
    "        \n",
    "        for modality_index in range(num_modalities):\n",
    "            x_input[:, modality_index * num_timestamps: modality_index * num_timestamps + timestamp + 1] = np.copy(\n",
    "                x_data[:, modality_index * num_timestamps: modality_index * num_timestamps + timestamp + 1])\n",
    "            mask_input[:, modality_index * num_timestamps: modality_index * num_timestamps + timestamp + 1] = np.copy(\n",
    "                mask[:, modality_index * num_timestamps: modality_index * num_timestamps + timestamp + 1])\n",
    "\n",
    "        timestamp_rep = np.repeat(timestamp, x_input.shape[0]).reshape(-1, 1)\n",
    "        pred = classifier.predict(np.concatenate([x_input * mask_input, timestamp_rep], axis=-1), verbose=0)\n",
    "        y_pred[:, timestamp, :] = pred\n",
    "    y_pred = torch.Tensor(y_pred)\n",
    "    if isinstance(y_true, tf.Tensor):\n",
    "        y_true = torch.Tensor(y_true.numpy())\n",
    "    else:\n",
    "        y_true = torch.Tensor(y_true)\n",
    "    total_cost = torch.zeros(mask.shape[0])\n",
    "    for modality in range(num_modalities):\n",
    "        modality_cost = 1 if modality in [0, 1] else 0.5\n",
    "        total_cost += mask[:, modality * num_timestamps: (modality + 1) * num_timestamps].sum(1) * acquisition_cost * modality_cost\n",
    "    total_loss = compute_accumulated_loss(y_pred, y_true, loss_function) + total_cost\n",
    "    return total_loss\n",
    "\n",
    "def get_potential_features(x, y, classifier, prev_masks, acquisition_cost, d=48, num_masks=1500, topk=5, num_timestamps=12, num_modalities=4):\n",
    "    new_masks = np.concatenate(\n",
    "        [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(num_masks)], 1\n",
    "        )\n",
    "    new_masks = np.float32(new_masks.T)\n",
    "\n",
    "    # repeat for parallelization\n",
    "    x_rep = np.repeat(x, num_masks, axis=0)\n",
    "    y_rep = np.repeat(y, num_masks, axis=0)\n",
    "    new_masks = np.concatenate([new_masks for _ in range(x.shape[0])], 0)\n",
    "    prev_masks_rep = np.repeat(prev_masks, num_masks, axis=0)\n",
    "\n",
    "    # combine previous masks with new masks\n",
    "    N = x_rep.shape[0]\n",
    "    segments_previous = prev_masks_rep.reshape(N, num_modalities, num_timestamps)\n",
    "    segments_after = new_masks.reshape(N, num_modalities, num_timestamps)\n",
    "    last_indices = np.where(segments_previous == 1, np.arange(num_timestamps), -1)\n",
    "    last_indices_per_segment = np.max(last_indices, axis=2)  \n",
    "    final_last_index = np.max(last_indices_per_segment, axis=1)  \n",
    "    mask = np.arange(num_timestamps)[None, None, :] > final_last_index[:, None, None]\n",
    "    final_segments = np.where(mask, segments_after, segments_previous)\n",
    "    final = final_segments.reshape(N, num_timestamps * num_modalities)\n",
    "    \n",
    "    # compute loss for current and future masks\n",
    "    current_loss = compute_loss_timestep(y, x, prev_masks, classifier, acquisition_cost, nn.CrossEntropyLoss(reduction='none'))\n",
    "    future_loss = compute_loss_timestep(y_rep, x_rep, final, classifier, acquisition_cost, nn.CrossEntropyLoss(reduction='none'))\n",
    "    \n",
    "    # get the top k min loss\n",
    "    top_k_sets = []\n",
    "    distributions = []\n",
    "    terminations = []\n",
    "    percentage_unique = []\n",
    "    for i in range(x.shape[0]):\n",
    "        min_current_loss = current_loss[i]\n",
    "        single_sample_future_loss = np.copy(future_loss[i * num_masks: (i + 1) * num_masks])\n",
    "        single_sample_future_set = np.copy(final[i * num_masks: (i + 1) * num_masks])\n",
    "        \n",
    "        top_k_loss = np.argsort(single_sample_future_loss)\n",
    "        min_k_future_loss = single_sample_future_loss[top_k_loss]\n",
    "        min_k_subsets = single_sample_future_set[top_k_loss]\n",
    "        \n",
    "        unique_min_k_subsets, unique_indices = np.unique(min_k_subsets, axis=0, return_index=True)\n",
    "        unique_min_k_future_loss = min_k_future_loss[unique_indices]\n",
    "        \n",
    "        sorted_unique_indices = np.argsort(unique_min_k_future_loss)\n",
    "        unique_min_k_future_loss = unique_min_k_future_loss[sorted_unique_indices]\n",
    "        unique_min_k_subsets = unique_min_k_subsets[sorted_unique_indices]\n",
    "        \n",
    "        if unique_min_k_subsets.shape[0] <= 1:\n",
    "            percentage_unique.append(0)\n",
    "            topk_min = topk\n",
    "            if np.equal(unique_min_k_subsets[0], prev_masks[i]).all():\n",
    "                termination = np.ones(topk)\n",
    "                unique_min_k_subsets = np.repeat(prev_masks[i][None, :], topk, axis=0)\n",
    "                subset_losses = np.ones(topk) * min_current_loss\n",
    "            else:\n",
    "                if unique_min_k_future_loss[0] >= min_current_loss:\n",
    "                    termination = np.ones(topk)\n",
    "                    unique_min_k_subsets = np.repeat(prev_masks[i][None, :], topk, axis=0)\n",
    "                    subset_losses = np.ones(topk) * min_current_loss\n",
    "                else: \n",
    "                    termination = np.zeros(topk)\n",
    "                    unique_min_k_subsets = np.repeat(unique_min_k_subsets[0], topk, axis=0)\n",
    "                    subset_losses = np.repeat(unique_min_k_future_loss[0], topk)\n",
    "        else: \n",
    "            percentage_unique.append(1)\n",
    "            topk_min = min(topk, unique_min_k_subsets.shape[0])\n",
    "            termination = np.zeros(topk_min)\n",
    "            subset_losses = unique_min_k_future_loss[:topk_min]\n",
    "\n",
    "            for j in range(topk_min):\n",
    "                if unique_min_k_future_loss[j] >= min_current_loss:\n",
    "                    termination[j] = 1\n",
    "                    unique_min_k_subsets[j] = prev_masks[i]\n",
    "                    \n",
    "        unique_min_k_subsets = unique_min_k_subsets[:topk_min]\n",
    "        unique_min_k_subsets[:,:d] -= prev_masks[i]\n",
    "        unique_min_k_subsets = np.concatenate([unique_min_k_subsets, termination[:, None]], axis=1) # shape (topk, d+1)\n",
    "        \n",
    "        # turn into probabilities\n",
    "        unique_min_k_subsets = unique_min_k_subsets / np.sum(unique_min_k_subsets, axis=1)[:, None]\n",
    "        subset_losses = subset_losses[:topk_min]\n",
    "        weights = np.exp(-subset_losses[:topk_min])\n",
    "        weights /= np.sum(weights)\n",
    "        distribution = np.sum(unique_min_k_subsets * weights[:, None], axis=0) \n",
    "        \n",
    "        distributions.append(distribution)\n",
    "        terminations.append(termination)\n",
    "\n",
    "    distributions = np.array(distributions) # shape (batch_size, d)\n",
    "    distributions = tf.cast(distributions, tf.float32)\n",
    "    distributions = distributions / (tf.reduce_sum(distributions, axis=1, keepdims=True) + 1e-12) # for numerical stability\n",
    "    # print(\"check sum of distributions: \", np.sum(distributions, axis=1))\n",
    "    \n",
    "    return distributions\n",
    "\n",
    "def compute_neighbors(x, distribution):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    distribution = tf.cast(distribution, tf.float32)\n",
    "    \n",
    "    sorted_indices = []\n",
    "    for i in range(x.shape[0]):\n",
    "        distance = tf.norm(distribution - distribution[i], axis=1) \n",
    "        distance = tf.concat([distance[:i], distance[i+1:]], axis=0) # remove the distance to itself\n",
    "        sorted_index = tf.argsort(distance)\n",
    "        sorted_indices.append(sorted_index)\n",
    "        \n",
    "    return sorted_indices\n",
    "    \n",
    "\n",
    "def get_pairs(x, distribution, k=3):\n",
    "    sorted_indices = compute_neighbors(x, distribution)\n",
    "    pairs = []\n",
    "    for i in range(x.shape[0]):\n",
    "        positive_pairs = sorted_indices[i][:k]\n",
    "        negative_pairs = sorted_indices[i][k:] \n",
    "        used_negative_pairs = []\n",
    "        for j in range(k * 2):\n",
    "            temp_pairs = []\n",
    "            temp_pairs.append(x[i])\n",
    "            temp_pairs.append(x[positive_pairs[j % k]])\n",
    "            random_negative = np.random.choice(negative_pairs)\n",
    "            \n",
    "            while random_negative in used_negative_pairs:\n",
    "                random_negative = np.random.choice(negative_pairs)\n",
    "            temp_pairs.append(x[random_negative])\n",
    "            used_negative_pairs.append(random_negative)\n",
    "            \n",
    "            pairs.append(temp_pairs)\n",
    "            \n",
    "    pairs = np.array(pairs) # shape (batch_size * k * 2, 3, d)\n",
    "    x_anchor, x_positive, x_negative = pairs[:, 0], pairs[:, 1], pairs[:, 2]\n",
    "    x_anchor = tf.concat([x_anchor, x_anchor], axis=0)\n",
    "    x_pair = tf.concat([x_positive, x_negative], axis=0)\n",
    "    target = tf.concat([tf.ones(x_positive.shape[0]), tf.zeros(x_negative.shape[0])], axis=0)\n",
    "    \n",
    "    # shuffle the pairs\n",
    "    indices = tf.range(start=0, limit=x_pair.shape[0], dtype=tf.int32)\n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    x_anchor = tf.gather(x_anchor, shuffled_indices)\n",
    "    x_pair = tf.gather(x_pair, shuffled_indices)\n",
    "    target = tf.gather(target, shuffled_indices)\n",
    "    \n",
    "    return x_anchor, x_pair, target\n",
    "\n",
    "# Contrastive loss function\n",
    "def contrastive_loss(y_true, distances, margin=1.0):\n",
    "    positive_loss = y_true * tf.square(distances)  # Similar pairs\n",
    "    negative_loss = (1 - y_true) * tf.square(tf.maximum(margin - distances, 0))  # Dissimilar pairs\n",
    "    return tf.reduce_mean(0.5 * (positive_loss + negative_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Start Training***\n",
      "Epoch 1/50\n",
      "Training batch 0\n",
      "tf.Tensor(5.2170415, shape=(), dtype=float32)\n",
      "Training batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2551118/1661060476.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  unique_min_k_subsets = unique_min_k_subsets / np.sum(unique_min_k_subsets, axis=1)[:, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(nan, shape=(), dtype=float32)\n",
      "Training batch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macquisition_cost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     56\u001b[0m     step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, x, y, classifier, acquisition_cost, alpha)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(model, optimizer, x, y, classifier, acquisition_cost, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     x, prev_masks \u001b[38;5;241m=\u001b[39m x_masked(x)\n\u001b[0;32m----> 3\u001b[0m     distributions \u001b[38;5;241m=\u001b[39m \u001b[43mget_potential_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_masks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macquisition_cost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     x_anchor, x_pair, target \u001b[38;5;241m=\u001b[39m get_pairs(np\u001b[38;5;241m.\u001b[39mcopy(x) \u001b[38;5;241m*\u001b[39m prev_masks, distributions)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "Cell \u001b[0;32mIn[13], line 140\u001b[0m, in \u001b[0;36mget_potential_features\u001b[0;34m(x, y, classifier, prev_masks, acquisition_cost, d, num_masks, topk, num_timestamps, num_modalities)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# compute loss for current and future masks\u001b[39;00m\n\u001b[1;32m    139\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m compute_loss_timestep(y, x, prev_masks, classifier, acquisition_cost, nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 140\u001b[0m future_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_timestep\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macquisition_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# get the top k min loss\u001b[39;00m\n\u001b[1;32m    143\u001b[0m top_k_sets \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[13], line 97\u001b[0m, in \u001b[0;36mcompute_loss_timestep\u001b[0;34m(y_true, x_data, mask, classifier, acquisition_cost, loss_function, num_timestamps, num_modalities)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modality_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_modalities):\n\u001b[1;32m     95\u001b[0m     x_input[:, modality_index \u001b[38;5;241m*\u001b[39m num_timestamps: modality_index \u001b[38;5;241m*\u001b[39m num_timestamps \u001b[38;5;241m+\u001b[39m timestamp \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(\n\u001b[1;32m     96\u001b[0m         x_data[:, modality_index \u001b[38;5;241m*\u001b[39m num_timestamps: modality_index \u001b[38;5;241m*\u001b[39m num_timestamps \u001b[38;5;241m+\u001b[39m timestamp \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 97\u001b[0m     mask_input[:, modality_index \u001b[38;5;241m*\u001b[39m num_timestamps: modality_index \u001b[38;5;241m*\u001b[39m num_timestamps \u001b[38;5;241m+\u001b[39m timestamp \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodality_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_timestamps\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodality_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_timestamps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m timestamp_rep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(timestamp, x_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mconcatenate([x_input \u001b[38;5;241m*\u001b[39m mask_input, timestamp_rep], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/numpy/lib/function_base.py:869\u001b[0m, in \u001b[0;36m_copy_dispatcher\u001b[0;34m(a, order, subok)\u001b[0m\n\u001b[1;32m    864\u001b[0m         np\u001b[38;5;241m.\u001b[39mcopyto(result, choice, where\u001b[38;5;241m=\u001b[39mcond)\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 869\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy_dispatcher\u001b[39m(a, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[1;32m    873\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_copy_dispatcher)\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(a, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_step(model, optimizer, x, y, classifier, acquisition_cost, alpha=1):\n",
    "    x, prev_masks = x_masked(x)\n",
    "    distributions = get_potential_features(np.copy(x), np.copy(y), classifier, np.copy(prev_masks), acquisition_cost)\n",
    "    \n",
    "    x_anchor, x_pair, target = get_pairs(np.copy(x) * prev_masks, distributions)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        emb1, emb2 = model([x_anchor, x_pair])\n",
    "        distances = tf.norm(emb1 - emb2, axis=1)\n",
    "        loss = contrastive_loss(target, distances)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    print(loss)\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, x, y, classifier, acquisition_cost, alpha=1):\n",
    "    x_val, prev_masks_val = x_masked(x)\n",
    "    distributions_val = get_potential_features(np.copy(x_val), np.copy(y), classifier, np.copy(prev_masks_val), acquisition_cost)\n",
    "    \n",
    "    x_anchor_val, x_pair_val, target_val = get_pairs(np.copy(x_val) * prev_masks_val, distributions_val)\n",
    "    \n",
    "    emb1_val, emb2_val = model([x_anchor_val, x_pair_val])\n",
    "    distances_val = tf.norm(emb1_val - emb2_val, axis=1)\n",
    "    loss_val = contrastive_loss(target_val, distances_val)\n",
    "    print(loss_val)\n",
    "    return loss_val\n",
    "\n",
    "\n",
    "X, X_val, y, y_val = train_x, val_x, train_y, val_y\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "num_features = 48\n",
    "model = create_siamese_model(num_features=num_features, embedding_dim=32)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
    "# todo: cahnga batch size back \n",
    "# get rid of the break statement\n",
    "\n",
    "acquisition_cost = 0.025\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "print(\"***Start Training***\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    step_count = 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_x, batch_y in dataset:\n",
    "        print(f\"Training batch {step_count}\")\n",
    "        loss = train_step(model, optimizer, batch_x, batch_y, classifier, acquisition_cost)\n",
    "        epoch_loss += loss.numpy()\n",
    "        step_count += 1\n",
    "    \n",
    "    train_loss = epoch_loss / step_count\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
    "    val_loss_sum = 0.0\n",
    "    val_steps = 0\n",
    "    for val_x_batch, val_y_batch in val_dataset:\n",
    "        print(f\"Validation batch {val_steps}\")\n",
    "        loss_val = validation_step(model, val_x_batch, val_y_batch, classifier, acquisition_cost)\n",
    "        val_loss_sum += loss_val.numpy()\n",
    "        val_steps += 1\n",
    "        \n",
    "    val_loss = val_loss_sum / val_steps\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save('/work/users/d/d/ddinh/aaco/models/siamese_adni.h5')\n",
    "        model.save('/work/users/d/d/ddinh/aaco/models/siamese_adni.keras')\n",
    "        model.save_weights('/work/users/d/d/ddinh/aaco/models/siamese_adni.weights.h5')\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "print(\"Training Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in weight: kernel\n",
      "NaN detected in weight: bias\n",
      "NaN detected in weight: kernel\n",
      "NaN detected in weight: bias\n",
      "NaN detected in weight: kernel\n",
      "NaN detected in weight: bias\n",
      "NaN detected in weight: kernel\n",
      "NaN detected in weight: bias\n"
     ]
    }
   ],
   "source": [
    "def check_weights_for_nan(model):\n",
    "    \"\"\"\n",
    "    Check all weights of a Keras model for NaN values.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to check.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints which layers or weights have NaN values.\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        for weight in layer.weights:\n",
    "            weight_name = weight.name\n",
    "            if tf.reduce_any(tf.math.is_nan(weight)).numpy():\n",
    "                print(f\"NaN detected in weight: {weight_name}\")\n",
    "            else:\n",
    "                print(f\"Weight {weight_name} is clean (no NaNs).\")\n",
    "# Assuming `model` is your Keras model\n",
    "check_weights_for_nan(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
