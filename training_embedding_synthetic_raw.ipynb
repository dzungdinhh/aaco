{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 21:53:49.450024: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-22 21:53:49.895829: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-22 21:53:50.063144: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-22 21:53:51.194940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-22 21:53:55.587239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('/work/users/d/d/ddinh/aaco/src')\n",
    "from load_dataset import load_adni_data\n",
    "from cvar_sensing.utils import prepare_time_series, batch_interp_nd\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_digits.npy')\n",
    "X_train_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_counter.npy')\n",
    "y_train = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_labels.npy')\n",
    "X_test_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_digits.npy')\n",
    "X_test_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_counter.npy')\n",
    "y_test = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_labels.npy')\n",
    "\n",
    "train_y = np.eye(np.max(y_train) + 1)[y_train]\n",
    "y_test = np.eye(np.max(y_test) + 1)[y_test]\n",
    "\n",
    "train_x = np.concatenate([X_train_digits, X_train_counter], axis=1)\n",
    "X_test = np.concatenate([X_test_digits, X_test_counter], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(n_estimators=256, device='gpu')\n",
    "classifier.load_model('/work/users/d/d/ddinh/aaco/models/synthetic_raw.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: implement progressively acquire features version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.518925518925519\n",
      "Percentage of terminated:  0.5316455696202531\n",
      "Percentage of terminated:  0.5486761710794298\n",
      "Percentage of terminated:  0.554251012145749\n",
      "Percentage of terminated:  0.5012224938875306\n",
      "Percentage of terminated:  0.5449029126213593\n",
      "Percentage of terminated:  0.5253061224489796\n",
      "Percentage of terminated:  0.5739307535641548\n",
      "Percentage of terminated:  0.5293637846655791\n",
      "Percentage of terminated:  0.5304952926729432\n",
      "Percentage of terminated:  0.5155993431855501\n",
      "Percentage of terminated:  0.523673469387755\n",
      "Percentage of terminated:  0.5317073170731708\n",
      "Percentage of terminated:  0.5089942763695829\n",
      "Percentage of terminated:  0.5192385581206966\n",
      "Percentage of terminated:  0.5201301342008947\n",
      "Percentage of terminated:  0.4975470155355683\n",
      "Percentage of terminated:  0.5561866125760649\n",
      "Percentage of terminated:  0.5097402597402597\n",
      "Percentage of terminated:  0.5262301748678324\n",
      "Percentage of terminated:  0.5534335635920358\n",
      "Percentage of terminated:  0.4810126582278481\n",
      "Percentage of terminated:  0.5008130081300813\n",
      "Percentage of terminated:  0.5486365486365486\n",
      "Percentage of terminated:  0.5444535073409462\n",
      "Percentage of terminated:  0.5514018691588785\n",
      "Percentage of terminated:  0.5251215559157212\n",
      "Percentage of terminated:  0.5333876221498371\n",
      "Percentage of terminated:  0.525910931174089\n",
      "Percentage of terminated:  0.531326281529699\n",
      "Percentage of terminated:  0.5231331168831169\n",
      "Percentage of terminated:  0.5134255492270138\n",
      "Percentage of terminated:  0.5760162601626017\n",
      "Percentage of terminated:  0.5589074602527517\n",
      "Percentage of terminated:  0.5589074602527517\n",
      "Percentage of terminated:  0.5419039869812856\n",
      "Percentage of terminated:  0.5514436762911753\n",
      "Percentage of terminated:  0.5079043372517228\n",
      "Percentage of terminated:  0.5424489795918367\n",
      "Percentage of terminated:  0.5231331168831169\n",
      "Percentage of terminated:  0.5302476654486399\n",
      "Percentage of terminated:  0.5054811205846529\n",
      "Percentage of terminated:  0.5093419983753046\n",
      "Percentage of terminated:  0.5532953620829943\n",
      "Percentage of terminated:  0.5534100246507806\n",
      "Percentage of terminated:  0.4927360774818402\n",
      "Percentage of terminated:  0.5266802443991854\n",
      "Percentage of terminated:  0.48659626320064986\n",
      "Percentage of terminated:  0.5314029363784666\n",
      "Percentage of terminated:  0.5054811205846529\n",
      "Percentage of terminated:  0.5266368442456283\n",
      "Percentage of terminated:  0.5138211382113821\n",
      "Percentage of terminated:  0.535119772634998\n",
      "Percentage of terminated:  0.5290768605124034\n",
      "Percentage of terminated:  0.5266368442456283\n",
      "Percentage of terminated:  0.4979707792207792\n",
      "Percentage of terminated:  0.5542810323637853\n",
      "Percentage of terminated:  0.5116279069767442\n",
      "Percentage of terminated:  0.5164433617539586\n",
      "Percentage of terminated:  0.5511618426416632\n",
      "Percentage of terminated:  0.5075602778912954\n",
      "Percentage of terminated:  0.5467860048820179\n",
      "Percentage of terminated:  0.5012145748987854\n",
      "Percentage of terminated:  0.5262088581877286\n",
      "Percentage of terminated:  0.5081037277147488\n",
      "Percentage of terminated:  0.5275173257236038\n",
      "Percentage of terminated:  0.5124948791478902\n",
      "Percentage of terminated:  0.5211382113821138\n",
      "Percentage of terminated:  0.5101461038961039\n",
      "Percentage of terminated:  0.5383360522022839\n",
      "Percentage of terminated:  0.5448247758761207\n",
      "Percentage of terminated:  0.5374233128834356\n",
      "Percentage of terminated:  0.5390243902439025\n",
      "Percentage of terminated:  0.5354523227383863\n",
      "Percentage of terminated:  0.5752248569092395\n",
      "Percentage of terminated:  0.5248979591836734\n",
      "Percentage of terminated:  0.5355257815671944\n",
      "Percentage of terminated:  0.5431492842535788\n",
      "Percentage of terminated:  0.5130718954248366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:35:10.142347: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5093419983753046\n",
      "Percentage of terminated:  0.5586682907023954\n",
      "Percentage of terminated:  0.5257774140752864\n",
      "Percentage of terminated:  0.5395918367346939\n",
      "Percentage of terminated:  0.5314656922452294\n",
      "Percentage of terminated:  0.5143782908059943\n",
      "Percentage of terminated:  0.5091426249492076\n",
      "Percentage of terminated:  0.495928338762215\n",
      "Percentage of terminated:  0.5204918032786885\n",
      "Percentage of terminated:  0.5334148329258354\n",
      "Percentage of terminated:  0.5486365486365486\n",
      "Percentage of terminated:  0.5177333876885446\n",
      "Percentage of terminated:  0.5364859355890746\n",
      "Percentage of terminated:  0.5255681818181818\n",
      "Percentage of terminated:  0.49572649572649574\n",
      "Percentage of terminated:  0.5048740861088545\n",
      "Percentage of terminated:  0.532462229481421\n",
      "Percentage of terminated:  0.5177849636216654\n",
      "Percentage of terminated:  0.530296868645791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:45:38.817477: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.558641975308642\n",
      "Epoch 1, Train Loss: 4.6585, Validation Loss: 3.8610\n",
      "Percentage of terminated:  0.5338193600648036\n",
      "Percentage of terminated:  0.5221995926680244\n",
      "Percentage of terminated:  0.534447615165104\n",
      "Percentage of terminated:  0.4945321992709599\n",
      "Percentage of terminated:  0.5549674267100977\n",
      "Percentage of terminated:  0.5079365079365079\n",
      "Percentage of terminated:  0.5198863636363636\n",
      "Percentage of terminated:  0.49572301425661913\n",
      "Percentage of terminated:  0.5576528518670496\n",
      "Percentage of terminated:  0.5348552792498981\n",
      "Percentage of terminated:  0.5432300163132137\n",
      "Percentage of terminated:  0.5142392188771359\n",
      "Percentage of terminated:  0.5584045584045584\n",
      "Percentage of terminated:  0.5177045177045178\n",
      "Percentage of terminated:  0.46441642944286293\n",
      "Percentage of terminated:  0.5328947368421053\n",
      "Percentage of terminated:  0.4873262469337694\n",
      "Percentage of terminated:  0.5287169042769857\n",
      "Percentage of terminated:  0.5688110749185668\n",
      "Percentage of terminated:  0.5083231831100284\n",
      "Percentage of terminated:  0.5272652726527265\n",
      "Percentage of terminated:  0.5008183306055647\n",
      "Percentage of terminated:  0.513491743858236\n",
      "Percentage of terminated:  0.5312117503059975\n",
      "Percentage of terminated:  0.5495090016366612\n",
      "Percentage of terminated:  0.5259713701431493\n",
      "Percentage of terminated:  0.5498575498575499\n",
      "Percentage of terminated:  0.547911547911548\n",
      "Percentage of terminated:  0.5303957568339454\n",
      "Percentage of terminated:  0.5356849876948319\n",
      "Percentage of terminated:  0.5400895400895401\n",
      "Percentage of terminated:  0.5301302931596091\n",
      "Percentage of terminated:  0.5002035002035002\n",
      "Percentage of terminated:  0.5451573355128729\n",
      "Percentage of terminated:  0.505271695052717\n",
      "Percentage of terminated:  0.5118078175895765\n",
      "Percentage of terminated:  0.5081699346405228\n",
      "Percentage of terminated:  0.565800162469537\n",
      "Percentage of terminated:  0.5489638358390898\n",
      "Percentage of terminated:  0.5738241308793456\n",
      "Percentage of terminated:  0.5344262295081967\n",
      "Percentage of terminated:  0.5095645095645096\n",
      "Percentage of terminated:  0.5290060851926978\n",
      "Percentage of terminated:  0.5251328156926849\n",
      "Percentage of terminated:  0.5180673974827447\n",
      "Percentage of terminated:  0.5030500203334689\n",
      "Percentage of terminated:  0.5135245901639345\n",
      "Percentage of terminated:  0.49695245835026414\n",
      "Percentage of terminated:  0.5356849876948319\n",
      "Percentage of terminated:  0.5314656922452294\n",
      "Percentage of terminated:  0.5327902240325866\n",
      "Percentage of terminated:  0.5432653061224489\n",
      "Percentage of terminated:  0.5179299103504482\n",
      "Percentage of terminated:  0.5285481239804242\n",
      "Percentage of terminated:  0.48441926345609065\n",
      "Percentage of terminated:  0.5529652351738241\n",
      "Percentage of terminated:  0.5080906148867314\n",
      "Percentage of terminated:  0.5551921504497138\n",
      "Percentage of terminated:  0.5252152521525215\n",
      "Percentage of terminated:  0.5392755392755393\n",
      "Percentage of terminated:  0.53173311635476\n",
      "Percentage of terminated:  0.5475704369130258\n",
      "Percentage of terminated:  0.5347137637028014\n",
      "Percentage of terminated:  0.5233798195242002\n",
      "Percentage of terminated:  0.5168767791785278\n",
      "Percentage of terminated:  0.5149651496514965\n",
      "Percentage of terminated:  0.5241013071895425\n",
      "Percentage of terminated:  0.5555555555555556\n",
      "Percentage of terminated:  0.5245098039215687\n",
      "Percentage of terminated:  0.5751231527093597\n",
      "Percentage of terminated:  0.5505709624796085\n",
      "Percentage of terminated:  0.545086938940558\n",
      "Percentage of terminated:  0.5544754961522884\n",
      "Percentage of terminated:  0.5150040551500406\n",
      "Percentage of terminated:  0.5430327868852459\n",
      "Percentage of terminated:  0.5535641547861507\n",
      "Percentage of terminated:  0.5198863636363636\n",
      "Percentage of terminated:  0.5269387755102041\n",
      "Percentage of terminated:  0.5723684210526315\n",
      "Percentage of terminated:  0.549043549043549\n",
      "Percentage of terminated:  0.5705546492659054\n",
      "Percentage of terminated:  0.5328169588259274\n",
      "Percentage of terminated:  0.535583570557137\n",
      "Percentage of terminated:  0.5385242560130452\n",
      "Percentage of terminated:  0.5140758873929009\n",
      "Percentage of terminated:  0.5147474747474747\n",
      "Percentage of terminated:  0.5351594439901881\n",
      "Percentage of terminated:  0.5183374083129584\n",
      "Percentage of terminated:  0.5555555555555556\n",
      "Percentage of terminated:  0.57997557997558\n",
      "Percentage of terminated:  0.530278232405892\n",
      "Percentage of terminated:  0.5261011419249593\n",
      "Percentage of terminated:  0.4924643584521385\n",
      "Percentage of terminated:  0.5358014646053703\n",
      "Percentage of terminated:  0.49494541043267287\n",
      "Percentage of terminated:  0.5710238769728855\n",
      "Percentage of terminated:  0.5258866693844272\n",
      "Percentage of terminated:  0.5222539812168232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 23:36:46.261983: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5874613003095975\n",
      "Epoch 2, Train Loss: 3.6198, Validation Loss: 3.4627\n",
      "Percentage of terminated:  0.5180234912920211\n",
      "Percentage of terminated:  0.5095567303782025\n",
      "Percentage of terminated:  0.5168493706861551\n",
      "Percentage of terminated:  0.5206645056726094\n",
      "Percentage of terminated:  0.5004078303425775\n",
      "Percentage of terminated:  0.5148676171079429\n",
      "Percentage of terminated:  0.5261224489795918\n",
      "Percentage of terminated:  0.487566245413779\n",
      "Percentage of terminated:  0.5466015466015466\n",
      "Percentage of terminated:  0.5388071895424836\n",
      "Percentage of terminated:  0.5051250512505125\n",
      "Percentage of terminated:  0.5127685447912445\n",
      "Percentage of terminated:  0.488834754364596\n",
      "Percentage of terminated:  0.5176901179341196\n",
      "Percentage of terminated:  0.5458996328029376\n",
      "Percentage of terminated:  0.5207317073170732\n",
      "Percentage of terminated:  0.4963592233009709\n",
      "Percentage of terminated:  0.5417175417175417\n",
      "Percentage of terminated:  0.5648261758691207\n",
      "Percentage of terminated:  0.535234215885947\n",
      "Percentage of terminated:  0.49898744430943703\n",
      "Percentage of terminated:  0.5087647778230738\n",
      "Percentage of terminated:  0.5524275805793554\n",
      "Percentage of terminated:  0.526829268292683\n",
      "Percentage of terminated:  0.4720194647201946\n",
      "Percentage of terminated:  0.5367736692401462\n",
      "Percentage of terminated:  0.5194274028629857\n",
      "Percentage of terminated:  0.5247766043866775\n",
      "Percentage of terminated:  0.5085924713584288\n",
      "Percentage of terminated:  0.541853817884851\n",
      "Percentage of terminated:  0.5034566897112648\n",
      "Percentage of terminated:  0.5087221095334685\n",
      "Percentage of terminated:  0.5137651821862348\n",
      "Percentage of terminated:  0.5349122090649244\n",
      "Percentage of terminated:  0.5150284321689683\n",
      "Percentage of terminated:  0.5083571137382796\n",
      "Percentage of terminated:  0.5253682487725041\n",
      "Percentage of terminated:  0.5463875205254516\n",
      "Percentage of terminated:  0.4938875305623472\n",
      "Percentage of terminated:  0.5308692120227457\n",
      "Percentage of terminated:  0.5055305202785744\n",
      "Percentage of terminated:  0.5368248772504092\n",
      "Percentage of terminated:  0.5237130117551683\n",
      "Percentage of terminated:  0.5113913751017087\n",
      "Percentage of terminated:  0.5579119086460033\n",
      "Percentage of terminated:  0.556920556920557\n",
      "Percentage of terminated:  0.5296747967479675\n",
      "Percentage of terminated:  0.5647921760391198\n",
      "Percentage of terminated:  0.5278795278795279\n",
      "Percentage of terminated:  0.4955211726384365\n",
      "Percentage of terminated:  0.5631665299425759\n",
      "Percentage of terminated:  0.5114099429502853\n",
      "Percentage of terminated:  0.5804878048780487\n",
      "Percentage of terminated:  0.5293159609120521\n",
      "Percentage of terminated:  0.48921448921448923\n",
      "Percentage of terminated:  0.525609756097561\n",
      "Percentage of terminated:  0.5547415547415547\n",
      "Percentage of terminated:  0.5038759689922481\n",
      "Percentage of terminated:  0.5102124183006536\n",
      "Percentage of terminated:  0.4963084495488105\n",
      "Percentage of terminated:  0.5318536785861077\n",
      "Percentage of terminated:  0.5622440622440622\n",
      "Percentage of terminated:  0.5412919051512673\n",
      "Percentage of terminated:  0.5261224489795918\n",
      "Percentage of terminated:  0.5008149959250203\n",
      "Percentage of terminated:  0.5046766978446523\n",
      "Percentage of terminated:  0.5417007358953393\n",
      "Percentage of terminated:  0.534629404617254\n",
      "Percentage of terminated:  0.5215097402597403\n",
      "Percentage of terminated:  0.5076923076923077\n",
      "Percentage of terminated:  0.5006139991813344\n",
      "Percentage of terminated:  0.5160113498175922\n",
      "Percentage of terminated:  0.5268112975849365\n",
      "Percentage of terminated:  0.524330900243309\n",
      "Percentage of terminated:  0.526294498381877\n",
      "Percentage of terminated:  0.5519639934533551\n",
      "Percentage of terminated:  0.5690923957481603\n",
      "Percentage of terminated:  0.5484922575387123\n",
      "Percentage of terminated:  0.5233333333333333\n",
      "Percentage of terminated:  0.5241183623834617\n",
      "Percentage of terminated:  0.5257142857142857\n",
      "Percentage of terminated:  0.534826883910387\n",
      "Percentage of terminated:  0.5197717081125153\n",
      "Percentage of terminated:  0.5430436556507547\n",
      "Percentage of terminated:  0.546497337156903\n",
      "Percentage of terminated:  0.49938900203665987\n",
      "Percentage of terminated:  0.4963384865744508\n",
      "Percentage of terminated:  0.5223395613322502\n",
      "Percentage of terminated:  0.5248052480524805\n",
      "Percentage of terminated:  0.5138211382113821\n",
      "Percentage of terminated:  0.5391875256462865\n",
      "Percentage of terminated:  0.5230142566191446\n",
      "Percentage of terminated:  0.5122249388753056\n",
      "Percentage of terminated:  0.5132060138155221\n",
      "Percentage of terminated:  0.5614536545528788\n",
      "Percentage of terminated:  0.5265935850588713\n",
      "Percentage of terminated:  0.5008176614881439\n",
      "Percentage of terminated:  0.46160097521332794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.4973302822273074\n",
      "Epoch 3, Train Loss: 3.3975, Validation Loss: 3.2857\n",
      "Percentage of terminated:  0.5326530612244897\n",
      "Percentage of terminated:  0.5247443762781187\n",
      "Percentage of terminated:  0.5530085959885387\n",
      "Percentage of terminated:  0.5103954341622503\n",
      "Percentage of terminated:  0.5413256955810147\n",
      "Percentage of terminated:  0.5237899917965545\n",
      "Percentage of terminated:  0.5445665445665445\n",
      "Percentage of terminated:  0.5340399510803098\n",
      "Percentage of terminated:  0.5172694026818366\n",
      "Percentage of terminated:  0.5361371988566762\n",
      "Percentage of terminated:  0.5058775841102554\n",
      "Percentage of terminated:  0.5160896130346232\n",
      "Percentage of terminated:  0.48742903487429035\n",
      "Percentage of terminated:  0.4820554649265905\n",
      "Percentage of terminated:  0.4778365189101261\n",
      "Percentage of terminated:  0.574735987002437\n",
      "Percentage of terminated:  0.5444489963129865\n",
      "Percentage of terminated:  0.4853061224489796\n",
      "Percentage of terminated:  0.5219869706840391\n",
      "Percentage of terminated:  0.5330342577487766\n",
      "Percentage of terminated:  0.5223214285714286\n",
      "Percentage of terminated:  0.516721044045677\n",
      "Percentage of terminated:  0.5306536743808363\n",
      "Percentage of terminated:  0.48632094732543896\n",
      "Percentage of terminated:  0.5268729641693811\n",
      "Percentage of terminated:  0.55180820804551\n",
      "Percentage of terminated:  0.5478119935170178\n",
      "Percentage of terminated:  0.5270655270655271\n",
      "Percentage of terminated:  0.553147996729354\n",
      "Percentage of terminated:  0.529817444219067\n",
      "Percentage of terminated:  0.5304205798285014\n",
      "Percentage of terminated:  0.5197074360016254\n",
      "Percentage of terminated:  0.5262943334692214\n",
      "Percentage of terminated:  0.5014210312626878\n",
      "Percentage of terminated:  0.46774847870182557\n",
      "Percentage of terminated:  0.5446355446355446\n",
      "Percentage of terminated:  0.5364859355890746\n",
      "Percentage of terminated:  0.515695067264574\n",
      "Percentage of terminated:  0.49351701782820095\n",
      "Percentage of terminated:  0.5062931384490459\n",
      "Percentage of terminated:  0.4989816700610998\n",
      "Percentage of terminated:  0.5472779369627507\n",
      "Percentage of terminated:  0.5357142857142857\n",
      "Percentage of terminated:  0.532053899550837\n",
      "Percentage of terminated:  0.5244642135058634\n",
      "Percentage of terminated:  0.5520748576078113\n",
      "Percentage of terminated:  0.5261224489795918\n",
      "Percentage of terminated:  0.5620408163265306\n",
      "Percentage of terminated:  0.5392394822006472\n",
      "Percentage of terminated:  0.5555555555555556\n",
      "Percentage of terminated:  0.48998774008990603\n",
      "Percentage of terminated:  0.5415309446254072\n",
      "Percentage of terminated:  0.4963235294117647\n",
      "Percentage of terminated:  0.488456865127582\n",
      "Percentage of terminated:  0.5283557731538148\n",
      "Percentage of terminated:  0.5415132924335379\n",
      "Percentage of terminated:  0.5499796001631987\n",
      "Percentage of terminated:  0.494509963399756\n",
      "Percentage of terminated:  0.5686756867568675\n",
      "Percentage of terminated:  0.4906122448979592\n",
      "Percentage of terminated:  0.5273469387755102\n",
      "Percentage of terminated:  0.49979814291481633\n",
      "Percentage of terminated:  0.5303957568339454\n",
      "Percentage of terminated:  0.5049099836333879\n",
      "Percentage of terminated:  0.5595579205894392\n",
      "Percentage of terminated:  0.48550428746427116\n",
      "Percentage of terminated:  0.5314029363784666\n",
      "Percentage of terminated:  0.48984565393988627\n",
      "Percentage of terminated:  0.5295081967213114\n",
      "Percentage of terminated:  0.5444126074498568\n",
      "Percentage of terminated:  0.5161027313493681\n",
      "Percentage of terminated:  0.5330093155123532\n",
      "Percentage of terminated:  0.4927243330638642\n",
      "Percentage of terminated:  0.5301057770545159\n",
      "Percentage of terminated:  0.45424969499796664\n",
      "Percentage of terminated:  0.5328467153284672\n",
      "Percentage of terminated:  0.536734693877551\n",
      "Percentage of terminated:  0.5226069246435845\n",
      "Percentage of terminated:  0.5501618122977346\n",
      "Percentage of terminated:  0.5114192495921697\n",
      "Percentage of terminated:  0.5340122199592668\n",
      "Percentage of terminated:  0.49571603427172584\n",
      "Percentage of terminated:  0.5252032520325203\n",
      "Percentage of terminated:  0.525609756097561\n",
      "Percentage of terminated:  0.5120753172329103\n",
      "Percentage of terminated:  0.4837133550488599\n",
      "Percentage of terminated:  0.49897917517354023\n",
      "Percentage of terminated:  0.5609955120359037\n",
      "Percentage of terminated:  0.5222358221134231\n",
      "Percentage of terminated:  0.5329512893982808\n",
      "Percentage of terminated:  0.5191524042379788\n",
      "Percentage of terminated:  0.5243704305442729\n",
      "Percentage of terminated:  0.5087362860625761\n",
      "Percentage of terminated:  0.502643350955673\n",
      "Percentage of terminated:  0.5171288743882545\n",
      "Percentage of terminated:  0.5522145469321414\n",
      "Percentage of terminated:  0.556642216788916\n",
      "Percentage of terminated:  0.5138211382113821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 01:19:00.814119: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5265180630284396\n",
      "Epoch 4, Train Loss: 3.1292, Validation Loss: 3.0272\n",
      "Percentage of terminated:  0.5395006139991814\n",
      "Percentage of terminated:  0.5411716509627202\n",
      "Percentage of terminated:  0.5567855678556786\n",
      "Percentage of terminated:  0.5400243803331979\n",
      "Percentage of terminated:  0.543451652386781\n",
      "Percentage of terminated:  0.5550570962479608\n",
      "Percentage of terminated:  0.5164300202839757\n",
      "Percentage of terminated:  0.5189409368635438\n",
      "Percentage of terminated:  0.5002048340843916\n",
      "Percentage of terminated:  0.540976988292289\n",
      "Percentage of terminated:  0.5356113627006999\n",
      "Percentage of terminated:  0.5473000406008932\n",
      "Percentage of terminated:  0.4735557363710334\n",
      "Percentage of terminated:  0.5052974735126324\n",
      "Percentage of terminated:  0.5199511400651465\n",
      "Percentage of terminated:  0.5605875152998776\n",
      "Percentage of terminated:  0.5221995926680244\n",
      "Percentage of terminated:  0.49898580121703856\n",
      "Percentage of terminated:  0.5372340425531915\n",
      "Percentage of terminated:  0.5495273325113029\n",
      "Percentage of terminated:  0.5302040816326531\n",
      "Percentage of terminated:  0.5\n",
      "Percentage of terminated:  0.5087862689006948\n",
      "Percentage of terminated:  0.5234789710085749\n",
      "Percentage of terminated:  0.5289356535815459\n",
      "Percentage of terminated:  0.52719836400818\n",
      "Percentage of terminated:  0.5466830466830467\n",
      "Percentage of terminated:  0.5205535205535206\n",
      "Percentage of terminated:  0.46574225122349105\n",
      "Percentage of terminated:  0.5190283400809717\n",
      "Percentage of terminated:  0.5231895850284785\n",
      "Percentage of terminated:  0.5298659081674116\n",
      "Percentage of terminated:  0.5301548492257538\n",
      "Percentage of terminated:  0.5318627450980392\n",
      "Percentage of terminated:  0.4936914936914937\n",
      "Percentage of terminated:  0.527562270314414\n",
      "Percentage of terminated:  0.5052931596091205\n",
      "Percentage of terminated:  0.5252032520325203\n",
      "Percentage of terminated:  0.5002028397565923\n",
      "Percentage of terminated:  0.4989800081599347\n",
      "Percentage of terminated:  0.4931062449310625\n",
      "Percentage of terminated:  0.5293159609120521\n",
      "Percentage of terminated:  0.5291479820627802\n",
      "Percentage of terminated:  0.5312627707396812\n",
      "Percentage of terminated:  0.5211038961038961\n",
      "Percentage of terminated:  0.5034708044099633\n",
      "Percentage of terminated:  0.5268729641693811\n",
      "Percentage of terminated:  0.4997974888618874\n",
      "Percentage of terminated:  0.5032362459546925\n",
      "Percentage of terminated:  0.5283326538931921\n",
      "Percentage of terminated:  0.5238095238095238\n",
      "Percentage of terminated:  0.5406620351450756\n",
      "Percentage of terminated:  0.4683954619124797\n",
      "Percentage of terminated:  0.5368335368335369\n",
      "Percentage of terminated:  0.5505709624796085\n",
      "Percentage of terminated:  0.5069331158238173\n",
      "Percentage of terminated:  0.5128205128205128\n",
      "Percentage of terminated:  0.5324092947411333\n",
      "Percentage of terminated:  0.5292207792207793\n",
      "Percentage of terminated:  0.5380855397148676\n",
      "Percentage of terminated:  0.5699672667757774\n",
      "Percentage of terminated:  0.5317073170731708\n",
      "Percentage of terminated:  0.5191993464052288\n",
      "Percentage of terminated:  0.5316973415132924\n",
      "Percentage of terminated:  0.5615258408531584\n",
      "Percentage of terminated:  0.5169042769857434\n",
      "Percentage of terminated:  0.5546938775510204\n",
      "Percentage of terminated:  0.5425101214574899\n",
      "Percentage of terminated:  0.49510603588907015\n",
      "Percentage of terminated:  0.5580828594638505\n",
      "Percentage of terminated:  0.5004058441558441\n",
      "Percentage of terminated:  0.5065520065520066\n",
      "Percentage of terminated:  0.5361494719740049\n",
      "Percentage of terminated:  0.5236348818255909\n",
      "Percentage of terminated:  0.5303957568339454\n",
      "Percentage of terminated:  0.5225701504676697\n",
      "Percentage of terminated:  0.5149918962722853\n",
      "Percentage of terminated:  0.5403523146251537\n",
      "Percentage of terminated:  0.4565916398713826\n",
      "Percentage of terminated:  0.5428103236378533\n",
      "Percentage of terminated:  0.5036734693877551\n",
      "Percentage of terminated:  0.512214983713355\n",
      "Percentage of terminated:  0.5278795278795279\n",
      "Percentage of terminated:  0.5259926320098239\n",
      "Percentage of terminated:  0.5065252854812398\n",
      "Percentage of terminated:  0.4944920440636475\n",
      "Percentage of terminated:  0.5317848410757946\n",
      "Percentage of terminated:  0.5318013951579811\n",
      "Percentage of terminated:  0.5109400324149108\n",
      "Percentage of terminated:  0.5328974254188803\n",
      "Percentage of terminated:  0.5042839657282742\n",
      "Percentage of terminated:  0.497765136123527\n",
      "Percentage of terminated:  0.5519348268839104\n",
      "Percentage of terminated:  0.5390243902439025\n",
      "Percentage of terminated:  0.5081833060556464\n",
      "Percentage of terminated:  0.5389636882904937\n",
      "Percentage of terminated:  0.5402112103980503\n",
      "Percentage of terminated:  0.5058680696074463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.44427363566487316\n",
      "Epoch 5, Train Loss: 2.9703, Validation Loss: 2.9273\n",
      "Percentage of terminated:  0.5190437601296597\n",
      "Percentage of terminated:  0.5368935996738687\n",
      "Percentage of terminated:  0.5535055350553506\n",
      "Percentage of terminated:  0.5461224489795918\n",
      "Percentage of terminated:  0.5192307692307693\n",
      "Percentage of terminated:  0.512006512006512\n",
      "Percentage of terminated:  0.5459016393442623\n",
      "Percentage of terminated:  0.5164068908941756\n",
      "Percentage of terminated:  0.49878147847278637\n",
      "Percentage of terminated:  0.5101957585644372\n",
      "Percentage of terminated:  0.5251122907309106\n",
      "Percentage of terminated:  0.5108207431604737\n",
      "Percentage of terminated:  0.5532001630656339\n",
      "Percentage of terminated:  0.5236541598694943\n",
      "Percentage of terminated:  0.5113544201135442\n",
      "Percentage of terminated:  0.5223033252230332\n",
      "Percentage of terminated:  0.5412919051512673\n",
      "Percentage of terminated:  0.5574172456068656\n",
      "Percentage of terminated:  0.5099633997559984\n",
      "Percentage of terminated:  0.527169505271695\n",
      "Percentage of terminated:  0.5263372805226623\n",
      "Percentage of terminated:  0.5489240763296792\n",
      "Percentage of terminated:  0.48962993086620576\n",
      "Percentage of terminated:  0.531671434409481\n",
      "Percentage of terminated:  0.5197717081125153\n",
      "Percentage of terminated:  0.496149169031212\n",
      "Percentage of terminated:  0.5324302134646962\n",
      "Percentage of terminated:  0.5239448051948052\n",
      "Percentage of terminated:  0.5014152850788516\n",
      "Percentage of terminated:  0.5060975609756098\n",
      "Percentage of terminated:  0.5199511400651465\n",
      "Percentage of terminated:  0.5335252982311806\n",
      "Percentage of terminated:  0.49552845528455286\n",
      "Percentage of terminated:  0.4971544715447154\n",
      "Percentage of terminated:  0.5116279069767442\n",
      "Percentage of terminated:  0.5477448191791955\n",
      "Percentage of terminated:  0.5201793721973094\n",
      "Percentage of terminated:  0.5185034566897113\n",
      "Percentage of terminated:  0.5310457516339869\n",
      "Percentage of terminated:  0.5389955083707636\n",
      "Percentage of terminated:  0.5373012637586628\n",
      "Percentage of terminated:  0.5311102074013827\n",
      "Percentage of terminated:  0.5301253538212697\n",
      "Percentage of terminated:  0.5239852398523985\n",
      "Percentage of terminated:  0.514831369362048\n",
      "Percentage of terminated:  0.49451442503047544\n",
      "Percentage of terminated:  0.5077868852459017\n",
      "Percentage of terminated:  0.5272429332240884\n",
      "Percentage of terminated:  0.5101791530944625\n",
      "Percentage of terminated:  0.5213849287169042\n",
      "Percentage of terminated:  0.5570497147514263\n",
      "Percentage of terminated:  0.559918200408998\n",
      "Percentage of terminated:  0.5075172694026818\n",
      "Percentage of terminated:  0.5211382113821138\n",
      "Percentage of terminated:  0.5456389452332657\n",
      "Percentage of terminated:  0.5385556915544676\n",
      "Percentage of terminated:  0.5318200243210377\n",
      "Percentage of terminated:  0.5399753997539976\n",
      "Percentage of terminated:  0.5104124132298897\n",
      "Percentage of terminated:  0.5445261437908496\n",
      "Percentage of terminated:  0.5336872192731728\n",
      "Percentage of terminated:  0.4957403651115619\n",
      "Percentage of terminated:  0.5085020242914979\n",
      "Percentage of terminated:  0.5012165450121655\n",
      "Percentage of terminated:  0.5263157894736842\n",
      "Percentage of terminated:  0.5107505070993915\n",
      "Percentage of terminated:  0.5224123879380603\n",
      "Percentage of terminated:  0.5278342137342543\n",
      "Percentage of terminated:  0.5093117408906883\n",
      "Percentage of terminated:  0.5087647778230738\n",
      "Percentage of terminated:  0.526955816781516\n",
      "Percentage of terminated:  0.5395653956539566\n",
      "Percentage of terminated:  0.5386807817589576\n",
      "Percentage of terminated:  0.5537459283387622\n",
      "Percentage of terminated:  0.4894308943089431\n",
      "Percentage of terminated:  0.5048899755501223\n",
      "Percentage of terminated:  0.5428109854604201\n",
      "Percentage of terminated:  0.5316455696202531\n",
      "Percentage of terminated:  0.5780730897009967\n",
      "Percentage of terminated:  0.5110114192495921\n",
      "Percentage of terminated:  0.5580448065173116\n",
      "Percentage of terminated:  0.5261865793780688\n",
      "Percentage of terminated:  0.5688671556642216\n",
      "Percentage of terminated:  0.5161422149570903\n",
      "Percentage of terminated:  0.4926829268292683\n",
      "Percentage of terminated:  0.5592668024439919\n",
      "Percentage of terminated:  0.5476482617586912\n",
      "Percentage of terminated:  0.5124135124135124\n",
      "Percentage of terminated:  0.513039934800326\n",
      "Percentage of terminated:  0.5358024691358024\n",
      "Percentage of terminated:  0.5026412027631044\n",
      "Percentage of terminated:  0.5122050447518307\n",
      "Percentage of terminated:  0.5232323232323233\n",
      "Percentage of terminated:  0.5265089722675367\n",
      "Percentage of terminated:  0.5148434322895485\n",
      "Percentage of terminated:  0.5404742436631235\n",
      "Percentage of terminated:  0.5637775960752248\n",
      "Percentage of terminated:  0.49634146341463414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5252293577981652\n",
      "Epoch 6, Train Loss: 2.9036, Validation Loss: 2.8620\n",
      "Percentage of terminated:  0.5689233278955954\n",
      "Percentage of terminated:  0.5379253792537926\n",
      "Percentage of terminated:  0.5234025234025234\n",
      "Percentage of terminated:  0.5578732106339468\n",
      "Percentage of terminated:  0.5335775335775336\n",
      "Percentage of terminated:  0.5408496732026143\n",
      "Percentage of terminated:  0.510012259910094\n",
      "Percentage of terminated:  0.5466395112016293\n",
      "Percentage of terminated:  0.535234215885947\n",
      "Percentage of terminated:  0.5450121654501217\n",
      "Percentage of terminated:  0.5495934959349593\n",
      "Percentage of terminated:  0.5488997555012225\n",
      "Percentage of terminated:  0.5351020408163265\n",
      "Percentage of terminated:  0.518579011841568\n",
      "Percentage of terminated:  0.529817444219067\n",
      "Percentage of terminated:  0.5152874031797798\n",
      "Percentage of terminated:  0.5403651115618662\n",
      "Percentage of terminated:  0.5373927257866775\n",
      "Percentage of terminated:  0.526955816781516\n",
      "Percentage of terminated:  0.511400651465798\n",
      "Percentage of terminated:  0.5346169602621876\n",
      "Percentage of terminated:  0.48860862489829127\n",
      "Percentage of terminated:  0.5568321707016824\n",
      "Percentage of terminated:  0.5333604556550041\n",
      "Percentage of terminated:  0.5252648736756316\n",
      "Percentage of terminated:  0.529817444219067\n",
      "Percentage of terminated:  0.49817739975698666\n",
      "Percentage of terminated:  0.5225885225885226\n",
      "Percentage of terminated:  0.5152874031797798\n",
      "Percentage of terminated:  0.5161027313493681\n",
      "Percentage of terminated:  0.5481120584652862\n",
      "Percentage of terminated:  0.5294840294840295\n",
      "Percentage of terminated:  0.5426704777460187\n",
      "Percentage of terminated:  0.5156567710451403\n",
      "Percentage of terminated:  0.5071225071225072\n",
      "Percentage of terminated:  0.5703824247355573\n",
      "Percentage of terminated:  0.5288852725793328\n",
      "Percentage of terminated:  0.5452332657200811\n",
      "Percentage of terminated:  0.5147299509001637\n",
      "Percentage of terminated:  0.487408610885459\n",
      "Percentage of terminated:  0.5423797881010595\n",
      "Percentage of terminated:  0.5064987814784728\n",
      "Percentage of terminated:  0.5203748981255094\n",
      "Percentage of terminated:  0.5671154630762953\n",
      "Percentage of terminated:  0.538619556285949\n",
      "Percentage of terminated:  0.5371498172959805\n",
      "Percentage of terminated:  0.5240619902120718\n",
      "Percentage of terminated:  0.5572768039135753\n",
      "Percentage of terminated:  0.5345320800980793\n",
      "Percentage of terminated:  0.5515548281505729\n",
      "Percentage of terminated:  0.5296281160604822\n",
      "Percentage of terminated:  0.532599837000815\n",
      "Percentage of terminated:  0.5152253349573691\n",
      "Percentage of terminated:  0.5228013029315961\n",
      "Percentage of terminated:  0.5204702067288204\n",
      "Percentage of terminated:  0.5170870626525631\n",
      "Percentage of terminated:  0.5367736692401462\n",
      "Percentage of terminated:  0.532139951179821\n",
      "Percentage of terminated:  0.5138099106417546\n",
      "Percentage of terminated:  0.49328995526636843\n",
      "Percentage of terminated:  0.5044751830756713\n",
      "Percentage of terminated:  0.49228269699431354\n",
      "Percentage of terminated:  0.5255623721881391\n",
      "Percentage of terminated:  0.5307035380235868\n",
      "Percentage of terminated:  0.5390879478827362\n",
      "Percentage of terminated:  0.5446791226645004\n",
      "Percentage of terminated:  0.5545638945233266\n",
      "Percentage of terminated:  0.513281569268492\n",
      "Percentage of terminated:  0.5256514657980456\n",
      "Percentage of terminated:  0.5508786268900695\n",
      "Percentage of terminated:  0.5546970313135421\n",
      "Percentage of terminated:  0.5155102040816326\n",
      "Percentage of terminated:  0.5214198286413708\n",
      "Percentage of terminated:  0.48309572301425663\n",
      "Percentage of terminated:  0.5122050447518307\n",
      "Percentage of terminated:  0.5264873675631622\n",
      "Percentage of terminated:  0.5209605209605209\n",
      "Percentage of terminated:  0.5385242560130452\n",
      "Percentage of terminated:  0.5032467532467533\n",
      "Percentage of terminated:  0.5159313725490197\n",
      "Percentage of terminated:  0.5002039983680131\n",
      "Percentage of terminated:  0.5491869918699187\n",
      "Percentage of terminated:  0.5246636771300448\n",
      "Percentage of terminated:  0.5163265306122449\n",
      "Percentage of terminated:  0.5451573355128729\n",
      "Percentage of terminated:  0.5325102880658437\n",
      "Percentage of terminated:  0.532462229481421\n",
      "Percentage of terminated:  0.524390243902439\n",
      "Percentage of terminated:  0.4864097363083164\n",
      "Percentage of terminated:  0.5231331168831169\n",
      "Percentage of terminated:  0.4862012987012987\n",
      "Percentage of terminated:  0.5353946297803092\n",
      "Percentage of terminated:  0.5240032546786005\n",
      "Percentage of terminated:  0.5044715447154472\n",
      "Percentage of terminated:  0.5568228105906313\n",
      "Percentage of terminated:  0.5184124386252046\n",
      "Percentage of terminated:  0.5413290113452188\n",
      "Percentage of terminated:  0.502435064935065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5570987654320988\n",
      "Epoch 7, Train Loss: 2.8409, Validation Loss: 2.8424\n",
      "Percentage of terminated:  0.4735772357723577\n",
      "Percentage of terminated:  0.5038445973290165\n",
      "Percentage of terminated:  0.4947111472742067\n",
      "Percentage of terminated:  0.5566884176182708\n",
      "Percentage of terminated:  0.5158924205378973\n",
      "Percentage of terminated:  0.5482688391038697\n",
      "Percentage of terminated:  0.5194592380172061\n",
      "Percentage of terminated:  0.5282176207876573\n",
      "Percentage of terminated:  0.553417928776095\n",
      "Percentage of terminated:  0.5412955465587045\n",
      "Percentage of terminated:  0.5527597402597403\n",
      "Percentage of terminated:  0.5583673469387755\n",
      "Percentage of terminated:  0.4685714285714286\n",
      "Percentage of terminated:  0.5111925111925112\n",
      "Percentage of terminated:  0.5065252854812398\n",
      "Percentage of terminated:  0.547696038803557\n",
      "Percentage of terminated:  0.4993894993894994\n",
      "Percentage of terminated:  0.5362969004893964\n",
      "Percentage of terminated:  0.5460122699386503\n",
      "Percentage of terminated:  0.5472006538618717\n",
      "Percentage of terminated:  0.5142624286878565\n",
      "Percentage of terminated:  0.4946852003270646\n",
      "Percentage of terminated:  0.5135024549918167\n",
      "Percentage of terminated:  0.5253755582622818\n",
      "Percentage of terminated:  0.5197395197395197\n",
      "Percentage of terminated:  0.5404742436631235\n",
      "Percentage of terminated:  0.5417515274949084\n",
      "Percentage of terminated:  0.5343635624237495\n",
      "Percentage of terminated:  0.5075050709939148\n",
      "Percentage of terminated:  0.5203915171288744\n",
      "Percentage of terminated:  0.5120359037127703\n",
      "Percentage of terminated:  0.5249695493300852\n",
      "Percentage of terminated:  0.47128309572301424\n",
      "Percentage of terminated:  0.5745542949756888\n",
      "Percentage of terminated:  0.5114192495921697\n",
      "Percentage of terminated:  0.5306122448979592\n",
      "Percentage of terminated:  0.5032813781788351\n",
      "Percentage of terminated:  0.5205702647657842\n",
      "Percentage of terminated:  0.5629084967320261\n",
      "Percentage of terminated:  0.5118270799347472\n",
      "Percentage of terminated:  0.4991889699918897\n",
      "Percentage of terminated:  0.5146222583265637\n",
      "Percentage of terminated:  0.5453436356242375\n",
      "Percentage of terminated:  0.515695067264574\n",
      "Percentage of terminated:  0.531578947368421\n",
      "Percentage of terminated:  0.5211038961038961\n",
      "Percentage of terminated:  0.5138662316476346\n",
      "Percentage of terminated:  0.571078431372549\n",
      "Percentage of terminated:  0.5373983739837398\n",
      "Percentage of terminated:  0.5020358306188925\n",
      "Percentage of terminated:  0.5207317073170732\n",
      "Percentage of terminated:  0.49817444219066936\n",
      "Percentage of terminated:  0.5234981610134859\n",
      "Percentage of terminated:  0.5141930251419302\n",
      "Percentage of terminated:  0.5407497962510187\n",
      "Percentage of terminated:  0.5388071895424836\n",
      "Percentage of terminated:  0.49736948603804126\n",
      "Percentage of terminated:  0.5348178137651822\n",
      "Percentage of terminated:  0.5065466448445172\n",
      "Percentage of terminated:  0.5122050447518307\n",
      "Percentage of terminated:  0.5372340425531915\n",
      "Percentage of terminated:  0.559064807219032\n",
      "Percentage of terminated:  0.5543744889615699\n",
      "Percentage of terminated:  0.4606011372867587\n",
      "Percentage of terminated:  0.5393395841826335\n",
      "Percentage of terminated:  0.5270215359609914\n",
      "Percentage of terminated:  0.49326805385556916\n",
      "Percentage of terminated:  0.5012135922330098\n",
      "Percentage of terminated:  0.5062931384490459\n",
      "Percentage of terminated:  0.5194963444354184\n",
      "Percentage of terminated:  0.5313638203156617\n",
      "Percentage of terminated:  0.5129659643435981\n",
      "Percentage of terminated:  0.5281402936378466\n",
      "Percentage of terminated:  0.5198201879852881\n",
      "Percentage of terminated:  0.50163132137031\n",
      "Percentage of terminated:  0.5170731707317073\n",
      "Percentage of terminated:  0.49817739975698666\n",
      "Percentage of terminated:  0.5412099066179455\n",
      "Percentage of terminated:  0.5555555555555556\n",
      "Percentage of terminated:  0.5480179812014712\n",
      "Percentage of terminated:  0.5328200972447326\n",
      "Percentage of terminated:  0.5751020408163265\n",
      "Percentage of terminated:  0.5304205798285014\n",
      "Percentage of terminated:  0.5691683569979716\n",
      "Percentage of terminated:  0.5099553027224706\n",
      "Percentage of terminated:  0.5148797390949857\n",
      "Percentage of terminated:  0.4917171717171717\n",
      "Percentage of terminated:  0.5065412919051513\n",
      "Percentage of terminated:  0.5028432168968319\n",
      "Percentage of terminated:  0.5653061224489796\n",
      "Percentage of terminated:  0.4985835694050991\n",
      "Percentage of terminated:  0.5525672371638142\n",
      "Percentage of terminated:  0.5774877650897227\n",
      "Percentage of terminated:  0.5094108019639935\n",
      "Percentage of terminated:  0.5135572642654795\n",
      "Percentage of terminated:  0.5396303901437371\n",
      "Percentage of terminated:  0.5248169243287225\n",
      "Percentage of terminated:  0.4827024827024827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 04:44:12.758469: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5278838808250573\n",
      "Epoch 8, Train Loss: 2.8361, Validation Loss: 2.8060\n",
      "Percentage of terminated:  0.5311608961303462\n",
      "Percentage of terminated:  0.5507780507780508\n",
      "Percentage of terminated:  0.5154346060113729\n",
      "Percentage of terminated:  0.5170593013809911\n",
      "Percentage of terminated:  0.5300813008130081\n",
      "Percentage of terminated:  0.5373376623376623\n",
      "Percentage of terminated:  0.5276647681041498\n",
      "Percentage of terminated:  0.5376782077393075\n",
      "Percentage of terminated:  0.5042735042735043\n",
      "Percentage of terminated:  0.4905814905814906\n",
      "Percentage of terminated:  0.5124642419288925\n",
      "Percentage of terminated:  0.571604938271605\n",
      "Percentage of terminated:  0.5877085877085877\n",
      "Percentage of terminated:  0.5566925910765452\n",
      "Percentage of terminated:  0.5428221859706363\n",
      "Percentage of terminated:  0.5252442996742671\n",
      "Percentage of terminated:  0.4892929292929293\n",
      "Percentage of terminated:  0.5575980392156863\n",
      "Percentage of terminated:  0.5164969450101833\n",
      "Percentage of terminated:  0.5447486718430732\n",
      "Percentage of terminated:  0.532520325203252\n",
      "Percentage of terminated:  0.5199025182778229\n",
      "Percentage of terminated:  0.5486111111111112\n",
      "Percentage of terminated:  0.5105263157894737\n",
      "Percentage of terminated:  0.5150284321689683\n",
      "Percentage of terminated:  0.5564581640942323\n",
      "Percentage of terminated:  0.4997969955339017\n",
      "Percentage of terminated:  0.5022439820481436\n",
      "Percentage of terminated:  0.48964677222898906\n",
      "Percentage of terminated:  0.527027027027027\n",
      "Percentage of terminated:  0.5188524590163934\n",
      "Percentage of terminated:  0.5024410089503661\n",
      "Percentage of terminated:  0.5171149144254279\n",
      "Percentage of terminated:  0.49226384364820847\n",
      "Percentage of terminated:  0.5183823529411765\n",
      "Percentage of terminated:  0.5394736842105263\n",
      "Percentage of terminated:  0.48942229454841335\n",
      "Percentage of terminated:  0.5051041241322989\n",
      "Percentage of terminated:  0.5271096616388096\n",
      "Percentage of terminated:  0.50814332247557\n",
      "Percentage of terminated:  0.5443349753694581\n",
      "Percentage of terminated:  0.48444444444444446\n",
      "Percentage of terminated:  0.5660531697341513\n",
      "Percentage of terminated:  0.5549674267100977\n",
      "Percentage of terminated:  0.517979797979798\n",
      "Percentage of terminated:  0.525609756097561\n",
      "Percentage of terminated:  0.5150651465798045\n",
      "Percentage of terminated:  0.5294358135731807\n",
      "Percentage of terminated:  0.556772100567721\n",
      "Percentage of terminated:  0.5384927066450568\n",
      "Percentage of terminated:  0.5393075356415479\n",
      "Percentage of terminated:  0.5171009771986971\n",
      "Percentage of terminated:  0.541548439400081\n",
      "Percentage of terminated:  0.5294595692807802\n",
      "Percentage of terminated:  0.5036794766966476\n",
      "Percentage of terminated:  0.5232463295269169\n",
      "Percentage of terminated:  0.509979633401222\n",
      "Percentage of terminated:  0.5187449062754687\n",
      "Percentage of terminated:  0.5212418300653595\n",
      "Percentage of terminated:  0.5422077922077922\n",
      "Percentage of terminated:  0.5042596348884382\n",
      "Percentage of terminated:  0.5244299674267101\n",
      "Percentage of terminated:  0.5148073022312373\n",
      "Percentage of terminated:  0.557676685621446\n",
      "Percentage of terminated:  0.518170681910984\n",
      "Percentage of terminated:  0.5285714285714286\n",
      "Percentage of terminated:  0.5243353783231084\n",
      "Percentage of terminated:  0.5271035598705501\n",
      "Percentage of terminated:  0.5212418300653595\n",
      "Percentage of terminated:  0.537525354969574\n",
      "Percentage of terminated:  0.5192622950819672\n",
      "Percentage of terminated:  0.5235366352844862\n",
      "Percentage of terminated:  0.5191524042379788\n",
      "Percentage of terminated:  0.5426704777460187\n",
      "Percentage of terminated:  0.5059159526723787\n",
      "Percentage of terminated:  0.5128100854005694\n",
      "Percentage of terminated:  0.5497553017944535\n",
      "Percentage of terminated:  0.5297252972529726\n",
      "Percentage of terminated:  0.5394736842105263\n",
      "Percentage of terminated:  0.5220588235294118\n",
      "Percentage of terminated:  0.531084924827306\n",
      "Percentage of terminated:  0.512684124386252\n",
      "Percentage of terminated:  0.5424969499796666\n",
      "Percentage of terminated:  0.5306122448979592\n",
      "Percentage of terminated:  0.5145748987854251\n",
      "Percentage of terminated:  0.4959448499594485\n",
      "Percentage of terminated:  0.5549203756635361\n",
      "Percentage of terminated:  0.5046728971962616\n",
      "Percentage of terminated:  0.5227552275522755\n",
      "Percentage of terminated:  0.5089722675367048\n",
      "Percentage of terminated:  0.5186094069529652\n",
      "Percentage of terminated:  0.525071341214839\n",
      "Percentage of terminated:  0.5792058943921408\n",
      "Percentage of terminated:  0.5336597307221542\n",
      "Percentage of terminated:  0.5379618351603735\n",
      "Percentage of terminated:  0.5473300970873787\n",
      "Percentage of terminated:  0.5359026369168357\n",
      "Percentage of terminated:  0.4902518277822908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5431818181818182\n",
      "Epoch 9, Train Loss: 2.7976, Validation Loss: 2.7738\n",
      "Percentage of terminated:  0.5361963190184049\n",
      "Percentage of terminated:  0.5499796001631987\n",
      "Percentage of terminated:  0.48447712418300654\n",
      "Percentage of terminated:  0.561015561015561\n",
      "Percentage of terminated:  0.5400326797385621\n",
      "Percentage of terminated:  0.5277777777777778\n",
      "Percentage of terminated:  0.5378632828489562\n",
      "Percentage of terminated:  0.5568228105906313\n",
      "Percentage of terminated:  0.5525244299674267\n",
      "Percentage of terminated:  0.5290768605124034\n",
      "Percentage of terminated:  0.5088078656288406\n",
      "Percentage of terminated:  0.5403978887535525\n",
      "Percentage of terminated:  0.5684125050751117\n",
      "Percentage of terminated:  0.48580697485806973\n",
      "Percentage of terminated:  0.5245700245700246\n",
      "Percentage of terminated:  0.49552845528455286\n",
      "Percentage of terminated:  0.49060457516339867\n",
      "Percentage of terminated:  0.5087433916226108\n",
      "Percentage of terminated:  0.525030525030525\n",
      "Percentage of terminated:  0.5359477124183006\n",
      "Percentage of terminated:  0.5364265364265364\n",
      "Percentage of terminated:  0.5279022403258656\n",
      "Percentage of terminated:  0.5491470349309504\n",
      "Percentage of terminated:  0.5188640973630831\n",
      "Percentage of terminated:  0.5624227441285538\n",
      "Percentage of terminated:  0.5075233834892232\n",
      "Percentage of terminated:  0.5149651496514965\n",
      "Percentage of terminated:  0.494281045751634\n",
      "Percentage of terminated:  0.5099633997559984\n",
      "Percentage of terminated:  0.5671031096563012\n",
      "Percentage of terminated:  0.5272801302931596\n",
      "Percentage of terminated:  0.519559902200489\n",
      "Percentage of terminated:  0.5388369255795039\n",
      "Percentage of terminated:  0.5006145022531749\n",
      "Percentage of terminated:  0.5389002036659878\n",
      "Percentage of terminated:  0.5273611674098095\n",
      "Percentage of terminated:  0.4975747776879547\n",
      "Percentage of terminated:  0.552439024390244\n",
      "Percentage of terminated:  0.5263157894736842\n",
      "Percentage of terminated:  0.5452322738386308\n",
      "Percentage of terminated:  0.5101957585644372\n",
      "Percentage of terminated:  0.5248776508972267\n",
      "Percentage of terminated:  0.5467800729040098\n",
      "Percentage of terminated:  0.513911620294599\n",
      "Percentage of terminated:  0.5077298616761595\n",
      "Percentage of terminated:  0.5054811205846529\n",
      "Percentage of terminated:  0.4745417515274949\n",
      "Percentage of terminated:  0.5543345543345544\n",
      "Percentage of terminated:  0.5512454062882809\n",
      "Percentage of terminated:  0.49125660837738916\n",
      "Percentage of terminated:  0.5564123376623377\n",
      "Percentage of terminated:  0.5460097719869706\n",
      "Percentage of terminated:  0.49188311688311687\n",
      "Percentage of terminated:  0.5623980424143556\n",
      "Percentage of terminated:  0.5040551500405515\n",
      "Percentage of terminated:  0.5046539862403885\n",
      "Percentage of terminated:  0.5256723716381418\n",
      "Percentage of terminated:  0.5032520325203252\n",
      "Percentage of terminated:  0.5388053636733036\n",
      "Percentage of terminated:  0.5055034651447208\n",
      "Percentage of terminated:  0.5171990171990172\n",
      "Percentage of terminated:  0.5322253749493312\n",
      "Percentage of terminated:  0.5189101260675071\n",
      "Percentage of terminated:  0.566516577977896\n",
      "Percentage of terminated:  0.5258445258445258\n",
      "Percentage of terminated:  0.5081366965012205\n",
      "Percentage of terminated:  0.5211038961038961\n",
      "Percentage of terminated:  0.5107505070993915\n",
      "Percentage of terminated:  0.5305126118795769\n",
      "Percentage of terminated:  0.5087433916226108\n",
      "Percentage of terminated:  0.5298901992679951\n",
      "Percentage of terminated:  0.5012175324675324\n",
      "Percentage of terminated:  0.5383360522022839\n",
      "Percentage of terminated:  0.5228758169934641\n",
      "Percentage of terminated:  0.5378323108384458\n",
      "Percentage of terminated:  0.5556910569105691\n",
      "Percentage of terminated:  0.5216161616161616\n",
      "Percentage of terminated:  0.5546425636811833\n",
      "Percentage of terminated:  0.5620915032679739\n",
      "Percentage of terminated:  0.5741042345276873\n",
      "Percentage of terminated:  0.5330073349633252\n",
      "Percentage of terminated:  0.5472312703583062\n",
      "Percentage of terminated:  0.5459677419354839\n",
      "Percentage of terminated:  0.5394683026584867\n",
      "Percentage of terminated:  0.5322712418300654\n",
      "Percentage of terminated:  0.550837076357697\n",
      "Percentage of terminated:  0.5389025389025389\n",
      "Percentage of terminated:  0.5787755102040816\n",
      "Percentage of terminated:  0.5227457351746547\n",
      "Percentage of terminated:  0.5125201938610663\n",
      "Percentage of terminated:  0.4798206278026906\n",
      "Percentage of terminated:  0.512591389114541\n",
      "Percentage of terminated:  0.5725346373268133\n",
      "Percentage of terminated:  0.5177914110429448\n",
      "Percentage of terminated:  0.5060777957860616\n",
      "Percentage of terminated:  0.5257774140752864\n",
      "Percentage of terminated:  0.5264873675631622\n",
      "Percentage of terminated:  0.5156567710451403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5238828967642527\n",
      "Epoch 10, Train Loss: 2.7860, Validation Loss: 2.7436\n",
      "Percentage of terminated:  0.5381184103811841\n",
      "Percentage of terminated:  0.5422621478154349\n",
      "Percentage of terminated:  0.5314285714285715\n",
      "Percentage of terminated:  0.5004088307440719\n",
      "Percentage of terminated:  0.524098825435399\n",
      "Percentage of terminated:  0.530346232179226\n",
      "Percentage of terminated:  0.521810028536486\n",
      "Percentage of terminated:  0.5170731707317073\n",
      "Percentage of terminated:  0.5552835577315381\n",
      "Percentage of terminated:  0.5332517339861281\n",
      "Percentage of terminated:  0.5353946297803092\n",
      "Percentage of terminated:  0.5349025974025974\n",
      "Percentage of terminated:  0.4967373572593801\n",
      "Percentage of terminated:  0.5104551045510455\n",
      "Percentage of terminated:  0.5540041067761807\n",
      "Percentage of terminated:  0.5196914332115307\n",
      "Percentage of terminated:  0.533469387755102\n",
      "Percentage of terminated:  0.5519080837094789\n",
      "Percentage of terminated:  0.5144485144485145\n",
      "Percentage of terminated:  0.5395653956539566\n",
      "Percentage of terminated:  0.48128559804719284\n",
      "Percentage of terminated:  0.5372405372405372\n",
      "Percentage of terminated:  0.5354394491697043\n",
      "Percentage of terminated:  0.5248776508972267\n",
      "Percentage of terminated:  0.5171708912510221\n",
      "Percentage of terminated:  0.510569105691057\n",
      "Percentage of terminated:  0.552739165985282\n",
      "Percentage of terminated:  0.5635696821515892\n",
      "Percentage of terminated:  0.48199672667757776\n",
      "Percentage of terminated:  0.5297716150081566\n",
      "Percentage of terminated:  0.518352365415987\n",
      "Percentage of terminated:  0.5306039724361573\n",
      "Percentage of terminated:  0.5366449511400652\n",
      "Percentage of terminated:  0.5543300653594772\n",
      "Percentage of terminated:  0.5542903619357462\n",
      "Percentage of terminated:  0.49757085020242914\n",
      "Percentage of terminated:  0.4928948436865611\n",
      "Percentage of terminated:  0.5544231553200163\n",
      "Percentage of terminated:  0.5151143790849673\n",
      "Percentage of terminated:  0.5107942973523422\n",
      "Percentage of terminated:  0.5255060728744939\n",
      "Percentage of terminated:  0.48862713241267264\n",
      "Percentage of terminated:  0.5433360588716272\n",
      "Percentage of terminated:  0.5402533714752759\n",
      "Percentage of terminated:  0.48986212489862124\n",
      "Percentage of terminated:  0.539714867617108\n",
      "Percentage of terminated:  0.5879478827361564\n",
      "Percentage of terminated:  0.5417680454176804\n",
      "Percentage of terminated:  0.5223033252230332\n",
      "Percentage of terminated:  0.5271761340416837\n",
      "Percentage of terminated:  0.5551455514555146\n",
      "Percentage of terminated:  0.5458281956432388\n",
      "Percentage of terminated:  0.5177045177045178\n",
      "Percentage of terminated:  0.5546459271387638\n",
      "Percentage of terminated:  0.5315682281059063\n",
      "Percentage of terminated:  0.5585585585585585\n",
      "Percentage of terminated:  0.5408038976857491\n",
      "Percentage of terminated:  0.5603765861645518\n",
      "Percentage of terminated:  0.5327902240325866\n",
      "Percentage of terminated:  0.5263157894736842\n",
      "Percentage of terminated:  0.5347137637028014\n",
      "Percentage of terminated:  0.5341741253051261\n",
      "Percentage of terminated:  0.5297231270358306\n",
      "Percentage of terminated:  0.5067319461444308\n",
      "Percentage of terminated:  0.5294835298901993\n",
      "Percentage of terminated:  0.5483739837398374\n",
      "Percentage of terminated:  0.519559902200489\n",
      "Percentage of terminated:  0.5726004922067268\n",
      "Percentage of terminated:  0.5275397796817626\n",
      "Percentage of terminated:  0.5207823960880196\n",
      "Percentage of terminated:  0.5171568627450981\n",
      "Percentage of terminated:  0.5323302155347702\n",
      "Percentage of terminated:  0.53173311635476\n",
      "Percentage of terminated:  0.5449691991786447\n",
      "Percentage of terminated:  0.547070707070707\n",
      "Percentage of terminated:  0.5161686451084732\n",
      "Percentage of terminated:  0.5388071895424836\n",
      "Percentage of terminated:  0.5307125307125307\n",
      "Percentage of terminated:  0.41883116883116883\n",
      "Percentage of terminated:  0.5077361563517915\n",
      "Percentage of terminated:  0.5417007358953393\n",
      "Percentage of terminated:  0.5282555282555282\n",
      "Percentage of terminated:  0.536743808363784\n",
      "Percentage of terminated:  0.5521172638436482\n",
      "Percentage of terminated:  0.5237901586010574\n",
      "Percentage of terminated:  0.5109135004042037\n",
      "Percentage of terminated:  0.49150485436893204\n",
      "Percentage of terminated:  0.5297231270358306\n",
      "Percentage of terminated:  0.503037667071689\n",
      "Percentage of terminated:  0.5433455433455433\n",
      "Percentage of terminated:  0.5698619008935825\n",
      "Percentage of terminated:  0.5016299918500408\n",
      "Percentage of terminated:  0.5329243353783231\n",
      "Percentage of terminated:  0.5089503661513426\n",
      "Percentage of terminated:  0.5010158472165787\n",
      "Percentage of terminated:  0.5283557731538148\n",
      "Percentage of terminated:  0.5583777140516182\n",
      "Percentage of terminated:  0.5018315018315018\n",
      "Percentage of terminated:  0.5154559505409583\n",
      "Epoch 11, Train Loss: 2.7599, Validation Loss: 2.8004\n",
      "Percentage of terminated:  0.5527823240589198\n",
      "Percentage of terminated:  0.5020325203252033\n",
      "Percentage of terminated:  0.5311608961303462\n",
      "Percentage of terminated:  0.5186385737439222\n",
      "Percentage of terminated:  0.5105777054515866\n",
      "Percentage of terminated:  0.5589555283557731\n",
      "Percentage of terminated:  0.5251328156926849\n",
      "Percentage of terminated:  0.5339052287581699\n",
      "Percentage of terminated:  0.4902439024390244\n",
      "Percentage of terminated:  0.5128414186710151\n",
      "Percentage of terminated:  0.5612535612535613\n",
      "Percentage of terminated:  0.5198201879852881\n",
      "Percentage of terminated:  0.5549695740365111\n",
      "Percentage of terminated:  0.534637326813366\n",
      "Percentage of terminated:  0.5008130081300813\n",
      "Percentage of terminated:  0.5378632828489562\n",
      "Percentage of terminated:  0.5322448979591837\n",
      "Percentage of terminated:  0.5024291497975708\n",
      "Percentage of terminated:  0.5388685388685389\n",
      "Percentage of terminated:  0.5251739664347114\n",
      "Percentage of terminated:  0.5189409368635438\n",
      "Percentage of terminated:  0.5389926888708367\n",
      "Percentage of terminated:  0.4936501433838591\n",
      "Percentage of terminated:  0.5111381125961928\n",
      "Percentage of terminated:  0.5186839967506093\n",
      "Percentage of terminated:  0.46764346764346765\n",
      "Percentage of terminated:  0.5165238678090576\n",
      "Percentage of terminated:  0.5073349633251834\n",
      "Percentage of terminated:  0.5331715210355987\n",
      "Percentage of terminated:  0.5059353254195661\n",
      "Percentage of terminated:  0.5416666666666666\n",
      "Percentage of terminated:  0.5284090909090909\n",
      "Percentage of terminated:  0.5474068071312804\n",
      "Percentage of terminated:  0.5282176207876573\n",
      "Percentage of terminated:  0.5106122448979592\n",
      "Percentage of terminated:  0.5349219391947412\n",
      "Percentage of terminated:  0.5332788893425888\n",
      "Percentage of terminated:  0.5198059822150364\n",
      "Percentage of terminated:  0.5097799511002445\n",
      "Percentage of terminated:  0.524603497356649\n",
      "Percentage of terminated:  0.5373012637586628\n",
      "Percentage of terminated:  0.5385242560130452\n",
      "Percentage of terminated:  0.5299595141700405\n",
      "Percentage of terminated:  0.5464212678936605\n",
      "Percentage of terminated:  0.5255206206614945\n",
      "Percentage of terminated:  0.5447486718430732\n",
      "Percentage of terminated:  0.5089285714285714\n",
      "Percentage of terminated:  0.5232273838630807\n",
      "Percentage of terminated:  0.5540983606557377\n",
      "Percentage of terminated:  0.504885993485342\n",
      "Percentage of terminated:  0.5351594439901881\n",
      "Percentage of terminated:  0.5358735306039725\n",
      "Percentage of terminated:  0.5204750204750205\n",
      "Percentage of terminated:  0.5028478437754271\n",
      "Percentage of terminated:  0.5251328156926849\n",
      "Percentage of terminated:  0.5471087747674889\n",
      "Percentage of terminated:  0.4912209064924459\n",
      "Percentage of terminated:  0.4830097087378641\n",
      "Percentage of terminated:  0.5272357723577236\n",
      "Percentage of terminated:  0.5040683482506102\n",
      "Percentage of terminated:  0.5118852459016393\n",
      "Percentage of terminated:  0.5144016227180528\n",
      "Percentage of terminated:  0.5171708912510221\n",
      "Percentage of terminated:  0.5375203915171288\n",
      "Percentage of terminated:  0.4945233265720081\n",
      "Percentage of terminated:  0.4859470468431772\n",
      "Percentage of terminated:  0.5376782077393075\n",
      "Percentage of terminated:  0.5124439004487964\n",
      "Percentage of terminated:  0.5258481421647819\n",
      "Percentage of terminated:  0.5173682059664896\n",
      "Percentage of terminated:  0.5350344268934791\n",
      "Percentage of terminated:  0.5613534447615165\n",
      "Percentage of terminated:  0.4989816700610998\n",
      "Percentage of terminated:  0.5361667347772783\n",
      "Percentage of terminated:  0.49897750511247446\n",
      "Percentage of terminated:  0.5615823817292006\n",
      "Percentage of terminated:  0.5615038823048631\n",
      "Percentage of terminated:  0.5463038180341186\n",
      "Percentage of terminated:  0.4483870967741935\n",
      "Percentage of terminated:  0.5248371335504886\n",
      "Percentage of terminated:  0.5298659081674116\n",
      "Percentage of terminated:  0.47243772968558595\n",
      "Percentage of terminated:  0.5181855333060891\n",
      "Percentage of terminated:  0.5448639870077142\n",
      "Percentage of terminated:  0.5358306188925082\n",
      "Percentage of terminated:  0.5259077927376581\n",
      "Percentage of terminated:  0.5313131313131313\n",
      "Percentage of terminated:  0.5428687525396181\n",
      "Percentage of terminated:  0.5048740861088545\n",
      "Percentage of terminated:  0.5168905168905169\n",
      "Percentage of terminated:  0.5038633590890605\n",
      "Percentage of terminated:  0.5099633997559984\n",
      "Percentage of terminated:  0.5280259951259139\n",
      "Percentage of terminated:  0.5060876623376623\n",
      "Percentage of terminated:  0.5516680227827502\n",
      "Percentage of terminated:  0.5278116118554608\n",
      "Percentage of terminated:  0.5263372805226623\n",
      "Percentage of terminated:  0.5217391304347826\n",
      "Percentage of terminated:  0.5042340261739799\n",
      "Epoch 12, Train Loss: 2.7755, Validation Loss: 2.7934\n",
      "Percentage of terminated:  0.5227272727272727\n",
      "Percentage of terminated:  0.5363969093127288\n",
      "Percentage of terminated:  0.5081499592502038\n",
      "Percentage of terminated:  0.5402862985685072\n",
      "Percentage of terminated:  0.5177768696362893\n",
      "Percentage of terminated:  0.5309446254071661\n",
      "Percentage of terminated:  0.5673904137648504\n",
      "Percentage of terminated:  0.5690290864399836\n",
      "Percentage of terminated:  0.5121654501216545\n",
      "Percentage of terminated:  0.5165373621886484\n",
      "Percentage of terminated:  0.518170681910984\n",
      "Percentage of terminated:  0.5242165242165242\n",
      "Percentage of terminated:  0.5426482534524777\n",
      "Percentage of terminated:  0.5172553796183516\n",
      "Percentage of terminated:  0.5498981670061099\n",
      "Percentage of terminated:  0.5859663520722199\n",
      "Percentage of terminated:  0.5186385737439222\n",
      "Percentage of terminated:  0.520064856100527\n",
      "Percentage of terminated:  0.5519348268839104\n",
      "Percentage of terminated:  0.51925415484394\n",
      "Percentage of terminated:  0.5232843137254902\n",
      "Percentage of terminated:  0.5032388663967612\n",
      "Percentage of terminated:  0.5272357723577236\n",
      "Percentage of terminated:  0.5355102040816326\n",
      "Percentage of terminated:  0.5629599345870809\n",
      "Percentage of terminated:  0.532953620829943\n",
      "Percentage of terminated:  0.504684317718941\n",
      "Percentage of terminated:  0.5113821138211382\n",
      "Percentage of terminated:  0.5463076295389637\n",
      "Percentage of terminated:  0.4947154471544715\n",
      "Percentage of terminated:  0.5149313962873285\n",
      "Percentage of terminated:  0.5299145299145299\n",
      "Percentage of terminated:  0.5205034510759237\n",
      "Percentage of terminated:  0.5173257236037505\n",
      "Percentage of terminated:  0.5221857025472473\n",
      "Percentage of terminated:  0.5309698451507743\n",
      "Percentage of terminated:  0.551440329218107\n",
      "Percentage of terminated:  0.5211554109031733\n",
      "Percentage of terminated:  0.5381803411860276\n",
      "Percentage of terminated:  0.509979633401222\n",
      "Percentage of terminated:  0.5223577235772358\n",
      "Percentage of terminated:  0.5294117647058824\n",
      "Percentage of terminated:  0.5494908350305499\n",
      "Percentage of terminated:  0.5247241520228851\n",
      "Percentage of terminated:  0.5364562118126273\n",
      "Percentage of terminated:  0.5461945461945462\n",
      "Percentage of terminated:  0.5369018653690186\n",
      "Percentage of terminated:  0.5496742671009772\n",
      "Percentage of terminated:  0.526573998364677\n",
      "Percentage of terminated:  0.5119675456389452\n",
      "Percentage of terminated:  0.5465686274509803\n",
      "Percentage of terminated:  0.5343915343915344\n",
      "Percentage of terminated:  0.5403094462540716\n",
      "Percentage of terminated:  0.5422621478154349\n",
      "Percentage of terminated:  0.5316091954022989\n",
      "Percentage of terminated:  0.5286468915075173\n",
      "Percentage of terminated:  0.5083164300202839\n",
      "Percentage of terminated:  0.5215622457282343\n",
      "Percentage of terminated:  0.4943089430894309\n",
      "Percentage of terminated:  0.5407166123778502\n",
      "Percentage of terminated:  0.5370219690805533\n",
      "Percentage of terminated:  0.5571895424836601\n",
      "Percentage of terminated:  0.5244498777506112\n",
      "Percentage of terminated:  0.5524275805793554\n",
      "Percentage of terminated:  0.4857259380097879\n",
      "Percentage of terminated:  0.5206882425235559\n",
      "Percentage of terminated:  0.5300081103000811\n",
      "Percentage of terminated:  0.5089141004862237\n",
      "Percentage of terminated:  0.4949124949124949\n",
      "Percentage of terminated:  0.5211898940505297\n",
      "Percentage of terminated:  0.5192854242793341\n",
      "Percentage of terminated:  0.5203748981255094\n",
      "Percentage of terminated:  0.5329825981384055\n",
      "Percentage of terminated:  0.5546459271387638\n",
      "Percentage of terminated:  0.5350806451612903\n",
      "Percentage of terminated:  0.504684317718941\n",
      "Percentage of terminated:  0.5635540929658577\n",
      "Percentage of terminated:  0.5354523227383863\n",
      "Percentage of terminated:  0.5048543689320388\n",
      "Percentage of terminated:  0.5514705882352942\n",
      "Percentage of terminated:  0.5385556915544676\n",
      "Percentage of terminated:  0.5482295482295483\n",
      "Percentage of terminated:  0.5538336052202284\n",
      "Percentage of terminated:  0.5279249898083979\n",
      "Percentage of terminated:  0.49959546925566345\n",
      "Percentage of terminated:  0.49817739975698666\n",
      "Percentage of terminated:  0.49774866966844045\n",
      "Percentage of terminated:  0.4752836304700162\n",
      "Percentage of terminated:  0.5425790754257908\n",
      "Percentage of terminated:  0.5256933115823818\n",
      "Percentage of terminated:  0.5287872601061658\n",
      "Percentage of terminated:  0.48998774008990603\n",
      "Percentage of terminated:  0.5256723716381418\n",
      "Percentage of terminated:  0.5592668024439919\n",
      "Percentage of terminated:  0.5349979533360623\n",
      "Percentage of terminated:  0.5205368035786905\n",
      "Percentage of terminated:  0.5073409461663948\n",
      "Percentage of terminated:  0.5099553027224706\n",
      "Percentage of terminated:  0.5103766333589547\n",
      "Epoch 13, Train Loss: 2.7731, Validation Loss: 2.7782\n",
      "Percentage of terminated:  0.5445302968686457\n",
      "Percentage of terminated:  0.5331152902698283\n",
      "Percentage of terminated:  0.5327902240325866\n",
      "Percentage of terminated:  0.5535714285714286\n",
      "Percentage of terminated:  0.547085201793722\n",
      "Percentage of terminated:  0.5114099429502853\n",
      "Percentage of terminated:  0.5807504078303426\n",
      "Percentage of terminated:  0.5601304525071341\n",
      "Percentage of terminated:  0.5448247758761207\n",
      "Percentage of terminated:  0.4957644211375555\n",
      "Percentage of terminated:  0.5134146341463415\n",
      "Percentage of terminated:  0.5296766270978306\n",
      "Percentage of terminated:  0.48895253682487727\n",
      "Percentage of terminated:  0.5558727569331158\n",
      "Percentage of terminated:  0.5182153090462546\n",
      "Percentage of terminated:  0.5283326538931921\n",
      "Percentage of terminated:  0.5400654129190515\n",
      "Percentage of terminated:  0.4833468724614135\n",
      "Percentage of terminated:  0.49796747967479676\n",
      "Percentage of terminated:  0.5210118319053447\n",
      "Percentage of terminated:  0.5433455433455433\n",
      "Percentage of terminated:  0.5643524157531465\n",
      "Percentage of terminated:  0.5026455026455027\n",
      "Percentage of terminated:  0.5383048084759576\n",
      "Percentage of terminated:  0.549043549043549\n",
      "Percentage of terminated:  0.5225701504676697\n",
      "Percentage of terminated:  0.5626786443446304\n",
      "Percentage of terminated:  0.4926829268292683\n",
      "Percentage of terminated:  0.5483739837398374\n",
      "Percentage of terminated:  0.5057236304170074\n",
      "Percentage of terminated:  0.5071516142214957\n",
      "Percentage of terminated:  0.5088150881508815\n",
      "Percentage of terminated:  0.496149169031212\n",
      "Percentage of terminated:  0.5362556329373208\n",
      "Percentage of terminated:  0.5002043318348999\n",
      "Percentage of terminated:  0.5245835026412028\n",
      "Percentage of terminated:  0.523538961038961\n",
      "Percentage of terminated:  0.5093648208469055\n",
      "Percentage of terminated:  0.5162074554294975\n",
      "Percentage of terminated:  0.5240423797881011\n",
      "Percentage of terminated:  0.5408370581064608\n",
      "Percentage of terminated:  0.5190901705930138\n",
      "Percentage of terminated:  0.4945233265720081\n",
      "Percentage of terminated:  0.5266393442622951\n",
      "Percentage of terminated:  0.5252442996742671\n",
      "Percentage of terminated:  0.5261865793780688\n",
      "Percentage of terminated:  0.5016220600162206\n",
      "Percentage of terminated:  0.525071341214839\n",
      "Percentage of terminated:  0.5089795918367347\n",
      "Percentage of terminated:  0.5468622656886716\n",
      "Percentage of terminated:  0.5522995522995523\n",
      "Percentage of terminated:  0.5512611879576892\n",
      "Percentage of terminated:  0.48491027732463293\n",
      "Percentage of terminated:  0.518925518925519\n",
      "Percentage of terminated:  0.5339845339845339\n",
      "Percentage of terminated:  0.5222672064777328\n",
      "Percentage of terminated:  0.5149162239476911\n",
      "Percentage of terminated:  0.5233076611268748\n",
      "Percentage of terminated:  0.5168032786885246\n",
      "Percentage of terminated:  0.5180921052631579\n",
      "Percentage of terminated:  0.5198689061859894\n",
      "Percentage of terminated:  0.48048780487804876\n",
      "Percentage of terminated:  0.5234627831715211\n",
      "Percentage of terminated:  0.48266013871889024\n",
      "Percentage of terminated:  0.5640194489465153\n",
      "Percentage of terminated:  0.5253755582622818\n",
      "Percentage of terminated:  0.5102291325695582\n",
      "Percentage of terminated:  0.5239253852392538\n",
      "Percentage of terminated:  0.520259319286872\n",
      "Percentage of terminated:  0.5587637250915006\n",
      "Percentage of terminated:  0.5050999592003264\n",
      "Percentage of terminated:  0.5259077927376581\n",
      "Percentage of terminated:  0.5343417825020441\n",
      "Percentage of terminated:  0.4814663951120163\n",
      "Percentage of terminated:  0.5018389865140989\n",
      "Percentage of terminated:  0.518170681910984\n",
      "Percentage of terminated:  0.5077677841373671\n",
      "Percentage of terminated:  0.5376169174461163\n",
      "Percentage of terminated:  0.5737704918032787\n",
      "Percentage of terminated:  0.5159574468085106\n",
      "Percentage of terminated:  0.5128623928133932\n",
      "Percentage of terminated:  0.509009009009009\n",
      "Percentage of terminated:  0.48029256399837467\n",
      "Percentage of terminated:  0.5274278748476229\n",
      "Percentage of terminated:  0.5189101260675071\n",
      "Percentage of terminated:  0.4654752233956133\n",
      "Percentage of terminated:  0.5202288516550878\n",
      "Percentage of terminated:  0.5235366352844862\n",
      "Percentage of terminated:  0.5204081632653061\n",
      "Percentage of terminated:  0.5577705451586655\n",
      "Percentage of terminated:  0.5177189409368635\n",
      "Percentage of terminated:  0.5431492842535788\n",
      "Percentage of terminated:  0.538430256201708\n",
      "Percentage of terminated:  0.508585445625511\n",
      "Percentage of terminated:  0.5203915171288744\n",
      "Percentage of terminated:  0.4819032126880846\n",
      "Percentage of terminated:  0.4967479674796748\n",
      "Percentage of terminated:  0.5018329938900203\n",
      "Percentage of terminated:  0.5352760736196319\n",
      "Epoch 14, Train Loss: 2.7685, Validation Loss: 2.7989\n",
      "Percentage of terminated:  0.5174938974776241\n",
      "Percentage of terminated:  0.5606183889340928\n",
      "Percentage of terminated:  0.5443037974683544\n",
      "Percentage of terminated:  0.5222672064777328\n",
      "Percentage of terminated:  0.5263157894736842\n",
      "Percentage of terminated:  0.5006080259424402\n",
      "Percentage of terminated:  0.5593151243375458\n",
      "Percentage of terminated:  0.5336872192731728\n",
      "Percentage of terminated:  0.5140644109253975\n",
      "Percentage of terminated:  0.5815746753246753\n",
      "Percentage of terminated:  0.5321589512494879\n",
      "Percentage of terminated:  0.5004081632653061\n",
      "Percentage of terminated:  0.5121753246753247\n",
      "Percentage of terminated:  0.533469387755102\n",
      "Percentage of terminated:  0.5412919051512673\n",
      "Percentage of terminated:  0.5059111292295149\n",
      "Percentage of terminated:  0.523673469387755\n",
      "Percentage of terminated:  0.5142276422764228\n",
      "Percentage of terminated:  0.546345447121274\n",
      "Percentage of terminated:  0.5556915544675642\n",
      "Percentage of terminated:  0.5653061224489796\n",
      "Percentage of terminated:  0.5196595054722335\n",
      "Percentage of terminated:  0.5472697636511817\n",
      "Percentage of terminated:  0.5071109305160504\n",
      "Percentage of terminated:  0.5494908350305499\n",
      "Percentage of terminated:  0.5181262729124236\n",
      "Percentage of terminated:  0.5114192495921697\n",
      "Percentage of terminated:  0.5620437956204379\n",
      "Percentage of terminated:  0.5645292207792207\n",
      "Percentage of terminated:  0.5273611674098095\n",
      "Percentage of terminated:  0.5026262626262626\n",
      "Percentage of terminated:  0.5285714285714286\n",
      "Percentage of terminated:  0.5272801302931596\n",
      "Percentage of terminated:  0.49634443541835904\n",
      "Percentage of terminated:  0.5310204081632653\n",
      "Percentage of terminated:  0.5399594320486816\n",
      "Percentage of terminated:  0.5333333333333333\n",
      "Percentage of terminated:  0.5524275805793554\n",
      "Percentage of terminated:  0.5366748166259169\n",
      "Percentage of terminated:  0.5417007358953393\n",
      "Percentage of terminated:  0.4932349323493235\n",
      "Percentage of terminated:  0.5277324632952691\n",
      "Percentage of terminated:  0.5217568117120781\n",
      "Percentage of terminated:  0.5\n",
      "Percentage of terminated:  0.48609975470155353\n",
      "Percentage of terminated:  0.5091352009744214\n",
      "Percentage of terminated:  0.5318887980376125\n",
      "Percentage of terminated:  0.5071167141114274\n",
      "Percentage of terminated:  0.5470085470085471\n",
      "Percentage of terminated:  0.5234981610134859\n",
      "Percentage of terminated:  0.5365353037766831\n",
      "Percentage of terminated:  0.5318013951579811\n",
      "Percentage of terminated:  0.5350162866449512\n",
      "Percentage of terminated:  0.5582444626743233\n",
      "Percentage of terminated:  0.467860048820179\n",
      "Percentage of terminated:  0.5471698113207547\n",
      "Percentage of terminated:  0.5196754563894523\n",
      "Percentage of terminated:  0.5413779046066042\n",
      "Percentage of terminated:  0.5032599837000815\n",
      "Percentage of terminated:  0.5140644109253975\n",
      "Percentage of terminated:  0.49979550102249487\n",
      "Percentage of terminated:  0.5223214285714286\n",
      "Percentage of terminated:  0.5526855268552685\n",
      "Percentage of terminated:  0.5004065040650406\n",
      "Percentage of terminated:  0.5244299674267101\n",
      "Percentage of terminated:  0.5342019543973942\n",
      "Percentage of terminated:  0.5012116316639742\n",
      "Percentage of terminated:  0.5412392285597045\n",
      "Percentage of terminated:  0.5111111111111111\n",
      "Percentage of terminated:  0.5273469387755102\n",
      "Percentage of terminated:  0.49167681688997156\n",
      "Percentage of terminated:  0.5279805352798054\n",
      "Percentage of terminated:  0.47952979327117956\n",
      "Percentage of terminated:  0.5243353783231084\n",
      "Percentage of terminated:  0.5185487158581329\n",
      "Percentage of terminated:  0.5384299219071106\n",
      "Percentage of terminated:  0.5234248788368336\n",
      "Percentage of terminated:  0.5331975560081467\n",
      "Percentage of terminated:  0.5921052631578947\n",
      "Percentage of terminated:  0.5269828291087489\n",
      "Percentage of terminated:  0.5286236297198539\n",
      "Percentage of terminated:  0.5294835298901993\n",
      "Percentage of terminated:  0.5054989816700611\n",
      "Percentage of terminated:  0.4979641693811075\n",
      "Percentage of terminated:  0.497165991902834\n",
      "Percentage of terminated:  0.5040783034257749\n",
      "Percentage of terminated:  0.49693752552062065\n",
      "Percentage of terminated:  0.5265322912381736\n",
      "Percentage of terminated:  0.5153751537515375\n",
      "Percentage of terminated:  0.5773785218456513\n",
      "Percentage of terminated:  0.5483739837398374\n",
      "Percentage of terminated:  0.5047073270568972\n",
      "Percentage of terminated:  0.5387123064384678\n",
      "Percentage of terminated:  0.5063136456211813\n",
      "Percentage of terminated:  0.5566925910765452\n",
      "Percentage of terminated:  0.5179007323026851\n",
      "Percentage of terminated:  0.5477941176470589\n",
      "Percentage of terminated:  0.5163372327551432\n",
      "Percentage of terminated:  0.5091883614088821\n",
      "Epoch 15, Train Loss: 2.7551, Validation Loss: 2.7609\n",
      "Percentage of terminated:  0.5899427636958299\n",
      "Percentage of terminated:  0.5349122090649244\n",
      "Percentage of terminated:  0.535234215885947\n",
      "Percentage of terminated:  0.515827922077922\n",
      "Percentage of terminated:  0.5289087947882736\n",
      "Percentage of terminated:  0.5177333876885446\n",
      "Percentage of terminated:  0.49002849002849\n",
      "Percentage of terminated:  0.5358598207008965\n",
      "Percentage of terminated:  0.5234406848756624\n",
      "Percentage of terminated:  0.5455287872601061\n",
      "Percentage of terminated:  0.5509541209906618\n",
      "Percentage of terminated:  0.5065093572009765\n",
      "Percentage of terminated:  0.525050916496945\n",
      "Percentage of terminated:  0.569672131147541\n",
      "Percentage of terminated:  0.5054766734279919\n",
      "Percentage of terminated:  0.5097879282218597\n",
      "Percentage of terminated:  0.5283095723014256\n",
      "Percentage of terminated:  0.5365556458164095\n",
      "Percentage of terminated:  0.5510785510785511\n",
      "Percentage of terminated:  0.5286236297198539\n",
      "Percentage of terminated:  0.5444489963129865\n",
      "Percentage of terminated:  0.5304136253041363\n",
      "Percentage of terminated:  0.5193798449612403\n",
      "Percentage of terminated:  0.5008136696501221\n",
      "Percentage of terminated:  0.5457503050020334\n",
      "Percentage of terminated:  0.49796747967479676\n",
      "Percentage of terminated:  0.4928948436865611\n",
      "Percentage of terminated:  0.5271255060728745\n",
      "Percentage of terminated:  0.5354362965997542\n",
      "Percentage of terminated:  0.5433006535947712\n",
      "Percentage of terminated:  0.5217391304347826\n",
      "Percentage of terminated:  0.5360908353609084\n",
      "Percentage of terminated:  0.5631385369840621\n",
      "Percentage of terminated:  0.5226253567060742\n",
      "Percentage of terminated:  0.5308441558441559\n",
      "Percentage of terminated:  0.5603413246647704\n",
      "Percentage of terminated:  0.5728354534263439\n",
      "Percentage of terminated:  0.5069501226492232\n",
      "Percentage of terminated:  0.5225701504676697\n",
      "Percentage of terminated:  0.48029256399837467\n",
      "Percentage of terminated:  0.5163532297628781\n",
      "Percentage of terminated:  0.5048543689320388\n",
      "Percentage of terminated:  0.5403752039151712\n",
      "Percentage of terminated:  0.5584045584045584\n",
      "Percentage of terminated:  0.5587275693311582\n",
      "Percentage of terminated:  0.563162184189079\n",
      "Percentage of terminated:  0.5379591836734694\n",
      "Percentage of terminated:  0.5701431492842536\n",
      "Percentage of terminated:  0.5022312373225152\n",
      "Percentage of terminated:  0.5423312883435583\n",
      "Percentage of terminated:  0.5145075602778912\n",
      "Percentage of terminated:  0.511779041429732\n",
      "Percentage of terminated:  0.5075111652456354\n",
      "Percentage of terminated:  0.5452683326505531\n",
      "Percentage of terminated:  0.5297473512632437\n",
      "Percentage of terminated:  0.5279578606158833\n",
      "Percentage of terminated:  0.5622457282343368\n",
      "Percentage of terminated:  0.5193798449612403\n",
      "Percentage of terminated:  0.5446791226645004\n",
      "Percentage of terminated:  0.5116374030216415\n",
      "Percentage of terminated:  0.536743808363784\n",
      "Percentage of terminated:  0.488663967611336\n",
      "Percentage of terminated:  0.5048780487804878\n",
      "Percentage of terminated:  0.5560556464811784\n",
      "Percentage of terminated:  0.5153721682847896\n",
      "Percentage of terminated:  0.5177705977382876\n",
      "Percentage of terminated:  0.5109400324149108\n",
      "Percentage of terminated:  0.5014268242967794\n",
      "Percentage of terminated:  0.49122807017543857\n",
      "Percentage of terminated:  0.4989866234292663\n",
      "Percentage of terminated:  0.5006065507480792\n",
      "Percentage of terminated:  0.564437194127243\n",
      "Percentage of terminated:  0.547261663286004\n",
      "Percentage of terminated:  0.5443400081732734\n",
      "Percentage of terminated:  0.5290532303941488\n",
      "Percentage of terminated:  0.5175081433224755\n",
      "Percentage of terminated:  0.5339329517579722\n",
      "Percentage of terminated:  0.560064935064935\n",
      "Percentage of terminated:  0.45307443365695793\n",
      "Percentage of terminated:  0.4931118314424635\n",
      "Percentage of terminated:  0.5093648208469055\n",
      "Percentage of terminated:  0.5404636030906873\n",
      "Percentage of terminated:  0.49959250203748984\n",
      "Percentage of terminated:  0.5321138211382114\n",
      "Percentage of terminated:  0.5260162601626016\n",
      "Percentage of terminated:  0.496532027743778\n",
      "Percentage of terminated:  0.5141104294478528\n",
      "Percentage of terminated:  0.5182333873581848\n",
      "Percentage of terminated:  0.5053191489361702\n",
      "Percentage of terminated:  0.5120359037127703\n",
      "Percentage of terminated:  0.5517805976258698\n",
      "Percentage of terminated:  0.4934906427990236\n",
      "Percentage of terminated:  0.4865853658536585\n",
      "Percentage of terminated:  0.53330608908868\n",
      "Percentage of terminated:  0.5251122907309106\n",
      "Percentage of terminated:  0.5536585365853659\n",
      "Percentage of terminated:  0.511646914589293\n",
      "Percentage of terminated:  0.5244498777506112\n",
      "Percentage of terminated:  0.5165767154973014\n",
      "Epoch 16, Train Loss: 2.7585, Validation Loss: 2.8086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 11:33:27.684825: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.4928774928774929\n",
      "Percentage of terminated:  0.5432399512789281\n",
      "Percentage of terminated:  0.509105625252934\n",
      "Percentage of terminated:  0.5161290322580645\n",
      "Percentage of terminated:  0.5002026753141467\n",
      "Percentage of terminated:  0.513623424156161\n",
      "Percentage of terminated:  0.5704863973619126\n",
      "Percentage of terminated:  0.5224489795918368\n",
      "Percentage of terminated:  0.5281632653061225\n",
      "Percentage of terminated:  0.5543300653594772\n",
      "Percentage of terminated:  0.5336872192731728\n",
      "Percentage of terminated:  0.5820040899795501\n",
      "Percentage of terminated:  0.5331695331695332\n",
      "Percentage of terminated:  0.5102291325695582\n",
      "Percentage of terminated:  0.5679414157851912\n",
      "Percentage of terminated:  0.5245635403978888\n",
      "Percentage of terminated:  0.4875864875864876\n",
      "Percentage of terminated:  0.5616830065359477\n",
      "Percentage of terminated:  0.5294117647058824\n",
      "Percentage of terminated:  0.5523848348960456\n",
      "Percentage of terminated:  0.5392798690671031\n",
      "Percentage of terminated:  0.5319755600814664\n",
      "Percentage of terminated:  0.5426356589147286\n",
      "Percentage of terminated:  0.5008176614881439\n",
      "Percentage of terminated:  0.517479674796748\n",
      "Percentage of terminated:  0.5279934613812832\n",
      "Percentage of terminated:  0.5185336048879837\n",
      "Percentage of terminated:  0.5101626016260162\n",
      "Percentage of terminated:  0.5110114192495921\n",
      "Percentage of terminated:  0.515077424612877\n",
      "Percentage of terminated:  0.5093572009764036\n",
      "Percentage of terminated:  0.5160503860219423\n",
      "Percentage of terminated:  0.5545565999182672\n",
      "Percentage of terminated:  0.5243704305442729\n",
      "Percentage of terminated:  0.5369918699186992\n",
      "Percentage of terminated:  0.5161290322580645\n",
      "Percentage of terminated:  0.5463791700569569\n",
      "Percentage of terminated:  0.5433102887352582\n",
      "Percentage of terminated:  0.5251533742331288\n",
      "Percentage of terminated:  0.509009009009009\n",
      "Percentage of terminated:  0.5215272136474411\n",
      "Percentage of terminated:  0.5156822810590631\n",
      "Percentage of terminated:  0.5164567249085737\n",
      "Percentage of terminated:  0.5319235461569743\n",
      "Percentage of terminated:  0.5556458164094232\n",
      "Percentage of terminated:  0.5105777054515866\n",
      "Percentage of terminated:  0.48819218241042345\n",
      "Percentage of terminated:  0.5305202785743548\n",
      "Percentage of terminated:  0.5128623928133932\n",
      "Percentage of terminated:  0.525499796001632\n",
      "Percentage of terminated:  0.5206713057715923\n",
      "Percentage of terminated:  0.5387438825448614\n",
      "Percentage of terminated:  0.5367647058823529\n",
      "Percentage of terminated:  0.5358306188925082\n",
      "Percentage of terminated:  0.5083503054989816\n",
      "Percentage of terminated:  0.5365853658536586\n",
      "Percentage of terminated:  0.5048740861088545\n",
      "Percentage of terminated:  0.45801217038539555\n",
      "Percentage of terminated:  0.5401459854014599\n",
      "Percentage of terminated:  0.509915014164306\n",
      "Percentage of terminated:  0.5721816707218167\n",
      "Percentage of terminated:  0.5419039869812856\n",
      "Percentage of terminated:  0.5012175324675324\n",
      "Percentage of terminated:  0.5466775377089278\n",
      "Percentage of terminated:  0.535931790499391\n",
      "Percentage of terminated:  0.5444579780755177\n",
      "Percentage of terminated:  0.5152377082486794\n",
      "Percentage of terminated:  0.5338775510204081\n",
      "Percentage of terminated:  0.5020441537203598\n",
      "Percentage of terminated:  0.5489638358390898\n",
      "Percentage of terminated:  0.5144957125357289\n",
      "Percentage of terminated:  0.4918300653594771\n",
      "Percentage of terminated:  0.4969499796665311\n",
      "Percentage of terminated:  0.5254790052996331\n",
      "Percentage of terminated:  0.5272652726527265\n",
      "Percentage of terminated:  0.5309194467046379\n",
      "Percentage of terminated:  0.557543716958113\n",
      "Percentage of terminated:  0.5034538805363673\n",
      "Percentage of terminated:  0.5032258064516129\n",
      "Percentage of terminated:  0.545639771801141\n",
      "Percentage of terminated:  0.5032520325203252\n",
      "Percentage of terminated:  0.5352917176662587\n",
      "Percentage of terminated:  0.5\n",
      "Percentage of terminated:  0.5101708706265257\n",
      "Percentage of terminated:  0.4959514170040486\n",
      "Percentage of terminated:  0.5144720750101917\n",
      "Percentage of terminated:  0.5267893660531697\n",
      "Percentage of terminated:  0.5251623376623377\n",
      "Percentage of terminated:  0.5144485144485145\n",
      "Percentage of terminated:  0.49918633034987797\n",
      "Percentage of terminated:  0.5103954341622503\n",
      "Percentage of terminated:  0.5500821018062397\n",
      "Percentage of terminated:  0.5249088699878494\n",
      "Percentage of terminated:  0.5234406848756624\n",
      "Percentage of terminated:  0.5433455433455433\n",
      "Percentage of terminated:  0.5487954267047774\n",
      "Percentage of terminated:  0.5336322869955157\n",
      "Percentage of terminated:  0.4971521562245728\n",
      "Percentage of terminated:  0.5386973180076629\n",
      "Epoch 17, Train Loss: 2.7531, Validation Loss: 2.7850\n",
      "Percentage of terminated:  0.539708265802269\n",
      "Percentage of terminated:  0.5292445166531276\n",
      "Percentage of terminated:  0.5189409368635438\n",
      "Percentage of terminated:  0.5171009771986971\n",
      "Percentage of terminated:  0.507492912110166\n",
      "Percentage of terminated:  0.519301097114994\n",
      "Percentage of terminated:  0.5609955120359037\n",
      "Percentage of terminated:  0.5584890333062551\n",
      "Percentage of terminated:  0.5289052890528906\n",
      "Percentage of terminated:  0.5247967479674797\n",
      "Percentage of terminated:  0.5101874490627547\n",
      "Percentage of terminated:  0.5024370430544273\n",
      "Percentage of terminated:  0.5026519787841697\n",
      "Percentage of terminated:  0.5437012720558063\n",
      "Percentage of terminated:  0.5211726384364821\n",
      "Percentage of terminated:  0.5278568523790158\n",
      "Percentage of terminated:  0.518563851489188\n",
      "Percentage of terminated:  0.5279578606158833\n",
      "Percentage of terminated:  0.4929121101660591\n",
      "Percentage of terminated:  0.5159183673469387\n",
      "Percentage of terminated:  0.5252237591537836\n",
      "Percentage of terminated:  0.508957654723127\n",
      "Percentage of terminated:  0.4973523421588595\n",
      "Percentage of terminated:  0.5145075602778912\n",
      "Percentage of terminated:  0.5586319218241043\n",
      "Percentage of terminated:  0.5018389865140989\n",
      "Percentage of terminated:  0.5510455104551045\n",
      "Percentage of terminated:  0.5157464212678936\n",
      "Percentage of terminated:  0.5\n",
      "Percentage of terminated:  0.5212464589235127\n",
      "Percentage of terminated:  0.5339845339845339\n",
      "Percentage of terminated:  0.5570032573289903\n",
      "Percentage of terminated:  0.5218800648298217\n",
      "Percentage of terminated:  0.5172272354388844\n",
      "Percentage of terminated:  0.5572768039135753\n",
      "Percentage of terminated:  0.5438884331419196\n",
      "Percentage of terminated:  0.5457119741100324\n",
      "Percentage of terminated:  0.5421052631578948\n",
      "Percentage of terminated:  0.5220228384991843\n",
      "Percentage of terminated:  0.5140988966080915\n",
      "Percentage of terminated:  0.5261865793780688\n",
      "Percentage of terminated:  0.5155610155610155\n",
      "Percentage of terminated:  0.5415986949429038\n",
      "Percentage of terminated:  0.4969499796665311\n",
      "Percentage of terminated:  0.5504065040650407\n",
      "Percentage of terminated:  0.4846374436706268\n",
      "Percentage of terminated:  0.5146938775510204\n",
      "Percentage of terminated:  0.524390243902439\n",
      "Percentage of terminated:  0.5120162932790224\n",
      "Percentage of terminated:  0.5268552685526855\n",
      "Percentage of terminated:  0.5036437246963563\n",
      "Percentage of terminated:  0.5367858610768599\n",
      "Percentage of terminated:  0.5553287055941201\n",
      "Percentage of terminated:  0.5004078303425775\n",
      "Percentage of terminated:  0.5603588907014682\n",
      "Percentage of terminated:  0.5437956204379562\n",
      "Percentage of terminated:  0.515139116202946\n",
      "Percentage of terminated:  0.5218635063342869\n",
      "Percentage of terminated:  0.5208845208845209\n",
      "Percentage of terminated:  0.5097560975609756\n",
      "Percentage of terminated:  0.49307817589576547\n",
      "Percentage of terminated:  0.5459723352318958\n",
      "Percentage of terminated:  0.5690359477124183\n",
      "Percentage of terminated:  0.5712540716612378\n",
      "Percentage of terminated:  0.5159574468085106\n",
      "Percentage of terminated:  0.5201465201465202\n",
      "Percentage of terminated:  0.5413441955193483\n",
      "Percentage of terminated:  0.5190437601296597\n",
      "Percentage of terminated:  0.512214983713355\n",
      "Percentage of terminated:  0.5354107648725213\n",
      "Percentage of terminated:  0.5266585266585266\n",
      "Percentage of terminated:  0.514531313958248\n",
      "Percentage of terminated:  0.5426929392446633\n",
      "Percentage of terminated:  0.5256305939788446\n",
      "Percentage of terminated:  0.5338467774625051\n",
      "Percentage of terminated:  0.5532435740514076\n",
      "Percentage of terminated:  0.5287915652879156\n",
      "Percentage of terminated:  0.5164969450101833\n",
      "Percentage of terminated:  0.5214521452145214\n",
      "Percentage of terminated:  0.5085714285714286\n",
      "Percentage of terminated:  0.5273611674098095\n",
      "Percentage of terminated:  0.515089722675367\n",
      "Percentage of terminated:  0.5417690417690417\n",
      "Percentage of terminated:  0.5527065527065527\n",
      "Percentage of terminated:  0.5212244897959184\n",
      "Percentage of terminated:  0.542962356792144\n",
      "Percentage of terminated:  0.496742671009772\n",
      "Percentage of terminated:  0.5448639870077142\n",
      "Percentage of terminated:  0.5416154161541615\n",
      "Percentage of terminated:  0.5288852725793328\n",
      "Percentage of terminated:  0.5207148659626321\n",
      "Percentage of terminated:  0.5322712418300654\n",
      "Percentage of terminated:  0.5250917992656059\n",
      "Percentage of terminated:  0.5403752039151712\n",
      "Percentage of terminated:  0.5363783346806791\n",
      "Percentage of terminated:  0.5419381107491856\n",
      "Percentage of terminated:  0.5315168767791786\n",
      "Percentage of terminated:  0.5125709651257097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5408950617283951\n",
      "Epoch 18, Train Loss: 2.7496, Validation Loss: 2.7145\n",
      "Percentage of terminated:  0.5614320585842149\n",
      "Percentage of terminated:  0.5154346060113729\n",
      "Percentage of terminated:  0.5196238757154538\n",
      "Percentage of terminated:  0.5319672131147541\n",
      "Percentage of terminated:  0.5281314168377823\n",
      "Percentage of terminated:  0.5278116118554608\n",
      "Percentage of terminated:  0.5208503679476697\n",
      "Percentage of terminated:  0.49756295694557273\n",
      "Percentage of terminated:  0.5521899304134261\n",
      "Percentage of terminated:  0.5062676910634857\n",
      "Percentage of terminated:  0.5360323886639676\n",
      "Percentage of terminated:  0.5254098360655738\n",
      "Percentage of terminated:  0.5207993474714518\n",
      "Percentage of terminated:  0.5150040551500406\n",
      "Percentage of terminated:  0.5124948791478902\n",
      "Percentage of terminated:  0.5181262729124236\n",
      "Percentage of terminated:  0.5331423895253683\n",
      "Percentage of terminated:  0.5170593013809911\n",
      "Percentage of terminated:  0.5293159609120521\n",
      "Percentage of terminated:  0.5437169581130541\n",
      "Percentage of terminated:  0.5318805488297014\n",
      "Percentage of terminated:  0.555737035524704\n",
      "Percentage of terminated:  0.5252442996742671\n",
      "Percentage of terminated:  0.512591389114541\n",
      "Percentage of terminated:  0.5284952849528495\n",
      "Percentage of terminated:  0.5154849225753871\n",
      "Percentage of terminated:  0.5357723577235772\n",
      "Percentage of terminated:  0.5315057283142389\n",
      "Percentage of terminated:  0.4798206278026906\n",
      "Percentage of terminated:  0.5199836867862969\n",
      "Percentage of terminated:  0.5056956875508544\n",
      "Percentage of terminated:  0.5261027923917442\n",
      "Percentage of terminated:  0.5335775335775336\n",
      "Percentage of terminated:  0.5535055350553506\n",
      "Percentage of terminated:  0.535303776683087\n",
      "Percentage of terminated:  0.5453802125919869\n",
      "Percentage of terminated:  0.5360488798370672\n",
      "Percentage of terminated:  0.5345320800980793\n",
      "Percentage of terminated:  0.5203086921202275\n",
      "Percentage of terminated:  0.5395006139991814\n",
      "Percentage of terminated:  0.5377435064935064\n",
      "Percentage of terminated:  0.5117982099267697\n",
      "Percentage of terminated:  0.5209775967413443\n",
      "Percentage of terminated:  0.5079689415610952\n",
      "Percentage of terminated:  0.5876835236541599\n",
      "Percentage of terminated:  0.5265306122448979\n",
      "Percentage of terminated:  0.5412919051512673\n",
      "Percentage of terminated:  0.5079043372517228\n",
      "Percentage of terminated:  0.5235772357723577\n",
      "Percentage of terminated:  0.5226537216828478\n",
      "Percentage of terminated:  0.5346091205211726\n",
      "Percentage of terminated:  0.48905109489051096\n",
      "Percentage of terminated:  0.4855749695245835\n",
      "Percentage of terminated:  0.5623461853978671\n",
      "Percentage of terminated:  0.5205368035786905\n",
      "Percentage of terminated:  0.4934426229508197\n",
      "Percentage of terminated:  0.5527597402597403\n",
      "Percentage of terminated:  0.5047034764826176\n",
      "Percentage of terminated:  0.5084814216478191\n",
      "Percentage of terminated:  0.5604395604395604\n",
      "Percentage of terminated:  0.5786240786240786\n",
      "Percentage of terminated:  0.5255414793624846\n",
      "Percentage of terminated:  0.5505297473512633\n",
      "Percentage of terminated:  0.5224489795918368\n",
      "Percentage of terminated:  0.5049019607843137\n",
      "Percentage of terminated:  0.511400651465798\n",
      "Percentage of terminated:  0.4876568191015783\n",
      "Percentage of terminated:  0.534826883910387\n",
      "Percentage of terminated:  0.5093724531377343\n",
      "Percentage of terminated:  0.5054722334819619\n",
      "Percentage of terminated:  0.5297009422367882\n",
      "Percentage of terminated:  0.5434871376071866\n",
      "Percentage of terminated:  0.5369995956328346\n",
      "Percentage of terminated:  0.5085365853658537\n",
      "Percentage of terminated:  0.5384615384615384\n",
      "Percentage of terminated:  0.4816924328722539\n",
      "Percentage of terminated:  0.5014210312626878\n",
      "Percentage of terminated:  0.5568181818181818\n",
      "Percentage of terminated:  0.47419354838709676\n",
      "Percentage of terminated:  0.5219869706840391\n",
      "Percentage of terminated:  0.5071283095723014\n",
      "Percentage of terminated:  0.5195071868583162\n",
      "Percentage of terminated:  0.47617107942973524\n",
      "Percentage of terminated:  0.5063033753558357\n",
      "Percentage of terminated:  0.5088078656288406\n",
      "Percentage of terminated:  0.540484997944924\n",
      "Percentage of terminated:  0.5560995512035903\n",
      "Percentage of terminated:  0.5460820138043037\n",
      "Percentage of terminated:  0.5334414268342116\n",
      "Percentage of terminated:  0.5254790052996331\n",
      "Percentage of terminated:  0.5639344262295082\n",
      "Percentage of terminated:  0.536019536019536\n",
      "Percentage of terminated:  0.5247241520228851\n",
      "Percentage of terminated:  0.5118657937806874\n",
      "Percentage of terminated:  0.5114099429502853\n",
      "Percentage of terminated:  0.5439313445034737\n",
      "Percentage of terminated:  0.5550986842105263\n",
      "Percentage of terminated:  0.5267019975540155\n",
      "Percentage of terminated:  0.47380585516178736\n",
      "Epoch 19, Train Loss: 2.7599, Validation Loss: 2.7577\n",
      "Percentage of terminated:  0.5024370430544273\n",
      "Percentage of terminated:  0.49492488834754367\n",
      "Percentage of terminated:  0.5424836601307189\n",
      "Percentage of terminated:  0.5209947003668977\n",
      "Percentage of terminated:  0.5230706410779911\n",
      "Percentage of terminated:  0.518563851489188\n",
      "Percentage of terminated:  0.534826883910387\n",
      "Percentage of terminated:  0.5288617886178861\n",
      "Percentage of terminated:  0.5150651465798045\n",
      "Percentage of terminated:  0.48533007334963324\n",
      "Percentage of terminated:  0.5403978887535525\n",
      "Percentage of terminated:  0.5541090317331163\n",
      "Percentage of terminated:  0.5075602778912954\n",
      "Percentage of terminated:  0.5043015157722245\n",
      "Percentage of terminated:  0.5278795278795279\n",
      "Percentage of terminated:  0.5274193548387097\n",
      "Percentage of terminated:  0.541157294213529\n",
      "Percentage of terminated:  0.5588838736151005\n",
      "Percentage of terminated:  0.5138888888888888\n",
      "Percentage of terminated:  0.5336597307221542\n",
      "Percentage of terminated:  0.5337124289195776\n",
      "Percentage of terminated:  0.4971498371335505\n",
      "Percentage of terminated:  0.5008130081300813\n",
      "Percentage of terminated:  0.5512454062882809\n",
      "Percentage of terminated:  0.5791190864600326\n",
      "Percentage of terminated:  0.5166531275385865\n",
      "Percentage of terminated:  0.510551948051948\n",
      "Percentage of terminated:  0.556013179571664\n",
      "Percentage of terminated:  0.5280259951259139\n",
      "Percentage of terminated:  0.5247766043866775\n",
      "Percentage of terminated:  0.5263584752635847\n",
      "Percentage of terminated:  0.5093724531377343\n",
      "Percentage of terminated:  0.5061025223759154\n",
      "Percentage of terminated:  0.5248664200575421\n",
      "Percentage of terminated:  0.49533089727974017\n",
      "Percentage of terminated:  0.495928338762215\n",
      "Percentage of terminated:  0.47058823529411764\n",
      "Percentage of terminated:  0.5398121682319315\n",
      "Percentage of terminated:  0.5429385429385429\n",
      "Percentage of terminated:  0.5843520782396088\n",
      "Percentage of terminated:  0.5215622457282343\n",
      "Percentage of terminated:  0.53836784409257\n",
      "Percentage of terminated:  0.5396825396825397\n",
      "Percentage of terminated:  0.5065306122448979\n",
      "Percentage of terminated:  0.5184734064149411\n",
      "Percentage of terminated:  0.5223759153783564\n",
      "Percentage of terminated:  0.551681706316653\n",
      "Percentage of terminated:  0.5397540983606557\n",
      "Percentage of terminated:  0.5245901639344263\n",
      "Percentage of terminated:  0.5212940212940212\n",
      "Percentage of terminated:  0.5188062142273099\n",
      "Percentage of terminated:  0.5420408163265306\n",
      "Percentage of terminated:  0.5394683026584867\n",
      "Percentage of terminated:  0.548202614379085\n",
      "Percentage of terminated:  0.506938775510204\n",
      "Percentage of terminated:  0.5306122448979592\n",
      "Percentage of terminated:  0.5362259516987311\n",
      "Percentage of terminated:  0.5301302931596091\n",
      "Percentage of terminated:  0.5290060851926978\n",
      "Percentage of terminated:  0.5365853658536586\n",
      "Percentage of terminated:  0.5556461475743987\n",
      "Percentage of terminated:  0.5318627450980392\n",
      "Percentage of terminated:  0.5099472188388144\n",
      "Percentage of terminated:  0.5304804198627372\n",
      "Percentage of terminated:  0.5276872964169381\n",
      "Percentage of terminated:  0.5307035380235868\n",
      "Percentage of terminated:  0.5301794453507341\n",
      "Percentage of terminated:  0.557242511284366\n",
      "Percentage of terminated:  0.48292682926829267\n",
      "Percentage of terminated:  0.4923014586709887\n",
      "Percentage of terminated:  0.5630354957160343\n",
      "Percentage of terminated:  0.4938925081433225\n",
      "Percentage of terminated:  0.5345528455284553\n",
      "Percentage of terminated:  0.5610252237591538\n",
      "Percentage of terminated:  0.5367047308319739\n",
      "Percentage of terminated:  0.5130825838103026\n",
      "Percentage of terminated:  0.5057049714751426\n",
      "Percentage of terminated:  0.5234215885947047\n",
      "Percentage of terminated:  0.6089743589743589\n",
      "Percentage of terminated:  0.5513447432762836\n",
      "Percentage of terminated:  0.5150162337662337\n",
      "Percentage of terminated:  0.5065040650406504\n",
      "Percentage of terminated:  0.5304205798285014\n",
      "Percentage of terminated:  0.5260139287177387\n",
      "Percentage of terminated:  0.5270215359609914\n",
      "Percentage of terminated:  0.5113544201135442\n",
      "Percentage of terminated:  0.5060827250608273\n",
      "Percentage of terminated:  0.5187601957585645\n",
      "Percentage of terminated:  0.5248979591836734\n",
      "Percentage of terminated:  0.5280163599182004\n",
      "Percentage of terminated:  0.542997542997543\n",
      "Percentage of terminated:  0.509795918367347\n",
      "Percentage of terminated:  0.5099391480730223\n",
      "Percentage of terminated:  0.566530612244898\n",
      "Percentage of terminated:  0.48128559804719284\n",
      "Percentage of terminated:  0.5175081433224755\n",
      "Percentage of terminated:  0.5149284253578732\n",
      "Percentage of terminated:  0.5383360522022839\n",
      "Percentage of terminated:  0.5111111111111111\n",
      "Epoch 20, Train Loss: 2.7609, Validation Loss: 2.7565\n",
      "Percentage of terminated:  0.5344757241942064\n",
      "Percentage of terminated:  0.497557003257329\n",
      "Percentage of terminated:  0.5046653144016228\n",
      "Percentage of terminated:  0.5344195519348269\n",
      "Percentage of terminated:  0.5307535641547861\n",
      "Percentage of terminated:  0.5373927257866775\n",
      "Percentage of terminated:  0.5483210483210483\n",
      "Percentage of terminated:  0.5359183673469388\n",
      "Percentage of terminated:  0.5379901960784313\n",
      "Percentage of terminated:  0.5215075788611225\n",
      "Percentage of terminated:  0.5306873977086743\n",
      "Percentage of terminated:  0.5339845339845339\n",
      "Percentage of terminated:  0.535655737704918\n",
      "Percentage of terminated:  0.5192854242793341\n",
      "Percentage of terminated:  0.5248573757131214\n",
      "Percentage of terminated:  0.5534770231801546\n",
      "Percentage of terminated:  0.5505297473512633\n",
      "Percentage of terminated:  0.5547355473554736\n",
      "Percentage of terminated:  0.5297716150081566\n",
      "Percentage of terminated:  0.5288617886178861\n",
      "Percentage of terminated:  0.5748063595597228\n",
      "Percentage of terminated:  0.5421735604217356\n",
      "Percentage of terminated:  0.4878345498783455\n",
      "Percentage of terminated:  0.5382424735557364\n",
      "Percentage of terminated:  0.5157078743370053\n",
      "Percentage of terminated:  0.5470155355682748\n",
      "Percentage of terminated:  0.4867292772560229\n",
      "Percentage of terminated:  0.5020308692120228\n",
      "Percentage of terminated:  0.5433360588716272\n",
      "Percentage of terminated:  0.5507717303005687\n",
      "Percentage of terminated:  0.5423797881010595\n",
      "Percentage of terminated:  0.5440816326530612\n",
      "Percentage of terminated:  0.5137126483831355\n",
      "Percentage of terminated:  0.5363748458692972\n",
      "Percentage of terminated:  0.5359183673469388\n",
      "Percentage of terminated:  0.5294117647058824\n",
      "Percentage of terminated:  0.5231462515362556\n",
      "Percentage of terminated:  0.4822231303637107\n",
      "Percentage of terminated:  0.5484266448712709\n",
      "Percentage of terminated:  0.5392755392755393\n",
      "Percentage of terminated:  0.5604575163398693\n",
      "Percentage of terminated:  0.5373317013463892\n",
      "Percentage of terminated:  0.5134037367993501\n",
      "Percentage of terminated:  0.5083707635769702\n",
      "Percentage of terminated:  0.5059063136456212\n",
      "Percentage of terminated:  0.5194115243154883\n",
      "Percentage of terminated:  0.5346169602621876\n",
      "Percentage of terminated:  0.5044861337683524\n",
      "Percentage of terminated:  0.5220588235294118\n",
      "Percentage of terminated:  0.5157078743370053\n",
      "Percentage of terminated:  0.4967558799675588\n",
      "Percentage of terminated:  0.5562347188264058\n",
      "Percentage of terminated:  0.5201465201465202\n",
      "Percentage of terminated:  0.5369237046103631\n",
      "Percentage of terminated:  0.5097640358014646\n",
      "Percentage of terminated:  0.5133142154854567\n",
      "Percentage of terminated:  0.5492671009771987\n",
      "Percentage of terminated:  0.5065093572009765\n",
      "Percentage of terminated:  0.5227642276422764\n",
      "Percentage of terminated:  0.5283095723014256\n",
      "Percentage of terminated:  0.5122249388753056\n",
      "Percentage of terminated:  0.5207485760781123\n",
      "Percentage of terminated:  0.5556910569105691\n",
      "Percentage of terminated:  0.5186839967506093\n",
      "Percentage of terminated:  0.5184584178498985\n",
      "Percentage of terminated:  0.5476093175316714\n",
      "Percentage of terminated:  0.49612086565945285\n",
      "Percentage of terminated:  0.5321992709599028\n",
      "Percentage of terminated:  0.5268904164981804\n",
      "Percentage of terminated:  0.5347985347985348\n",
      "Percentage of terminated:  0.5179591836734694\n",
      "Percentage of terminated:  0.5490196078431373\n",
      "Percentage of terminated:  0.5018300122000814\n",
      "Percentage of terminated:  0.5462321792260693\n",
      "Percentage of terminated:  0.5188794153471377\n",
      "Percentage of terminated:  0.5209187858900738\n",
      "Percentage of terminated:  0.5454914703493096\n",
      "Percentage of terminated:  0.5101957585644372\n",
      "Percentage of terminated:  0.5714285714285714\n",
      "Percentage of terminated:  0.5632885632885632\n",
      "Percentage of terminated:  0.5219333874898456\n",
      "Percentage of terminated:  0.5646341463414634\n",
      "Percentage of terminated:  0.5254168361122408\n",
      "Percentage of terminated:  0.5203252032520326\n",
      "Percentage of terminated:  0.5782009724473258\n",
      "Percentage of terminated:  0.506514657980456\n",
      "Percentage of terminated:  0.514018691588785\n",
      "Percentage of terminated:  0.502643350955673\n",
      "Percentage of terminated:  0.5085924713584288\n",
      "Percentage of terminated:  0.5506535947712419\n",
      "Percentage of terminated:  0.5155228758169934\n",
      "Percentage of terminated:  0.5476772616136919\n",
      "Percentage of terminated:  0.4963384865744508\n",
      "Percentage of terminated:  0.5305374592833876\n",
      "Percentage of terminated:  0.532520325203252\n",
      "Percentage of terminated:  0.5160634404229362\n",
      "Percentage of terminated:  0.5114099429502853\n",
      "Percentage of terminated:  0.5414454879542671\n",
      "Percentage of terminated:  0.5343511450381679\n",
      "Epoch 21, Train Loss: 2.7353, Validation Loss: 2.7305\n",
      "Percentage of terminated:  0.5089213300892133\n",
      "Percentage of terminated:  0.5706122448979591\n",
      "Percentage of terminated:  0.513039934800326\n",
      "Percentage of terminated:  0.5594605639558643\n",
      "Percentage of terminated:  0.509541209906618\n",
      "Percentage of terminated:  0.5444535073409462\n",
      "Percentage of terminated:  0.5156440471353109\n",
      "Percentage of terminated:  0.5359934183463595\n",
      "Percentage of terminated:  0.5374592833876222\n",
      "Percentage of terminated:  0.5040650406504065\n",
      "Percentage of terminated:  0.5341792877609497\n",
      "Percentage of terminated:  0.5269828291087489\n",
      "Percentage of terminated:  0.5608961303462322\n",
      "Percentage of terminated:  0.5431244914564687\n",
      "Percentage of terminated:  0.5317589576547231\n",
      "Percentage of terminated:  0.48963835839089803\n",
      "Percentage of terminated:  0.5539334955393349\n",
      "Percentage of terminated:  0.5487704918032786\n",
      "Percentage of terminated:  0.5159183673469387\n",
      "Percentage of terminated:  0.5113544201135442\n",
      "Percentage of terminated:  0.5285016286644951\n",
      "Percentage of terminated:  0.4545083639330885\n",
      "Percentage of terminated:  0.49326805385556916\n",
      "Percentage of terminated:  0.5409165302782324\n",
      "Percentage of terminated:  0.5633401221995926\n",
      "Percentage of terminated:  0.5555100368701352\n",
      "Percentage of terminated:  0.5458248472505092\n",
      "Percentage of terminated:  0.5390879478827362\n",
      "Percentage of terminated:  0.5476673427991886\n",
      "Percentage of terminated:  0.5608766233766234\n",
      "Percentage of terminated:  0.5203711173860428\n",
      "Percentage of terminated:  0.5138436482084691\n",
      "Percentage of terminated:  0.5321923390383048\n",
      "Percentage of terminated:  0.5068714632174616\n",
      "Percentage of terminated:  0.5585585585585585\n",
      "Percentage of terminated:  0.5164835164835165\n",
      "Percentage of terminated:  0.5036764705882353\n",
      "Percentage of terminated:  0.5046577561765897\n",
      "Percentage of terminated:  0.5312627707396812\n",
      "Percentage of terminated:  0.46588139723801786\n",
      "Percentage of terminated:  0.5636811832374692\n",
      "Percentage of terminated:  0.5055124540628828\n",
      "Percentage of terminated:  0.5603413246647704\n",
      "Percentage of terminated:  0.5233224222585925\n",
      "Percentage of terminated:  0.497765136123527\n",
      "Percentage of terminated:  0.4927302100161551\n",
      "Percentage of terminated:  0.5565040650406504\n",
      "Percentage of terminated:  0.559074299634592\n",
      "Percentage of terminated:  0.5762642740619902\n",
      "Percentage of terminated:  0.5302534750613246\n",
      "Percentage of terminated:  0.5338223308883455\n",
      "Percentage of terminated:  0.5459723352318958\n",
      "Percentage of terminated:  0.5156695156695157\n",
      "Percentage of terminated:  0.5156186612576065\n",
      "Percentage of terminated:  0.5247039608003267\n",
      "Percentage of terminated:  0.5238678090575275\n",
      "Percentage of terminated:  0.5696771557008582\n",
      "Percentage of terminated:  0.5509372453137734\n",
      "Percentage of terminated:  0.5296025952960259\n",
      "Percentage of terminated:  0.4985789687373122\n",
      "Percentage of terminated:  0.5210867802108679\n",
      "Percentage of terminated:  0.5085504885993485\n",
      "Percentage of terminated:  0.5220048899755502\n",
      "Percentage of terminated:  0.5113636363636364\n",
      "Percentage of terminated:  0.5109577922077922\n",
      "Percentage of terminated:  0.5437091503267973\n",
      "Percentage of terminated:  0.5330882352941176\n",
      "Percentage of terminated:  0.516260162601626\n",
      "Percentage of terminated:  0.5379591836734694\n",
      "Percentage of terminated:  0.5553739272578668\n",
      "Percentage of terminated:  0.5187144019528072\n",
      "Percentage of terminated:  0.5296266233766234\n",
      "Percentage of terminated:  0.5396119644300728\n",
      "Percentage of terminated:  0.5380545380545381\n",
      "Percentage of terminated:  0.5205034510759237\n",
      "Percentage of terminated:  0.555375909458367\n",
      "Percentage of terminated:  0.5242914979757085\n",
      "Percentage of terminated:  0.49775967413441957\n",
      "Percentage of terminated:  0.44193548387096776\n",
      "Percentage of terminated:  0.5307941653160454\n",
      "Percentage of terminated:  0.5729335494327391\n",
      "Percentage of terminated:  0.5436022819885901\n",
      "Percentage of terminated:  0.5093724531377343\n",
      "Percentage of terminated:  0.5287449392712551\n",
      "Percentage of terminated:  0.5246435845213849\n",
      "Percentage of terminated:  0.5089285714285714\n",
      "Percentage of terminated:  0.5158407798537774\n",
      "Percentage of terminated:  0.4959250203748981\n",
      "Percentage of terminated:  0.4971590909090909\n",
      "Percentage of terminated:  0.5161158710730314\n",
      "Percentage of terminated:  0.5336032388663967\n",
      "Percentage of terminated:  0.5008156606851549\n",
      "Percentage of terminated:  0.514018691588785\n",
      "Percentage of terminated:  0.512591389114541\n",
      "Percentage of terminated:  0.5461224489795918\n",
      "Percentage of terminated:  0.5306122448979592\n",
      "Percentage of terminated:  0.5140300935339569\n",
      "Percentage of terminated:  0.475020475020475\n",
      "Percentage of terminated:  0.5608263198163733\n",
      "Epoch 22, Train Loss: 2.7434, Validation Loss: 2.7562\n",
      "Percentage of terminated:  0.5196399345335515\n",
      "Percentage of terminated:  0.5478331970564186\n",
      "Percentage of terminated:  0.5059111292295149\n",
      "Percentage of terminated:  0.506514657980456\n",
      "Percentage of terminated:  0.5156313438895656\n",
      "Percentage of terminated:  0.5614320585842149\n",
      "Percentage of terminated:  0.48048780487804876\n",
      "Percentage of terminated:  0.5384615384615384\n",
      "Percentage of terminated:  0.5091575091575091\n",
      "Percentage of terminated:  0.5395273023634882\n",
      "Percentage of terminated:  0.536915504511895\n",
      "Percentage of terminated:  0.5427991886409737\n",
      "Percentage of terminated:  0.5040683482506102\n",
      "Percentage of terminated:  0.5379002837454399\n",
      "Percentage of terminated:  0.5558272208638957\n",
      "Percentage of terminated:  0.5615038823048631\n",
      "Percentage of terminated:  0.5173398612811098\n",
      "Percentage of terminated:  0.5280259951259139\n",
      "Percentage of terminated:  0.5283095723014256\n",
      "Percentage of terminated:  0.5486652977412732\n",
      "Percentage of terminated:  0.5159574468085106\n",
      "Percentage of terminated:  0.5352629433346923\n",
      "Percentage of terminated:  0.5146341463414634\n",
      "Percentage of terminated:  0.5378943056124539\n",
      "Percentage of terminated:  0.514831369362048\n",
      "Percentage of terminated:  0.518170681910984\n",
      "Percentage of terminated:  0.5491400491400491\n",
      "Percentage of terminated:  0.508110300081103\n",
      "Percentage of terminated:  0.5050833672224482\n",
      "Percentage of terminated:  0.5524929063640048\n",
      "Percentage of terminated:  0.5261651676206051\n",
      "Percentage of terminated:  0.511437908496732\n",
      "Percentage of terminated:  0.5202757502027575\n",
      "Percentage of terminated:  0.5387438825448614\n",
      "Percentage of terminated:  0.5380235868239122\n",
      "Percentage of terminated:  0.5516680227827502\n",
      "Percentage of terminated:  0.5632885632885632\n",
      "Percentage of terminated:  0.5565040650406504\n",
      "Percentage of terminated:  0.48919690175295555\n",
      "Percentage of terminated:  0.48322422258592473\n",
      "Percentage of terminated:  0.5120457329522254\n",
      "Percentage of terminated:  0.5440519691433211\n",
      "Percentage of terminated:  0.5395273023634882\n",
      "Percentage of terminated:  0.5165991902834008\n",
      "Percentage of terminated:  0.5381184103811841\n",
      "Percentage of terminated:  0.5006119951040392\n",
      "Percentage of terminated:  0.5337919870497774\n",
      "Percentage of terminated:  0.5142740619902121\n",
      "Percentage of terminated:  0.4979541734860884\n",
      "Percentage of terminated:  0.5162999185004075\n",
      "Percentage of terminated:  0.5165322580645161\n",
      "Percentage of terminated:  0.5211038961038961\n",
      "Percentage of terminated:  0.5188947582283625\n",
      "Percentage of terminated:  0.5451965950547223\n",
      "Percentage of terminated:  0.5079819893573475\n",
      "Percentage of terminated:  0.5389955083707636\n",
      "Percentage of terminated:  0.5331152902698283\n",
      "Percentage of terminated:  0.5541972290138549\n",
      "Percentage of terminated:  0.5334967320261438\n",
      "Percentage of terminated:  0.5112107623318386\n",
      "Percentage of terminated:  0.5412054120541205\n",
      "Percentage of terminated:  0.558262281770199\n",
      "Percentage of terminated:  0.5248776508972267\n",
      "Percentage of terminated:  0.4816775244299674\n",
      "Percentage of terminated:  0.534307754770605\n",
      "Percentage of terminated:  0.5443762781186094\n",
      "Percentage of terminated:  0.5358306188925082\n",
      "Percentage of terminated:  0.5050833672224482\n",
      "Percentage of terminated:  0.5451586655817738\n",
      "Percentage of terminated:  0.5462321792260693\n",
      "Percentage of terminated:  0.5293393516618794\n",
      "Percentage of terminated:  0.552006552006552\n",
      "Percentage of terminated:  0.5192465192465192\n",
      "Percentage of terminated:  0.5301106104055715\n",
      "Percentage of terminated:  0.5192229866450829\n",
      "Percentage of terminated:  0.5235559197050389\n",
      "Percentage of terminated:  0.4920699471329809\n",
      "Percentage of terminated:  0.5381803411860276\n",
      "Percentage of terminated:  0.5146579804560261\n",
      "Percentage of terminated:  0.5323828920570265\n",
      "Percentage of terminated:  0.5268948655256723\n",
      "Percentage of terminated:  0.5402862985685072\n",
      "Percentage of terminated:  0.49878048780487805\n",
      "Percentage of terminated:  0.5413290113452188\n",
      "Percentage of terminated:  0.5186246418338109\n",
      "Percentage of terminated:  0.5302534750613246\n",
      "Percentage of terminated:  0.5347896440129449\n",
      "Percentage of terminated:  0.504287464271131\n",
      "Percentage of terminated:  0.4895875867701103\n",
      "Percentage of terminated:  0.5330360762059181\n",
      "Percentage of terminated:  0.5692119232339731\n",
      "Percentage of terminated:  0.510586319218241\n",
      "Percentage of terminated:  0.5482171799027553\n",
      "Percentage of terminated:  0.5438667749796913\n",
      "Percentage of terminated:  0.5397727272727273\n",
      "Percentage of terminated:  0.5314401622718052\n",
      "Percentage of terminated:  0.5312883435582823\n",
      "Percentage of terminated:  0.5220588235294118\n",
      "Percentage of terminated:  0.5212027756360833\n",
      "Epoch 23, Train Loss: 2.7356, Validation Loss: 2.7350\n",
      "Percentage of terminated:  0.5394038383013475\n",
      "Percentage of terminated:  0.5592942141977841\n",
      "Percentage of terminated:  0.5498981670061099\n",
      "Percentage of terminated:  0.5372405372405372\n",
      "Percentage of terminated:  0.4755899104963385\n",
      "Percentage of terminated:  0.5215272136474411\n",
      "Percentage of terminated:  0.4850444624090542\n",
      "Percentage of terminated:  0.5226993865030675\n",
      "Percentage of terminated:  0.4967400162999185\n",
      "Percentage of terminated:  0.5010133765707336\n",
      "Percentage of terminated:  0.5184285135682463\n",
      "Percentage of terminated:  0.5470155355682748\n",
      "Percentage of terminated:  0.5278342137342543\n",
      "Percentage of terminated:  0.513601299228583\n",
      "Percentage of terminated:  0.5699755899104963\n",
      "Percentage of terminated:  0.554788961038961\n",
      "Percentage of terminated:  0.5083776052308949\n",
      "Percentage of terminated:  0.5398698128559805\n",
      "Percentage of terminated:  0.5210979106923392\n",
      "Percentage of terminated:  0.5331166192604633\n",
      "Percentage of terminated:  0.4867509172441908\n",
      "Percentage of terminated:  0.4949124949124949\n",
      "Percentage of terminated:  0.5154849225753871\n",
      "Percentage of terminated:  0.5511779041429732\n",
      "Percentage of terminated:  0.5151020408163265\n",
      "Percentage of terminated:  0.5375510204081633\n",
      "Percentage of terminated:  0.5379901960784313\n",
      "Percentage of terminated:  0.5616830065359477\n",
      "Percentage of terminated:  0.5446756425948592\n",
      "Percentage of terminated:  0.5062982527427875\n",
      "Percentage of terminated:  0.5780487804878048\n",
      "Percentage of terminated:  0.49268887083671814\n",
      "Percentage of terminated:  0.4977633184221228\n",
      "Percentage of terminated:  0.5381165919282511\n",
      "Percentage of terminated:  0.5572705117790414\n",
      "Percentage of terminated:  0.5113452188006483\n",
      "Percentage of terminated:  0.5349122090649244\n",
      "Percentage of terminated:  0.5699714402284781\n",
      "Percentage of terminated:  0.5442622950819672\n",
      "Percentage of terminated:  0.5379591836734694\n",
      "Percentage of terminated:  0.496742671009772\n",
      "Percentage of terminated:  0.48986212489862124\n",
      "Percentage of terminated:  0.5643119640669662\n",
      "Percentage of terminated:  0.5228198859005705\n",
      "Percentage of terminated:  0.5272579332790887\n",
      "Percentage of terminated:  0.5535714285714286\n",
      "Percentage of terminated:  0.5364265364265364\n",
      "Percentage of terminated:  0.5368635437881873\n",
      "Percentage of terminated:  0.5149918962722853\n",
      "Percentage of terminated:  0.5471928397070789\n",
      "Percentage of terminated:  0.5410706988148754\n",
      "Percentage of terminated:  0.5814047909053999\n",
      "Percentage of terminated:  0.5424836601307189\n",
      "Percentage of terminated:  0.5241379310344828\n",
      "Percentage of terminated:  0.5120162932790224\n",
      "Percentage of terminated:  0.5202123315639037\n",
      "Percentage of terminated:  0.477041852905323\n",
      "Percentage of terminated:  0.5091949325704945\n",
      "Percentage of terminated:  0.5309698451507743\n",
      "Percentage of terminated:  0.5169042769857434\n",
      "Percentage of terminated:  0.49612719119445575\n",
      "Percentage of terminated:  0.5284784377542717\n",
      "Percentage of terminated:  0.5165589660743134\n",
      "Percentage of terminated:  0.5396502643350956\n",
      "Percentage of terminated:  0.5611745513866232\n",
      "Percentage of terminated:  0.5465964343598055\n",
      "Percentage of terminated:  0.5050875050875051\n",
      "Percentage of terminated:  0.5459016393442623\n",
      "Percentage of terminated:  0.535977105478332\n",
      "Percentage of terminated:  0.5052845528455284\n",
      "Percentage of terminated:  0.5406389001213101\n",
      "Percentage of terminated:  0.5341463414634147\n",
      "Percentage of terminated:  0.5050709939148073\n",
      "Percentage of terminated:  0.5588235294117647\n",
      "Percentage of terminated:  0.5242758057935537\n",
      "Percentage of terminated:  0.4767726161369193\n",
      "Percentage of terminated:  0.5293159609120521\n",
      "Percentage of terminated:  0.544973544973545\n",
      "Percentage of terminated:  0.5394736842105263\n",
      "Percentage of terminated:  0.5288852725793328\n",
      "Percentage of terminated:  0.5477551020408163\n",
      "Percentage of terminated:  0.5311102074013827\n",
      "Percentage of terminated:  0.529171766625867\n",
      "Percentage of terminated:  0.48451507742461286\n",
      "Percentage of terminated:  0.5767668562144598\n",
      "Percentage of terminated:  0.5032733224222586\n",
      "Percentage of terminated:  0.5603096984515077\n",
      "Percentage of terminated:  0.5314285714285715\n",
      "Percentage of terminated:  0.4951259138911454\n",
      "Percentage of terminated:  0.5515126737530662\n",
      "Percentage of terminated:  0.532626427406199\n",
      "Percentage of terminated:  0.5459700283515594\n",
      "Percentage of terminated:  0.5276073619631901\n",
      "Percentage of terminated:  0.5104294478527608\n",
      "Percentage of terminated:  0.5335229581470947\n",
      "Percentage of terminated:  0.5389319200978394\n",
      "Percentage of terminated:  0.5423797881010595\n",
      "Percentage of terminated:  0.49796747967479676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of terminated:  0.5860749808722264\n",
      "Epoch 24, Train Loss: 2.7392, Validation Loss: 2.7097\n",
      "Percentage of terminated:  0.5164567249085737\n",
      "Percentage of terminated:  0.5214723926380368\n",
      "Percentage of terminated:  0.5242758057935537\n",
      "Percentage of terminated:  0.5244840145690004\n",
      "Percentage of terminated:  0.5916700694160882\n",
      "Percentage of terminated:  0.5268074735987003\n",
      "Percentage of terminated:  0.5234981610134859\n",
      "Percentage of terminated:  0.5391553915539156\n",
      "Percentage of terminated:  0.4902120717781403\n",
      "Percentage of terminated:  0.5166666666666667\n",
      "Percentage of terminated:  0.5300613496932516\n",
      "Percentage of terminated:  0.5406403940886699\n",
      "Percentage of terminated:  0.5051041241322989\n",
      "Percentage of terminated:  0.5002030044660982\n",
      "Percentage of terminated:  0.5065359477124183\n",
      "Percentage of terminated:  0.5390243902439025\n",
      "Percentage of terminated:  0.5419222903885481\n",
      "Percentage of terminated:  0.5326530612244897\n",
      "Percentage of terminated:  0.5344195519348269\n",
      "Percentage of terminated:  0.5280032467532467\n",
      "Percentage of terminated:  0.5109577922077922\n",
      "Percentage of terminated:  0.5096035962402943\n",
      "Percentage of terminated:  0.5287872601061658\n",
      "Percentage of terminated:  0.5061626951520132\n",
      "Percentage of terminated:  0.5154975530179445\n",
      "Percentage of terminated:  0.5172413793103449\n",
      "Percentage of terminated:  0.5476772616136919\n",
      "Percentage of terminated:  0.49572301425661913\n",
      "Percentage of terminated:  0.5402112103980503\n",
      "Percentage of terminated:  0.5069161920260374\n",
      "Percentage of terminated:  0.5251122907309106\n",
      "Percentage of terminated:  0.539714867617108\n",
      "Percentage of terminated:  0.532953620829943\n",
      "Percentage of terminated:  0.5317848410757946\n",
      "Percentage of terminated:  0.5487804878048781\n",
      "Percentage of terminated:  0.5252032520325203\n",
      "Percentage of terminated:  0.49959316517493896\n",
      "Percentage of terminated:  0.5423108218063466\n",
      "Percentage of terminated:  0.542997542997543\n",
      "Percentage of terminated:  0.5463875205254516\n",
      "Percentage of terminated:  0.5461254612546126\n",
      "Percentage of terminated:  0.514413317092976\n",
      "Percentage of terminated:  0.5382736156351792\n",
      "Percentage of terminated:  0.5268993839835729\n",
      "Percentage of terminated:  0.5251533742331288\n",
      "Percentage of terminated:  0.5215973920130399\n",
      "Percentage of terminated:  0.509580105992662\n",
      "Percentage of terminated:  0.5063136456211813\n",
      "Percentage of terminated:  0.5206375153248877\n",
      "Percentage of terminated:  0.4871899145994307\n",
      "Percentage of terminated:  0.5224123879380603\n",
      "Percentage of terminated:  0.554601226993865\n",
      "Percentage of terminated:  0.5288852725793328\n",
      "Percentage of terminated:  0.5561866125760649\n",
      "Percentage of terminated:  0.523517382413088\n",
      "Percentage of terminated:  0.5256305939788446\n",
      "Percentage of terminated:  0.5051124744376279\n",
      "Percentage of terminated:  0.5361201298701299\n",
      "Percentage of terminated:  0.5382087099424815\n",
      "Percentage of terminated:  0.5426008968609866\n",
      "Percentage of terminated:  0.47408906882591095\n",
      "Percentage of terminated:  0.5235580828594638\n",
      "Percentage of terminated:  0.5475223395613322\n",
      "Percentage of terminated:  0.5617056170561706\n",
      "Percentage of terminated:  0.5700245700245701\n",
      "Percentage of terminated:  0.5344757241942064\n",
      "Percentage of terminated:  0.5246235246235246\n",
      "Percentage of terminated:  0.5432098765432098\n",
      "Percentage of terminated:  0.49877949552481693\n",
      "Percentage of terminated:  0.4820846905537459\n",
      "Percentage of terminated:  0.5486761710794298\n",
      "Percentage of terminated:  0.5320097244732577\n",
      "Percentage of terminated:  0.5161158710730314\n",
      "Percentage of terminated:  0.5425051334702259\n",
      "Percentage of terminated:  0.5204205418520016\n",
      "Percentage of terminated:  0.5394629780309195\n",
      "Percentage of terminated:  0.552739165985282\n",
      "Percentage of terminated:  0.5238873009391588\n",
      "Percentage of terminated:  0.5863192182410424\n",
      "Percentage of terminated:  0.497165991902834\n",
      "Percentage of terminated:  0.5426952650748684\n",
      "Percentage of terminated:  0.5530955309553095\n",
      "Percentage of terminated:  0.511002444987775\n",
      "Percentage of terminated:  0.5112107623318386\n",
      "Percentage of terminated:  0.5151267375306623\n",
      "Percentage of terminated:  0.532733224222586\n",
      "Percentage of terminated:  0.5296266233766234\n",
      "Percentage of terminated:  0.5112474437627812\n",
      "Percentage of terminated:  0.5174371451743714\n",
      "Percentage of terminated:  0.5491636066911465\n",
      "Percentage of terminated:  0.5575980392156863\n",
      "Percentage of terminated:  0.4971381847914963\n",
      "Percentage of terminated:  0.5292207792207793\n",
      "Percentage of terminated:  0.49144951140065146\n",
      "Percentage of terminated:  0.5189718482252142\n",
      "Percentage of terminated:  0.5038791343405472\n",
      "Percentage of terminated:  0.5046691027202599\n",
      "Percentage of terminated:  0.5245835026412028\n",
      "Percentage of terminated:  0.5490045941807045\n",
      "Epoch 25, Train Loss: 2.7439, Validation Loss: 2.7777\n",
      "Percentage of terminated:  0.5599022004889975\n",
      "Percentage of terminated:  0.5528885272579332\n",
      "Percentage of terminated:  0.5259077927376581\n",
      "Percentage of terminated:  0.48356997971602433\n",
      "Percentage of terminated:  0.53556827473426\n",
      "Percentage of terminated:  0.5412581699346405\n",
      "Percentage of terminated:  0.541801948051948\n",
      "Percentage of terminated:  0.5303215303215303\n",
      "Percentage of terminated:  0.5097560975609756\n",
      "Percentage of terminated:  0.5096430036930653\n",
      "Percentage of terminated:  0.5454171804356761\n",
      "Percentage of terminated:  0.4963355048859935\n",
      "Percentage of terminated:  0.5198201879852881\n",
      "Percentage of terminated:  0.5627811860940696\n",
      "Percentage of terminated:  0.5296989422294548\n",
      "Percentage of terminated:  0.5089430894308943\n",
      "Percentage of terminated:  0.4881192106322996\n",
      "Percentage of terminated:  0.5511618426416632\n",
      "Percentage of terminated:  0.5338775510204081\n",
      "Percentage of terminated:  0.48176976648914377\n",
      "Percentage of terminated:  0.5207993474714518\n",
      "Percentage of terminated:  0.5264227642276422\n",
      "Percentage of terminated:  0.5126118795768918\n",
      "Percentage of terminated:  0.5323039414872003\n",
      "Percentage of terminated:  0.5408496732026143\n",
      "Percentage of terminated:  0.5147058823529411\n",
      "Percentage of terminated:  0.5400654129190515\n",
      "Percentage of terminated:  0.4993894993894994\n",
      "Percentage of terminated:  0.5193640440277212\n",
      "Percentage of terminated:  0.5264442636289667\n",
      "Percentage of terminated:  0.5591836734693878\n",
      "Percentage of terminated:  0.5669515669515669\n",
      "Percentage of terminated:  0.530278232405892\n",
      "Percentage of terminated:  0.5291479820627802\n",
      "Percentage of terminated:  0.5631901840490797\n",
      "Percentage of terminated:  0.5344757241942064\n",
      "Percentage of terminated:  0.515077424612877\n",
      "Percentage of terminated:  0.5840097402597403\n",
      "Percentage of terminated:  0.5445665445665445\n",
      "Percentage of terminated:  0.5400654129190515\n",
      "Percentage of terminated:  0.5402204981625153\n",
      "Percentage of terminated:  0.5810147299509002\n",
      "Percentage of terminated:  0.5437881873727087\n",
      "Percentage of terminated:  0.5024271844660194\n",
      "Percentage of terminated:  0.5250101667344449\n",
      "Percentage of terminated:  0.526829268292683\n",
      "Percentage of terminated:  0.5525244299674267\n",
      "Percentage of terminated:  0.5360323886639676\n",
      "Percentage of terminated:  0.5038633590890605\n",
      "Percentage of terminated:  0.5016339869281046\n",
      "Percentage of terminated:  0.5223086369218174\n",
      "Percentage of terminated:  0.49470252648736757\n",
      "Percentage of terminated:  0.5289825699229834\n",
      "Percentage of terminated:  0.5006085192697769\n",
      "Percentage of terminated:  0.5399673735725938\n",
      "Percentage of terminated:  0.5154346060113729\n",
      "Percentage of terminated:  0.5058633238980995\n",
      "Percentage of terminated:  0.520048602673147\n",
      "Percentage of terminated:  0.5714285714285714\n",
      "Percentage of terminated:  0.5151267375306623\n",
      "Percentage of terminated:  0.48239579117766085\n",
      "Percentage of terminated:  0.48122448979591836\n",
      "Percentage of terminated:  0.5115618661257606\n",
      "Percentage of terminated:  0.5124236252545825\n",
      "Percentage of terminated:  0.5034566897112648\n",
      "Percentage of terminated:  0.5407286123618502\n",
      "Percentage of terminated:  0.49124949124949124\n",
      "Percentage of terminated:  0.5086136177194421\n",
      "Percentage of terminated:  0.5351418002466092\n",
      "Percentage of terminated:  0.549043549043549\n",
      "Percentage of terminated:  0.5548780487804879\n",
      "Percentage of terminated:  0.5058633238980995\n",
      "Percentage of terminated:  0.5194963444354184\n",
      "Percentage of terminated:  0.532953620829943\n",
      "Percentage of terminated:  0.4908200734394125\n",
      "Percentage of terminated:  0.5220588235294118\n",
      "Percentage of terminated:  0.5237708248679399\n",
      "Percentage of terminated:  0.5065466448445172\n",
      "Percentage of terminated:  0.39672131147540984\n",
      "Percentage of terminated:  0.5130187144019528\n",
      "Percentage of terminated:  0.5329512893982808\n",
      "Percentage of terminated:  0.5348742903487429\n",
      "Percentage of terminated:  0.521827825377397\n",
      "Percentage of terminated:  0.5086136177194421\n",
      "Percentage of terminated:  0.5073589533932952\n",
      "Percentage of terminated:  0.5543345543345544\n",
      "Percentage of terminated:  0.5363265306122449\n",
      "Percentage of terminated:  0.5214748784440842\n",
      "Percentage of terminated:  0.5319755600814664\n",
      "Percentage of terminated:  0.5006085192697769\n",
      "Percentage of terminated:  0.5024370430544273\n",
      "Percentage of terminated:  0.515274949083503\n",
      "Percentage of terminated:  0.5375203915171288\n",
      "Percentage of terminated:  0.5008169934640523\n",
      "Percentage of terminated:  0.5444941080861438\n",
      "Percentage of terminated:  0.5412433969930922\n",
      "Percentage of terminated:  0.501010101010101\n",
      "Percentage of terminated:  0.5307125307125307\n",
      "Percentage of terminated:  0.538109756097561\n",
      "Epoch 26, Train Loss: 2.7508, Validation Loss: 2.7817\n",
      "Percentage of terminated:  0.5495716034271726\n",
      "Percentage of terminated:  0.5319148936170213\n",
      "Percentage of terminated:  0.4879640962872297\n",
      "Percentage of terminated:  0.5529219452390682\n",
      "Percentage of terminated:  0.49232633279483035\n",
      "Percentage of terminated:  0.5246507806080526\n",
      "Percentage of terminated:  0.5165723524656427\n",
      "Percentage of terminated:  0.5360065466448445\n",
      "Percentage of terminated:  0.5619396903015484\n",
      "Percentage of terminated:  0.5139002452984465\n",
      "Percentage of terminated:  0.5018374846876276\n",
      "Percentage of terminated:  0.5444897959183673\n",
      "Percentage of terminated:  0.5232084690553745\n",
      "Percentage of terminated:  0.5746329526916802\n",
      "Percentage of terminated:  0.49736948603804126\n",
      "Percentage of terminated:  0.4840016200891049\n",
      "Percentage of terminated:  0.5089430894308943\n",
      "Percentage of terminated:  0.5775510204081633\n",
      "Percentage of terminated:  0.5148193260251726\n",
      "Percentage of terminated:  0.5421588594704684\n",
      "Percentage of terminated:  0.526079869600652\n",
      "Percentage of terminated:  0.5132598939208486\n",
      "Percentage of terminated:  0.49858012170385396\n",
      "Percentage of terminated:  0.5012224938875306\n",
      "Percentage of terminated:  0.527562270314414\n",
      "Percentage of terminated:  0.5036585365853659\n",
      "Percentage of terminated:  0.5336872192731728\n",
      "Percentage of terminated:  0.5156440471353109\n",
      "Percentage of terminated:  0.49470252648736757\n",
      "Percentage of terminated:  0.531723291035612\n",
      "Percentage of terminated:  0.49816998779991867\n",
      "Percentage of terminated:  0.5281862745098039\n",
      "Percentage of terminated:  0.5425315425315426\n",
      "Percentage of terminated:  0.5361667347772783\n",
      "Percentage of terminated:  0.5497147514262428\n",
      "Percentage of terminated:  0.5645621181262729\n",
      "Percentage of terminated:  0.5612535612535613\n",
      "Percentage of terminated:  0.5087862689006948\n",
      "Percentage of terminated:  0.5408038976857491\n",
      "Percentage of terminated:  0.5162206001622061\n",
      "Percentage of terminated:  0.5008196721311475\n",
      "Percentage of terminated:  0.5415472779369628\n",
      "Percentage of terminated:  0.5386493083807974\n",
      "Percentage of terminated:  0.5238866396761134\n",
      "Percentage of terminated:  0.5625\n",
      "Percentage of terminated:  0.5450819672131147\n",
      "Percentage of terminated:  0.5449369662464416\n",
      "Percentage of terminated:  0.5287637698898409\n",
      "Percentage of terminated:  0.5149406467458043\n",
      "Percentage of terminated:  0.5337398373983739\n",
      "Percentage of terminated:  0.5576923076923077\n",
      "Percentage of terminated:  0.5350162866449512\n",
      "Percentage of terminated:  0.5242560130452507\n",
      "Percentage of terminated:  0.5235964198535394\n",
      "Percentage of terminated:  0.5105433901054339\n",
      "Percentage of terminated:  0.5018315018315018\n",
      "Percentage of terminated:  0.5503246753246753\n",
      "Percentage of terminated:  0.5006109979633401\n",
      "Percentage of terminated:  0.5141815235008104\n",
      "Percentage of terminated:  0.5335760517799353\n",
      "Percentage of terminated:  0.5\n",
      "Percentage of terminated:  0.5190283400809717\n",
      "Percentage of terminated:  0.5203915171288744\n",
      "Percentage of terminated:  0.5061124694376528\n",
      "Percentage of terminated:  0.5176757415684681\n",
      "Percentage of terminated:  0.5444941080861438\n",
      "Percentage of terminated:  0.5687960687960688\n",
      "Percentage of terminated:  0.5188640973630831\n",
      "Percentage of terminated:  0.5403752039151712\n",
      "Percentage of terminated:  0.5580539656582175\n",
      "Percentage of terminated:  0.49753086419753084\n",
      "Percentage of terminated:  0.5045118949958983\n",
      "Percentage of terminated:  0.5399099467867376\n",
      "Percentage of terminated:  0.516470109800732\n",
      "Percentage of terminated:  0.535234215885947\n",
      "Percentage of terminated:  0.5277098614506928\n",
      "Percentage of terminated:  0.545751633986928\n",
      "Percentage of terminated:  0.5266585266585266\n",
      "Percentage of terminated:  0.483974358974359\n",
      "Percentage of terminated:  0.5371663244353183\n",
      "Percentage of terminated:  0.5155681358673676\n",
      "Percentage of terminated:  0.532599837000815\n",
      "Percentage of terminated:  0.5240619902120718\n",
      "Percentage of terminated:  0.5564186426819296\n",
      "Percentage of terminated:  0.5361371988566762\n",
      "Percentage of terminated:  0.5347702318015454\n",
      "Percentage of terminated:  0.5317848410757946\n",
      "Percentage of terminated:  0.5028432168968319\n",
      "Percentage of terminated:  0.5314542483660131\n",
      "Percentage of terminated:  0.5272136474411048\n",
      "Percentage of terminated:  0.5060975609756098\n",
      "Percentage of terminated:  0.5097165991902834\n",
      "Percentage of terminated:  0.5316045380875203\n",
      "Percentage of terminated:  0.5169734151329244\n",
      "Percentage of terminated:  0.534940743767879\n",
      "Percentage of terminated:  0.5259713701431493\n",
      "Percentage of terminated:  0.5193798449612403\n",
      "Percentage of terminated:  0.4955138662316476\n",
      "Percentage of terminated:  0.5398773006134969\n",
      "Epoch 27, Train Loss: 2.7474, Validation Loss: 2.7207\n",
      "Percentage of terminated:  0.5327368849125661\n",
      "Percentage of terminated:  0.5394951140065146\n",
      "Percentage of terminated:  0.5040883074407195\n",
      "Percentage of terminated:  0.5335229581470947\n",
      "Percentage of terminated:  0.5412054120541205\n",
      "Percentage of terminated:  0.5467099918765231\n",
      "Percentage of terminated:  0.5040617384240454\n",
      "Percentage of terminated:  0.5404081632653062\n",
      "Percentage of terminated:  0.5152501016673444\n",
      "Percentage of terminated:  0.5486365486365486\n",
      "Percentage of terminated:  0.5287403179779862\n",
      "Percentage of terminated:  0.5622457282343368\n",
      "Percentage of terminated:  0.543451652386781\n",
      "Percentage of terminated:  0.5517382413087935\n",
      "Percentage of terminated:  0.5389002036659878\n",
      "Percentage of terminated:  0.4952888160589922\n",
      "Percentage of terminated:  0.4989866234292663\n",
      "Percentage of terminated:  0.5040683482506102\n",
      "Percentage of terminated:  0.5634206219312602\n",
      "Percentage of terminated:  0.5538836925579503\n",
      "Percentage of terminated:  0.5619125459746629\n",
      "Percentage of terminated:  0.5321853218532185\n",
      "Percentage of terminated:  0.553469387755102\n",
      "Percentage of terminated:  0.5191680261011419\n",
      "Percentage of terminated:  0.5159313725490197\n",
      "Percentage of terminated:  0.5291734197730956\n",
      "Percentage of terminated:  0.49817147501015846\n",
      "Percentage of terminated:  0.5226808336738864\n",
      "Percentage of terminated:  0.530879345603272\n",
      "Percentage of terminated:  0.5132598939208486\n",
      "Percentage of terminated:  0.5215447154471544\n",
      "Percentage of terminated:  0.46496557310652087\n",
      "Percentage of terminated:  0.5010233319688907\n",
      "Percentage of terminated:  0.48714810281517745\n",
      "Percentage of terminated:  0.5328169588259274\n",
      "Percentage of terminated:  0.5312627707396812\n",
      "Percentage of terminated:  0.5085924713584288\n",
      "Percentage of terminated:  0.5323828920570265\n",
      "Percentage of terminated:  0.5420065252854812\n",
      "Percentage of terminated:  0.5491470349309504\n",
      "Percentage of terminated:  0.5012175324675324\n",
      "Percentage of terminated:  0.5436022819885901\n",
      "Percentage of terminated:  0.5257774140752864\n",
      "Percentage of terminated:  0.5408702724684831\n",
      "Percentage of terminated:  0.5504699632202698\n",
      "Percentage of terminated:  0.5224673202614379\n",
      "Percentage of terminated:  0.5161422149570903\n",
      "Percentage of terminated:  0.5230330207908683\n",
      "Percentage of terminated:  0.5300122498979175\n",
      "Percentage of terminated:  0.5230769230769231\n",
      "Percentage of terminated:  0.503655564581641\n",
      "Percentage of terminated:  0.5426008968609866\n",
      "Percentage of terminated:  0.5071283095723014\n",
      "Percentage of terminated:  0.5117982099267697\n",
      "Percentage of terminated:  0.540650406504065\n",
      "Percentage of terminated:  0.5098119378577269\n",
      "Percentage of terminated:  0.5490036600244002\n",
      "Percentage of terminated:  0.5154346060113729\n",
      "Percentage of terminated:  0.5320616883116883\n",
      "Percentage of terminated:  0.5458248472505092\n",
      "Percentage of terminated:  0.5171288743882545\n",
      "Percentage of terminated:  0.555962555962556\n",
      "Percentage of terminated:  0.5187296416938111\n",
      "Percentage of terminated:  0.5219512195121951\n",
      "Percentage of terminated:  0.52312730249693\n",
      "Percentage of terminated:  0.5028455284552845\n",
      "Percentage of terminated:  0.544093851132686\n",
      "Percentage of terminated:  0.5251623376623377\n",
      "Percentage of terminated:  0.5304952926729432\n",
      "Percentage of terminated:  0.5022330491270808\n",
      "Percentage of terminated:  0.5214748784440842\n",
      "Percentage of terminated:  0.5415472779369628\n",
      "Percentage of terminated:  0.5179153094462541\n",
      "Percentage of terminated:  0.5378664495114006\n",
      "Percentage of terminated:  0.5363377994315875\n",
      "Percentage of terminated:  0.5116374030216415\n",
      "Percentage of terminated:  0.5036348949919225\n",
      "Percentage of terminated:  0.5601304525071341\n",
      "Percentage of terminated:  0.5566343042071198\n",
      "Percentage of terminated:  0.5264012997562957\n",
      "Percentage of terminated:  0.5075295075295075\n",
      "Percentage of terminated:  0.5581870151082075\n",
      "Percentage of terminated:  0.5692868719611021\n",
      "Percentage of terminated:  0.5118174409127955\n",
      "Percentage of terminated:  0.5198526401964797\n",
      "Percentage of terminated:  0.5355102040816326\n",
      "Percentage of terminated:  0.4788273615635179\n",
      "Percentage of terminated:  0.5182481751824818\n",
      "Percentage of terminated:  0.515695067264574\n",
      "Percentage of terminated:  0.5850588712951685\n",
      "Percentage of terminated:  0.5024330900243309\n",
      "Percentage of terminated:  0.5142624286878565\n",
      "Percentage of terminated:  0.528814935064935\n",
      "Percentage of terminated:  0.5484266448712709\n",
      "Percentage of terminated:  0.5212244897959184\n",
      "Percentage of terminated:  0.5303215303215303\n",
      "Percentage of terminated:  0.5065252854812398\n",
      "Percentage of terminated:  0.49188311688311687\n",
      "Percentage of terminated:  0.448512585812357\n",
      "Epoch 28, Train Loss: 2.7425, Validation Loss: 2.7649\n",
      "Percentage of terminated:  0.5679869334422213\n",
      "Percentage of terminated:  0.5024449877750611\n",
      "Percentage of terminated:  0.5288617886178861\n",
      "Percentage of terminated:  0.5240619902120718\n",
      "Percentage of terminated:  0.5295566502463054\n",
      "Percentage of terminated:  0.5531391054575298\n",
      "Percentage of terminated:  0.5210696920583469\n",
      "Percentage of terminated:  0.5621687729311048\n",
      "Percentage of terminated:  0.5574642126789366\n",
      "Percentage of terminated:  0.5133033155955792\n",
      "Percentage of terminated:  0.534864643150123\n",
      "Percentage of terminated:  0.5565075479396164\n",
      "Percentage of terminated:  0.4973630831643002\n",
      "Percentage of terminated:  0.5643442622950819\n",
      "Percentage of terminated:  0.5313517915309446\n",
      "Percentage of terminated:  0.5242165242165242\n",
      "Percentage of terminated:  0.5220048899755502\n",
      "Percentage of terminated:  0.5253961804144657\n",
      "Percentage of terminated:  0.5149040424663128\n",
      "Percentage of terminated:  0.5370521172638436\n",
      "Percentage of terminated:  0.5075541037158023\n",
      "Percentage of terminated:  0.5151639344262295\n",
      "Percentage of terminated:  0.517579721995094\n",
      "Percentage of terminated:  0.548202614379085\n",
      "Percentage of terminated:  0.532546786004882\n",
      "Percentage of terminated:  0.5585585585585585\n",
      "Percentage of terminated:  0.5309553095530956\n",
      "Percentage of terminated:  0.491683569979716\n",
      "Percentage of terminated:  0.534447615165104\n",
      "Percentage of terminated:  0.5316558441558441\n",
      "Percentage of terminated:  0.543859649122807\n",
      "Percentage of terminated:  0.50997150997151\n",
      "Percentage of terminated:  0.5111561866125761\n",
      "Percentage of terminated:  0.5135792460478313\n",
      "Percentage of terminated:  0.5126221498371335\n",
      "Percentage of terminated:  0.5326307255776247\n",
      "Percentage of terminated:  0.5349309504467912\n",
      "Percentage of terminated:  0.5398698128559805\n",
      "Percentage of terminated:  0.5779514603044015\n",
      "Percentage of terminated:  0.5083776052308949\n",
      "Percentage of terminated:  0.49371196754563895\n",
      "Percentage of terminated:  0.5442676458588331\n",
      "Percentage of terminated:  0.5705521472392638\n",
      "Percentage of terminated:  0.5158536585365854\n",
      "Percentage of terminated:  0.5502024291497976\n",
      "Percentage of terminated:  0.5210867802108679\n",
      "Percentage of terminated:  0.5140073081607796\n",
      "Percentage of terminated:  0.5018374846876276\n",
      "Percentage of terminated:  0.48284214775938633\n",
      "Percentage of terminated:  0.5109223300970874\n",
      "Percentage of terminated:  0.5004074979625102\n",
      "Percentage of terminated:  0.5101461038961039\n",
      "Percentage of terminated:  0.535977105478332\n",
      "Percentage of terminated:  0.4987854251012146\n",
      "Percentage of terminated:  0.527169505271695\n",
      "Percentage of terminated:  0.5284618490109003\n",
      "Percentage of terminated:  0.5103954341622503\n",
      "Percentage of terminated:  0.5198040016333197\n",
      "Percentage of terminated:  0.5314401622718052\n",
      "Percentage of terminated:  0.5392755392755393\n",
      "Percentage of terminated:  0.5174108971732896\n",
      "Percentage of terminated:  0.5189718482252142\n",
      "Percentage of terminated:  0.5220588235294118\n",
      "Percentage of terminated:  0.5065040650406504\n",
      "Percentage of terminated:  0.5307881773399015\n",
      "Percentage of terminated:  0.5285481239804242\n",
      "Percentage of terminated:  0.46542660735948244\n",
      "Percentage of terminated:  0.5259501430322844\n",
      "Percentage of terminated:  0.4893704006541292\n",
      "Percentage of terminated:  0.486661277283751\n",
      "Percentage of terminated:  0.5006080259424402\n",
      "Percentage of terminated:  0.521810028536486\n",
      "Percentage of terminated:  0.534637326813366\n",
      "Percentage of terminated:  0.5408288879770209\n",
      "Percentage of terminated:  0.5537525354969574\n",
      "Percentage of terminated:  0.5077487765089723\n",
      "Percentage of terminated:  0.521827825377397\n",
      "Percentage of terminated:  0.517536704730832\n",
      "Percentage of terminated:  0.5326797385620915\n",
      "Percentage of terminated:  0.5264012997562957\n",
      "Percentage of terminated:  0.5579119086460033\n",
      "Percentage of terminated:  0.5149434571890146\n",
      "Percentage of terminated:  0.525050916496945\n",
      "Percentage of terminated:  0.5385865150284321\n",
      "Percentage of terminated:  0.5162866449511401\n",
      "Percentage of terminated:  0.5030549898167006\n",
      "Percentage of terminated:  0.5333333333333333\n",
      "Percentage of terminated:  0.513458401305057\n",
      "Percentage of terminated:  0.5119578435346575\n",
      "Percentage of terminated:  0.5322185970636215\n",
      "Percentage of terminated:  0.563265306122449\n",
      "Percentage of terminated:  0.5293152931529316\n",
      "Percentage of terminated:  0.5194805194805194\n",
      "Percentage of terminated:  0.5239642567018684\n",
      "Percentage of terminated:  0.5162337662337663\n",
      "Percentage of terminated:  0.5169595422966898\n",
      "Percentage of terminated:  0.5278460278460279\n",
      "Percentage of terminated:  0.545639771801141\n",
      "Percentage of terminated:  0.5235562310030395\n",
      "Epoch 29, Train Loss: 2.7478, Validation Loss: 2.7245\n",
      "Percentage of terminated:  0.5115900772671818\n",
      "Percentage of terminated:  0.4809660253786328\n",
      "Percentage of terminated:  0.5641656416564166\n",
      "Percentage of terminated:  0.47130153597413094\n",
      "Percentage of terminated:  0.5482688391038697\n",
      "Percentage of terminated:  0.5354107648725213\n",
      "Percentage of terminated:  0.5086136177194421\n",
      "Percentage of terminated:  0.5249088699878494\n",
      "Percentage of terminated:  0.47685374846374434\n",
      "Percentage of terminated:  0.5604887983706721\n",
      "Percentage of terminated:  0.5491235222176927\n",
      "Percentage of terminated:  0.5305126118795769\n",
      "Percentage of terminated:  0.5410172272354389\n",
      "Percentage of terminated:  0.5020491803278688\n",
      "Percentage of terminated:  0.520048602673147\n",
      "Percentage of terminated:  0.5111834078893859\n",
      "Percentage of terminated:  0.5323565323565324\n",
      "Percentage of terminated:  0.5353783231083845\n",
      "Percentage of terminated:  0.5122050447518307\n",
      "Percentage of terminated:  0.5434162250305749\n",
      "Percentage of terminated:  0.5502450980392157\n",
      "Percentage of terminated:  0.5263803680981595\n",
      "Percentage of terminated:  0.5259713701431493\n",
      "Percentage of terminated:  0.5226808336738864\n",
      "Percentage of terminated:  0.5421588594704684\n",
      "Percentage of terminated:  0.5043015157722245\n",
      "Percentage of terminated:  0.5257189145402997\n",
      "Percentage of terminated:  0.5360488798370672\n",
      "Percentage of terminated:  0.5140758873929009\n",
      "Percentage of terminated:  0.5057049714751426\n",
      "Percentage of terminated:  0.5518503456689712\n",
      "Percentage of terminated:  0.5083435083435084\n",
      "Percentage of terminated:  0.515077424612877\n",
      "Percentage of terminated:  0.5395329782875871\n",
      "Percentage of terminated:  0.5530703538023587\n",
      "Percentage of terminated:  0.4963592233009709\n",
      "Percentage of terminated:  0.5328733766233766\n",
      "Percentage of terminated:  0.5214922952149229\n",
      "Percentage of terminated:  0.48273059731816337\n",
      "Percentage of terminated:  0.5219333874898456\n",
      "Percentage of terminated:  0.5394951140065146\n",
      "Percentage of terminated:  0.5390243902439025\n",
      "Percentage of terminated:  0.5577786851776235\n",
      "Percentage of terminated:  0.5239837398373983\n",
      "Percentage of terminated:  0.5245237130117552\n",
      "Percentage of terminated:  0.4853539462978031\n",
      "Percentage of terminated:  0.5525244299674267\n",
      "Percentage of terminated:  0.5373073803730738\n",
      "Percentage of terminated:  0.5382755842062853\n",
      "Percentage of terminated:  0.5134255492270138\n",
      "Percentage of terminated:  0.5489396411092985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class EmbeddingModels(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_features = 10 * 2\n",
    "        self.base_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.num_features,)),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "        ])\n",
    "        self.embedding_head = tf.keras.layers.Dense(self.num_features + 1)\n",
    "        self.prediction_head = tf.keras.layers.Dense(self.num_features + 1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        embedding_space = self.embedding_head(x)\n",
    "        prediction = self.prediction_head(x)\n",
    "        return embedding_space, prediction, x\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_features': self.num_features,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def embedding_loss(distribution_true, distribution_embedding):\n",
    "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(distribution_true, distribution_embedding)\n",
    "\n",
    "@tf.function\n",
    "def prediction_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.CategoricalCrossentropy(from_logits=True)(y_true, y_pred)\n",
    "\n",
    "def x_masked(x, d=20):\n",
    "    masks = np.concatenate(\n",
    "        [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(int(d/3))], 1, keepdims=True) for _ in range(x.shape[0])], 1\n",
    "        )\n",
    "    masks = np.float32(masks.T)\n",
    "\n",
    "    masks_zero = np.sum(masks, axis=1) == 0\n",
    "    \n",
    "    while np.sum(masks_zero) > 0:\n",
    "        # masks[masks_zero] = np.float32(np.concatenate(\n",
    "        #     [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(np.sum(masks_zero))], 1\n",
    "        #     )).T\n",
    "        masks[masks_zero] = np.float32(np.concatenate(\n",
    "            [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(int(d/3))], 1, keepdims=True) for _ in range(np.sum(masks_zero))], 1\n",
    "            )).T\n",
    "        \n",
    "        masks_zero = np.sum(masks, axis=1) == 0\n",
    "        \n",
    "    return np.copy(x), masks\n",
    "\n",
    "\n",
    "def compute_loss_timestep(y_true, x_data, mask, classifier, acquisition_cost, loss_function, num_timestamps=10, num_modalities=2):\n",
    "    y_pred = classifier.predict_proba(np.concatenate([x_data * mask, mask], axis=1))\n",
    "    y_pred = torch.Tensor(y_pred)\n",
    "    if isinstance(y_true, tf.Tensor):\n",
    "        y_true = torch.Tensor(y_true.numpy())\n",
    "    else:\n",
    "        y_true = torch.Tensor(y_true)\n",
    "    total_cost = torch.zeros(mask.shape[0])\n",
    "    total_cost += mask.sum(axis=1) * acquisition_cost\n",
    "    total_loss = loss_function(y_pred, y_true).numpy() + total_cost.numpy()\n",
    "    return total_loss\n",
    "\n",
    "def get_potential_features(x, y, classifier, prev_masks, acquisition_cost, d=20, num_masks=1500, topk=5, num_timestamps=10, num_modalities=2):\n",
    "    new_masks = np.concatenate(\n",
    "        [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(num_masks)], 1\n",
    "        )\n",
    "    new_masks = np.float32(new_masks.T)\n",
    "\n",
    "    # repeat for parallelization\n",
    "    x_rep = np.repeat(x, num_masks, axis=0)\n",
    "    y_rep = np.repeat(y, num_masks, axis=0)\n",
    "    new_masks = np.concatenate([new_masks for _ in range(x.shape[0])], 0)\n",
    "    prev_masks_rep = np.repeat(prev_masks, num_masks, axis=0)\n",
    "\n",
    "    # combine previous masks with new masks\n",
    "    N = x_rep.shape[0]\n",
    "    segments_previous = prev_masks_rep.reshape(N, num_modalities, num_timestamps)\n",
    "    segments_after = new_masks.reshape(N, num_modalities, num_timestamps)\n",
    "    last_indices = np.where(segments_previous == 1, np.arange(num_timestamps), -1)\n",
    "    last_indices_per_segment = np.max(last_indices, axis=2)  \n",
    "    final_last_index = np.max(last_indices_per_segment, axis=1)  \n",
    "    mask = np.arange(num_timestamps)[None, None, :] > final_last_index[:, None, None]\n",
    "    final_segments = np.where(mask, segments_after, segments_previous)\n",
    "    final = final_segments.reshape(N, num_timestamps * num_modalities)\n",
    "    \n",
    "    # compute loss for current and future masks\n",
    "    current_loss = compute_loss_timestep(y, x, prev_masks, classifier, acquisition_cost, nn.CrossEntropyLoss(reduction='none'))\n",
    "    future_loss = compute_loss_timestep(y_rep, x_rep, final, classifier, acquisition_cost, nn.CrossEntropyLoss(reduction='none'))\n",
    "    \n",
    "    # get the top k min loss\n",
    "    top_k_sets = []\n",
    "    distributions = []\n",
    "    terminations = []\n",
    "    for i in range(x.shape[0]):\n",
    "        min_current_loss = current_loss[i]\n",
    "        single_sample_future_loss = np.copy(future_loss[i * num_masks: (i + 1) * num_masks])\n",
    "        single_sample_future_set = np.copy(final[i * num_masks: (i + 1) * num_masks])\n",
    "        \n",
    "        top_k_loss = np.argsort(single_sample_future_loss)\n",
    "        min_k_future_loss = single_sample_future_loss[top_k_loss]\n",
    "        min_k_subsets = single_sample_future_set[top_k_loss]\n",
    "        \n",
    "        unique_min_k_subsets, unique_indices = np.unique(min_k_subsets, axis=0, return_index=True)\n",
    "        unique_min_k_future_loss = min_k_future_loss[unique_indices]\n",
    "        \n",
    "        sorted_unique_indices = np.argsort(unique_min_k_future_loss)\n",
    "        unique_min_k_future_loss = unique_min_k_future_loss[sorted_unique_indices]\n",
    "        unique_min_k_subsets = unique_min_k_subsets[sorted_unique_indices]\n",
    "        \n",
    "        if unique_min_k_subsets.shape[0] <= 1:\n",
    "            topk_min = topk\n",
    "            if np.equal(unique_min_k_subsets[0], prev_masks[i]).all():\n",
    "                termination = np.ones(topk)\n",
    "                unique_min_k_subsets = np.repeat(prev_masks[i][None, :], topk, axis=0)\n",
    "                subset_losses = np.ones(topk) * min_current_loss\n",
    "            else:\n",
    "                if unique_min_k_future_loss[0] >= min_current_loss:\n",
    "                    termination = np.ones(topk)\n",
    "                    unique_min_k_subsets = np.repeat(prev_masks[i][None, :], topk, axis=0)\n",
    "                    subset_losses = np.ones(topk) * min_current_loss\n",
    "                else: \n",
    "                    termination = np.zeros(topk)\n",
    "                    unique_min_k_subsets = np.repeat(unique_min_k_subsets[0], topk, axis=0)\n",
    "                    subset_losses = np.repeat(unique_min_k_future_loss[0], topk)\n",
    "        else: \n",
    "            topk_min = min(topk, unique_min_k_subsets.shape[0])\n",
    "            termination = np.zeros(topk_min)\n",
    "            subset_losses = unique_min_k_future_loss[:topk_min]\n",
    "\n",
    "            for j in range(topk_min):\n",
    "                if unique_min_k_future_loss[j] >= min_current_loss:\n",
    "                    termination[j] = 1\n",
    "                    unique_min_k_subsets[j] = prev_masks[i]\n",
    "                    \n",
    "        unique_min_k_subsets = unique_min_k_subsets[:topk_min]\n",
    "        unique_min_k_subsets[:,:d] -= prev_masks[i]\n",
    "        # get the first 1 in each segment\n",
    "        # todo: consider changing this into a for loop, might be faster\n",
    "        n = unique_min_k_subsets.shape[0]\n",
    "        segment_size = num_timestamps\n",
    "        num_segments = num_modalities\n",
    "        segments = unique_min_k_subsets.reshape(n, num_segments, segment_size)\n",
    "        first_one_indices = np.where(segments == 1, np.arange(segment_size), np.inf)\n",
    "        first_one_per_segment = np.min(first_one_indices, axis=2)\n",
    "        first_one_per_segment[first_one_per_segment == np.inf] = -1\n",
    "        global_first_one = np.min(\n",
    "            np.where(first_one_per_segment != -1, first_one_per_segment, np.inf), axis=1\n",
    "        )\n",
    "        global_first_one = np.where(global_first_one == np.inf, -1, global_first_one)\n",
    "        keep_indices = (first_one_indices == global_first_one[:, None, None])\n",
    "        transformed_masks = np.zeros_like(unique_min_k_subsets)\n",
    "        transformed_masks[keep_indices.reshape(unique_min_k_subsets.shape)] = 1\n",
    "        unique_min_k_subsets = np.copy(transformed_masks)\n",
    "        unique_min_k_subsets = np.concatenate([unique_min_k_subsets, termination[:, None]], axis=1) # shape (topk, d+1)\n",
    "        \n",
    "        # turn into probabilities\n",
    "        unique_min_k_subsets = unique_min_k_subsets / np.sum(unique_min_k_subsets, axis=1)[:, None]\n",
    "        subset_losses = subset_losses[:topk_min]\n",
    "        weights = np.exp(-subset_losses[:topk_min])\n",
    "        weights /= np.sum(weights)\n",
    "        distribution = np.sum(unique_min_k_subsets * weights[:, None], axis=0) \n",
    "        \n",
    "        distributions.append(distribution)\n",
    "        terminations.append(termination)\n",
    "\n",
    "    distributions = np.array(distributions) # shape (batch_size, d)\n",
    "    distributions /= (distributions.sum(axis=1, keepdims=True) + 1e-12) # normalize for numerical stability\n",
    "    # print(\"check sum of distributions: \", np.sum(distributions, axis=1))\n",
    "    \n",
    "    next_action = np.array([np.random.choice(distributions.shape[1], p=distributions[i]) for i in range(distributions.shape[0])])\n",
    "\n",
    "    terminations = np.concatenate(terminations)\n",
    "    print(\"Percentage of terminated: \", np.mean(terminations))\n",
    "    \n",
    "    return top_k_sets, distributions, next_action\n",
    "    \n",
    "# @tf.function\n",
    "def train_step(model, optimizer, x, y, classifier, acquisition_cost, alpha=1):\n",
    "    x, prev_masks = x_masked(x)\n",
    "    top_k_sets, distributions, next_feature = get_potential_features(np.copy(x), np.copy(y), classifier, np.copy(prev_masks), acquisition_cost)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        embedding_space, prediction, embedding = model(x*prev_masks)\n",
    "        next_feature = tf.one_hot(next_feature, 21)\n",
    "        loss = alpha * embedding_loss(distributions, embedding_space) + prediction_loss(next_feature, prediction)\n",
    "\n",
    "    # Backprop\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, x, y, classifier, acquisition_cost, alpha=1):\n",
    "    x, prev_masks = x_masked(x)\n",
    "\n",
    "    _, distributions, next_feature = get_potential_features(\n",
    "        x, y, classifier, np.copy(prev_masks), acquisition_cost\n",
    "    )\n",
    "    \n",
    "    embedding_space, prediction, embedding = model(x * prev_masks)\n",
    "\n",
    "    next_feature_onehot = tf.one_hot(next_feature, 21) \n",
    "\n",
    "    loss = (alpha * embedding_loss(distributions, embedding_space) \n",
    "            + prediction_loss(next_feature_onehot, prediction))\n",
    "    return loss\n",
    "\n",
    "\n",
    "X, X_val, y, y_val = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.shuffle(10000).batch(512).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "model = EmbeddingModels()\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "acquisition_cost = 0.015\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    step_count = 0\n",
    "\n",
    "    for batch_x, batch_y in dataset:\n",
    "        loss = train_step(model, optimizer, batch_x, batch_y, classifier, acquisition_cost)\n",
    "        epoch_loss += loss.numpy()\n",
    "        step_count += 1\n",
    "    train_loss = epoch_loss / step_count\n",
    "\n",
    "    val_loss_sum = 0.0\n",
    "    val_steps = 0\n",
    "    for val_x_batch, val_y_batch in val_dataset:\n",
    "        loss_val = validation_step(model, val_x_batch, val_y_batch, classifier, acquisition_cost)\n",
    "        val_loss_sum += loss_val.numpy()\n",
    "        val_steps += 1\n",
    "    val_loss = val_loss_sum / val_steps\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        model.save('/work/users/d/d/ddinh/aaco/models/embeddingembedding_synthetic_raw_gt.h5')\n",
    "        model.save('/work/users/d/d/ddinh/aaco/models/embedding_synthetic_raw.keras')\n",
    "        model.save_weights('/work/users/d/d/ddinh/aaco/models/embedding_synthetic_raw.weights.h5')\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "# add the regularizer same as divdis to avoid always predic as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('/work/users/d/d/ddinh/aaco/models/embedding_gt.h5')\n",
    "model.save('/work/users/d/d/ddinh/aaco/models/embedding_gt.keras')\n",
    "model.save_weights('/work/users/d/d/ddinh/aaco/models/embedding_gt.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing masks...\n",
      "computing predictions...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     mask_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(mask_input)\n\u001b[1;32m     45\u001b[0m     ts_rep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(ts, x_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_rep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     y_pred[:,ts,:] \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(y_pred)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:450\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ):\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:668\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[0;32m--> 668\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[1;32m    670\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[1;32m    671\u001b[0m         dataset\n\u001b[1;32m    672\u001b[0m     )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:677\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:140\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# performance.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m        A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2311\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    113\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1251\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1250\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1221\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1225\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:691\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;241m=\u001b[39m created_variables\n\u001b[0;32m--> 691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_scoped_tracing_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_capturing_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScopeType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVARIABLE_CREATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[1;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    698\u001b[0m )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:604\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options\u001b[0;34m(self, scope, scope_type)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    602\u001b[0m weak_wrapped_fn \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(wrapped_fn)\n\u001b[0;32m--> 604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_tracing_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_decorator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_decorator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapped_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:646\u001b[0m, in \u001b[0;36mFunction._generate_tracing_options\u001b[0;34m(self, fn, scope_type)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autograph:\n\u001b[1;32m    643\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph_util\u001b[38;5;241m.\u001b[39mpy_func_from_autograph(\n\u001b[1;32m    644\u001b[0m       fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_autograph_options)\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTracingOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolymorphic_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscope_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_retracing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_retracing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_captures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_captures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:16\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, python_function, name, polymorphic_type, default_values, scope_type, attributes, autograph, autograph_options, reduce_retracing, bind_graph_to_function, function_cache, function_captures, lock)\u001b[0m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:112\u001b[0m, in \u001b[0;36mTracingOptions.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolymorphic_type \u001b[38;5;241m=\u001b[39m function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType\u001b[38;5;241m.\u001b[39mfrom_callable(\n\u001b[1;32m    106\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_function\n\u001b[1;32m    107\u001b[0m   )\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_values \u001b[38;5;241m=\u001b[39m function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType\u001b[38;5;241m.\u001b[39mget_default_values(\n\u001b[1;32m    109\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_function\n\u001b[1;32m    110\u001b[0m   )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_signature \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_input_signature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolymorphic_type\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:169\u001b[0m, in \u001b[0;36mto_input_signature\u001b[0;34m(function_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameter\u001b[38;5;241m.\u001b[39mtype_constraint:\n\u001b[1;32m    163\u001b[0m   \u001b[38;5;66;03m# Generate legacy constraint representation.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m   constraint \u001b[38;5;241m=\u001b[39m parameter\u001b[38;5;241m.\u001b[39mtype_constraint\u001b[38;5;241m.\u001b[39mplaceholder_value(\n\u001b[1;32m    165\u001b[0m       trace_type\u001b[38;5;241m.\u001b[39mInternalPlaceholderContext(unnest_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m   )\n\u001b[1;32m    167\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    168\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, tensor\u001b[38;5;241m.\u001b[39mTensorSpec)\n\u001b[0;32m--> 169\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m   ):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# input_signature only supports contiguous TensorSpec composites\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     is_auto_constrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/nest.py:293\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.flatten\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten\u001b[39m(structure, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a flat list from a given structure.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/nest_util.py:712\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(modality, structure, expand_composites)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Flattens a nested structure.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m  TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m--> 712\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m    714\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_flatten(structure)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/nest_util.py:726\u001b[0m, in \u001b[0;36m_tf_core_flatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    724\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    725\u001b[0m expand_composites \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(expand_composites)\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pywrap_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_MASKS_CURRENT = 100\n",
    "NUM_MASKS_FUTURE = 100\n",
    "NUM_TS = 12\n",
    "NUM_MODALITIES = 4\n",
    "acquisition_cost = 0.1\n",
    "d = NUM_TS * NUM_MODALITIES\n",
    "loss_function = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "print(\"computing masks...\")\n",
    "\n",
    "ts_current = np.random.randint(0, num_ts, NUM_MASKS_CURRENT)\n",
    "b_c = []\n",
    "for i in range(NUM_MASKS_CURRENT):\n",
    "    mask_temp = []\n",
    "    for j in range(NUM_MODALITIES): \n",
    "        mask = np.sum(np.random.permutation(np.eye(NUM_TS))[:,:ts_current[i]+1][:, :np.random.randint(ts_current[i]+1)], 1)\n",
    "        mask_temp.append(mask)\n",
    "    b_c.append(mask_temp)\n",
    "b_c = np.array(b_c)\n",
    "b_c = np.transpose(b_c, (0, 2, 1)).reshape(-1, d)\n",
    "b_empty = np.sum(b_c, 1) == 0\n",
    "b_c = b_c[~b_empty]\n",
    "b_c = np.unique(b_c, axis=0)\n",
    "num_mask = b_c.shape[0]\n",
    "\n",
    "\n",
    "x_current = np.repeat(train_x, num_mask, axis=0)\n",
    "y_current = np.repeat(train_y, num_mask, axis=0)\n",
    "\n",
    "b_current = np.concatenate([b_c for _ in range(train_x.shape[0])], axis=0)\n",
    "print(\"computing predictions...\")\n",
    "\n",
    "y_pred = np.zeros(y_current.shape)\n",
    "for ts in range(NUM_TS):\n",
    "    print(ts)\n",
    "    x_input = np.zeros(x_current.shape)\n",
    "    mask_input = np.zeros(b_current.shape)\n",
    "    \n",
    "    for k in range(NUM_MODALITIES):\n",
    "        x_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(x_current[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "        mask_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(b_current[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "    \n",
    "    x_input = torch.Tensor(x_input)\n",
    "    mask_input = torch.Tensor(mask_input.numpy())\n",
    "    ts_rep = np.repeat(ts, x_input.shape[0]).reshape(-1, 1)\n",
    "    pred = classifier.predict(np.concatenate([x_input * mask_input, ts_rep], axis=-1), verbose=0)\n",
    "    y_pred[:,ts,:] = pred\n",
    "y_pred = torch.Tensor(y_pred)\n",
    "y_current = torch.Tensor(y_current)\n",
    "print(\"computing loss...\")\n",
    "cost = torch.zeros(b_current.shape[0])\n",
    "for m in range(NUM_MODALITIES):\n",
    "    modality_cost = 1 if m in [0, 1] else 0.5\n",
    "    cost += b_current[:, m * NUM_TS: (m + 1) * NUM_TS].sum(1) * acquisition_cost * modality_cost\n",
    "loss_current = compute_accumulated_loss(y_pred, y_current, loss_function) + cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing best mask...\n",
      "x_future shape:  (6355800, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"computing best mask...\")\n",
    "# while loop here to make sure mask is not 0\n",
    "Bf = []\n",
    "for i in range(NUM_MASKS_CURRENT):\n",
    "    for j in range(NUM_MASKS_FUTURE): \n",
    "        mask_temp = []\n",
    "        for j in range(NUM_MODALITIES): \n",
    "            mask = np.sum(np.random.permutation(np.eye(NUM_TS))[:,ts_current[i]+1:][:, :np.random.randint(NUM_TS-ts_current[i]+1)], 1)\n",
    "            mask_temp.append(mask)\n",
    "        Bf.append(mask_temp)\n",
    "Bf = np.array(Bf)\n",
    "Bf = np.transpose(Bf, (0, 2, 1)).reshape(-1, d)\n",
    "    \n",
    "Bf[0] = np.zeros(d) # current mask \n",
    "Bf = np.unique(Bf, axis=0)\n",
    "\n",
    "num_mask_future = Bf.shape[0]\n",
    " \n",
    "x_future = np.repeat(x_current, num_mask_future, axis=0)\n",
    "y_future = np.repeat(y_current, num_mask_future, axis=0)\n",
    "print(\"x_future shape: \", x_future.shape)\n",
    "\n",
    "b_current_for_future = np.repeat(b_c, num_mask_future, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bforb = np.repeat(Bf, num_mask, axis=0)\n",
    "\n",
    "Bforb = np.minimum(b_current_for_future+Bforb, 1)\n",
    "Bforb = np.concatenate([Bforb for _ in range(train_x.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing predictions...\n",
      "0\n",
      "\u001b[1m198619/198619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 394us/step\n",
      "1\n",
      "\u001b[1m198619/198619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 389us/step\n",
      "2\n",
      "\u001b[1m198619/198619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 394us/step\n",
      "3\n",
      "\u001b[1m 62848/198619\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 383us/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     mask_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(mask_input)\n\u001b[1;32m     14\u001b[0m     ts_rep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(ts, x_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_rep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     y_pred[:,ts,:] \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputing loss...\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:512\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    510\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    511\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[0;32m--> 512\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[1;32m    514\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"computing predictions...\")\n",
    "y_pred = np.zeros(y_future.shape)\n",
    "for ts in range(NUM_TS):\n",
    "    print(ts)\n",
    "    x_input = np.zeros(x_future.shape)\n",
    "    mask_input = np.zeros(Bforb.shape)\n",
    "    \n",
    "    for k in range(NUM_MODALITIES):\n",
    "        x_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(x_future[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "        mask_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(Bforb[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "    \n",
    "    x_input = torch.Tensor(x_input)\n",
    "    mask_input = torch.Tensor(mask_input)\n",
    "    ts_rep = np.repeat(ts, x_input.shape[0]).reshape(-1, 1)\n",
    "    pred = classifier.predict(np.concatenate([x_input * mask_input, ts_rep], axis=-1), verbose=1)\n",
    "    y_pred[:,ts,:] = pred\n",
    "\n",
    "print(\"computing loss...\")    \n",
    "y_pred = torch.Tensor(y_pred)\n",
    "y_future = torch.Tensor(y_future)\n",
    "cost = torch.zeros(Bforb.shape[0])\n",
    "for m in range(NUM_MODALITIES):\n",
    "    modality_cost = 1 if m in [0, 1] else 0.5\n",
    "    cost += Bforb[:, m * NUM_TS: (m + 1) * NUM_TS].sum(1) * acquisition_cost * modality_cost\n",
    "loss_future = compute_accumulated_loss(y_pred, y_future, loss_function) + cost \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[758316. 758987. 758008. 757953. 757620. 757955. 757972. 758435. 757102.\n",
      " 756993. 757385. 757279. 757917. 757722. 757728. 757501. 757812. 758526.\n",
      " 758420. 757669. 758158. 758848. 758452. 758176. 757722. 758034. 758743.\n",
      " 759001. 758049. 757567. 758648. 758536. 757489. 756505. 757903. 757405.\n",
      " 757466. 757965. 758633. 757916. 758311. 757459. 758096. 757114. 757111.\n",
      " 757801. 758337. 757861.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1418396, 49), (1418396, 3), (1418396, 48))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masksper = 256 \n",
    "d = train_x_ts.shape[1] - 1\n",
    "X_class = np.concatenate([train_x_ts]*masksper, 0)\n",
    "Y_class = np.concatenate([train_y_ts]*masksper, 0)\n",
    "B = np.concatenate(\n",
    "[np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class.shape[0])],\n",
    "1)\n",
    "B = np.float32(B.T)\n",
    "\n",
    "zero_mask = X_class[:,:d] == 0\n",
    "B[zero_mask] = 0\n",
    "# remove 0 mask\n",
    "mask_nonzero = np.sum(B, axis=1) != 0\n",
    "B = B[mask_nonzero]\n",
    "X_class = X_class[mask_nonzero]\n",
    "Y_class = Y_class[mask_nonzero]\n",
    "\n",
    "X_class[:,:d] = X_class[:,:d] * B\n",
    "X_class.shape, Y_class.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_current = np.random.randint(0, num_ts, NUM_MASKS_CURRENT)\n",
    "b_current = []\n",
    "for i in range(NUM_MASKS_CURRENT):\n",
    "    mask_temp = []\n",
    "    for j in range(NUM_MODALITIES): \n",
    "        mask = np.sum(np.random.permutation(np.eye(NUM_TS))[:,:ts_current[i]+1][:, :np.random.randint(ts_current[i]+1)], 1)\n",
    "        mask_temp.append(mask)\n",
    "    b_current.append(mask_temp)\n",
    "    \n",
    "b_current = np.array(b_current)\n",
    "b_current = np.transpose(b_current, (1, 0, 2)).reshape(-1, NUM_MODALITIES * NUM_TS)\n",
    "b_empty = np.sum(b_current, 1) == 0\n",
    "b_current = b_current[~b_empty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5595, 49), (5595, 3), (5595, 49))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masksper = 5\n",
    "d = val_x_ts.shape[1]\n",
    "X_class_val = np.concatenate([val_x_ts]*masksper, 0)\n",
    "Y_class_val = np.concatenate([val_y_ts]*masksper, 0)\n",
    "B_val = np.concatenate(\n",
    "[np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class_val.shape[0])],\n",
    "1)\n",
    "B_val = np.float32(B_val.T)\n",
    "\n",
    "# B_val = np.ones_like(B_val)\n",
    "\n",
    "zero_mask_val = X_class_val[:,:d] == 0\n",
    "B_val[zero_mask_val] = 0\n",
    "\n",
    "# remove 0 mask\n",
    "mask_nonzero_val = np.sum(B_val, axis=1) != 0\n",
    "B_val = B_val[mask_nonzero_val]\n",
    "X_class_val = X_class_val[mask_nonzero_val]\n",
    "Y_class_val = Y_class_val[mask_nonzero_val]\n",
    "\n",
    "X_class_val[:,:d] = X_class_val[:,:d] * B_val\n",
    "X_class_val.shape, Y_class_val.shape, B_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132588, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(logits, axis=1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m35373/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.6022 - loss: 0.7826 - prc: 0.6291 - roc_auc: 0.8070\n",
      "Epoch 1: val_roc_auc improved from -inf to 0.76294, saving model to /work/users/d/d/ddinh/aaco/models/mlp_test.keras\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1ms/step - accuracy: 0.6022 - loss: 0.7826 - prc: 0.6291 - roc_auc: 0.8070 - val_accuracy: 0.5645 - val_loss: 0.9315 - val_prc: 0.5431 - val_roc_auc: 0.7629\n",
      "Epoch 2/100\n",
      "\u001b[1m35363/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6201 - loss: 0.7467 - prc: 0.6643 - roc_auc: 0.8241\n",
      "Epoch 2: val_roc_auc improved from 0.76294 to 0.76604, saving model to /work/users/d/d/ddinh/aaco/models/mlp_test.keras\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1ms/step - accuracy: 0.6201 - loss: 0.7467 - prc: 0.6643 - roc_auc: 0.8241 - val_accuracy: 0.5657 - val_loss: 0.9197 - val_prc: 0.5514 - val_roc_auc: 0.7660\n",
      "Epoch 3/100\n",
      "\u001b[1m35340/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.6223 - loss: 0.7426 - prc: 0.6686 - roc_auc: 0.8261\n",
      "Epoch 3: val_roc_auc improved from 0.76604 to 0.76734, saving model to /work/users/d/d/ddinh/aaco/models/mlp_test.keras\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 973us/step - accuracy: 0.6223 - loss: 0.7426 - prc: 0.6686 - roc_auc: 0.8261 - val_accuracy: 0.5675 - val_loss: 0.9245 - val_prc: 0.5521 - val_roc_auc: 0.7673\n",
      "Epoch 4/100\n",
      "\u001b[1m35324/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.6237 - loss: 0.7398 - prc: 0.6713 - roc_auc: 0.8275\n",
      "Epoch 4: val_roc_auc did not improve from 0.76734\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 955us/step - accuracy: 0.6237 - loss: 0.7398 - prc: 0.6713 - roc_auc: 0.8275 - val_accuracy: 0.5539 - val_loss: 0.9287 - val_prc: 0.5460 - val_roc_auc: 0.7616\n",
      "Epoch 5/100\n",
      "\u001b[1m 4567/35377\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 939us/step - accuracy: 0.6242 - loss: 0.7391 - prc: 0.6721 - roc_auc: 0.8277"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, accuracy, roc_auc\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# history = train_model(model, train_x_ts, train_y_ts, val_x_ts, val_y_ts)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# evaluate_model(model, val_x_ts, val_y_ts)\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_class_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_class_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m evaluate_model(model, X_class_val, Y_class_val)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_history\u001b[39m(history):\n",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     42\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_roc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     43\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     44\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \n\u001b[1;32m     45\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/device:GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "TOTAL_FEATURES = train_x_ts.shape[-1]\n",
    "HIDDEN_DIM = 10\n",
    "NUM_CLASSES = train_y_ts.shape[-1]\n",
    "DROPOUT_RATE = 0.75\n",
    "\n",
    "def create_mlp():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(TOTAL_FEATURES,)),\n",
    "        layers.Dense(HIDDEN_DIM, activation='relu'),\n",
    "        # layers.Dropout(DROPOUT_RATE),\n",
    "        layers.Dense(HIDDEN_DIM, activation='relu'),\n",
    "        # layers.Dropout(DROPOUT_RATE),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model = create_mlp()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', AUC(name='roc_auc'), AUC(name='prc', curve='PR')]\n",
    ")\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        '/work/users/d/d/ddinh/aaco/models/mlp_embedding.keras',  \n",
    "        monitor='val_roc_auc',\n",
    "        mode='max',  \n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_roc_auc', \n",
    "        mode='max', \n",
    "        patience=10,  \n",
    "        verbose=1\n",
    "    )\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[checkpoint, early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    loss, accuracy, roc_auc, prc = model.evaluate(X, y, verbose=0)\n",
    "    print(f\"Loss: {loss}, Accuracy: {accuracy}, ROC AUC: {roc_auc}, PRC: {prc}\")\n",
    "    return loss, accuracy, roc_auc\n",
    "\n",
    "# history = train_model(model, train_x_ts, train_y_ts, val_x_ts, val_y_ts)\n",
    "# evaluate_model(model, val_x_ts, val_y_ts)\n",
    "history = train_model(model, X_class, Y_class, X_class_val, Y_class_val)\n",
    "evaluate_model(model, X_class_val, Y_class_val)\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['roc_auc'])\n",
    "    plt.plot(history.history['val_roc_auc'])\n",
    "    plt.title('model roc_auc')\n",
    "    plt.ylabel('roc_auc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['prc'])\n",
    "    plt.plot(history.history['val_prc'])\n",
    "    plt.title('model prc')\n",
    "    plt.ylabel('prc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "plot_history(history)\n",
    "#  val_prc: 0.5998 - val_roc_auc: 0.7917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_adni_data()\n",
    "x = dataset.x\n",
    "y = dataset.y\n",
    "\n",
    "mask_nan = np.isnan(x)\n",
    "\n",
    "x[mask_nan] = 0\n",
    "\n",
    "num_ts = y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06387226 0.64271457 0.29341317]\n",
      "[0.08805668 0.61234818 0.29959514]\n",
      "[0.10761421 0.59187817 0.30050761]\n",
      "[0.125      0.58974359 0.28525641]\n",
      "[0.16613757 0.54179894 0.29206349]\n",
      "[0.13573883 0.56872852 0.29553265]\n",
      "[0.178125  0.5796875 0.2421875]\n",
      "[0.18283582 0.53731343 0.27985075]\n",
      "[0.17350746 0.46455224 0.3619403 ]\n",
      "[0.18235294 0.45294118 0.36470588]\n",
      "[0.18596491 0.49473684 0.31929825]\n",
      "[0.17098446 0.43005181 0.39896373]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_ts):\n",
    "    count = [0,0,0]\n",
    "    total = 0\n",
    "    for j in range(y.shape[0]):\n",
    "        for k in range(3):\n",
    "            if y[j,i,k] == 1:\n",
    "                count[k] += 1\n",
    "                total += 1\n",
    "    count = np.array(count)\n",
    "    print(count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13054449 0.56998672 0.29946879]\n"
     ]
    }
   ],
   "source": [
    "count = [0,0,0]\n",
    "total = 0\n",
    "for i in range(num_ts):\n",
    "    for j in range(y.shape[0]):\n",
    "        for k in range(3):\n",
    "            if y[j,i,k] == 1:\n",
    "                count[k] += 1\n",
    "                total += 1\n",
    "count = np.array(count)\n",
    "print(count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40976380572188953"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_nan = x==0\n",
    "# get the number of nan values\n",
    "total = mask_nan.sum()\n",
    "probability = total / x.size\n",
    "probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((801, 12, 4), (201, 12, 4), (801, 12, 3), (201, 12, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into train and test sets 80/20 with random seed 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = prepare_time_series(torch.Tensor([range(0,12)] * x_train.shape[0]), torch.Tensor(x_train))[1]\n",
    "# y_train = prepare_time_series(torch.Tensor([range(0,12)] * y_train.shape[0]), torch.Tensor(y_train))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = torch.cat([x_train, x_train[:, [-1]]], dim=-2).numpy()\n",
    "# y_train = torch.cat([y_train, y_train[:, [-1]]], dim=-2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.round(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ts = []\n",
    "# y_ts = []\n",
    "\n",
    "# for i in range(num_ts): \n",
    "#     x_temp = np.zeros_like(x_train)\n",
    "#     x_temp[:, :i+1, :] = x_train[:, :i+1, :]\n",
    "#     x_ts.append(x_temp)    \n",
    "#     y_ts.append(y_train)\n",
    "# x_ts = np.concatenate(x_ts, axis=0)\n",
    "# y_ts = np.concatenate(y_ts, axis=0)\n",
    "# x_ts.shape, y_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"temp\n",
    "\"\"\"\n",
    "x_ts = x_train\n",
    "y_ts = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = []\n",
    "for item in x_ts:\n",
    "    x_all.append(item.flatten('F'))\n",
    "x_train = np.array(x_all)\n",
    "y_train = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_masks(inputs, pm=0.2, pd_m=0.4):\n",
    "    inputs = np.copy(inputs).reshape(-1, 12, 4)\n",
    "    n, T, M = inputs.shape\n",
    "    masked_inputs = inputs.copy()\n",
    "\n",
    "    mask_m1 = np.random.binomial(1, pm, size=(n, T, M))\n",
    "    \n",
    "    t_max = np.random.randint(0, T, size=n)\n",
    "    mask_m2 = np.ones((n, T, M))\n",
    "    for i in range(n):\n",
    "        mask_m2[i, t_max[i]:] = 0\n",
    "    \n",
    "    modality_drop = np.random.binomial(1, pd_m, size=(n, M))\n",
    "    mask_m3 = np.ones((n, T, M))\n",
    "    for m in range(M):\n",
    "        mask_m3[:, :, m] *= modality_drop[:, m].reshape(-1, 1)\n",
    "\n",
    "    final_mask = mask_m1 * mask_m2 * mask_m3\n",
    "    masked_inputs *= final_mask\n",
    "\n",
    "    return masked_inputs, final_mask.reshape(n, -1)\n",
    "\n",
    "pm = 0.2\n",
    "pd_m = 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksper = 256 * 2\n",
    "d = x_train.shape[1]\n",
    "X_class = np.concatenate([x_train]*masksper, 0)\n",
    "Y_class = np.concatenate([y_train]*masksper, 0)\n",
    "# B = np.concatenate(\n",
    "# [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class.shape[0])],\n",
    "# 1)\n",
    "# B = np.float32(B.T)\n",
    "\n",
    "_, B = apply_masks(X_class, pm, pd_m)\n",
    "    \n",
    "\"\"\"\n",
    "remove for interpolation\n",
    "\"\"\"\n",
    "zero_mask = X_class == 0\n",
    "B[zero_mask] = 0\n",
    "\n",
    "# # remove 0 mask\n",
    "mask_nonzero = np.sum(B, axis=1) != 0\n",
    "B = B[mask_nonzero]\n",
    "X_class = X_class[mask_nonzero]\n",
    "Y_class = Y_class[mask_nonzero]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_nan = X_class == 0\n",
    "# B[mask_nan] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class = np.concatenate((X_class*B, B), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((213901, 96), (213901, 12, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_class.shape, Y_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = []\n",
    "for i in range(B.shape[0]):\n",
    "    b = B[i]\n",
    "    temp = -1\n",
    "    for j in range(4):\n",
    "        part = b[num_ts*j:num_ts*(j+1)]\n",
    "        max_index = np.where(part == 1)[0]\n",
    "        if len(max_index) > 0:\n",
    "            max_index = max_index[-1]\n",
    "            if max_index > temp:\n",
    "                temp = max_index\n",
    "    location.append(temp)\n",
    "    \n",
    "location = np.array(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 16433]\n",
      " [    1 22512]\n",
      " [    2 24842]\n",
      " [    3 30042]\n",
      " [    4 27878]\n",
      " [    5 18594]\n",
      " [    6 21267]\n",
      " [    7  9046]\n",
      " [    8 19186]\n",
      " [    9  5576]\n",
      " [   10 10502]\n",
      " [   11  8023]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(location, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp = []\n",
    "for i in range(Y_class.shape[0]):\n",
    "    y_location = Y_class[i, location[i]]\n",
    "    mask_nan = np.isnan(y_location)\n",
    "    y_location[mask_nan] = 0\n",
    "    y_temp.append(y_location)\n",
    "y_temp = np.array(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove y with nan values\n",
    "mask_zero = np.sum(y_temp, axis=1) != 0\n",
    "X_class = X_class[mask_zero]\n",
    "Y_class = y_temp[mask_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_class = np.argmax(Y_class, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/xgboost/core.py:158: UserWarning: [10:37:46] WARNING: /workspace/src/context.cc:43: No visible GPU is found, setting device to CPU.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/xgboost/core.py:158: UserWarning: [10:37:46] WARNING: /workspace/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;gpu&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=256, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;gpu&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=256, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device='gpu', early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=256, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier\n",
    "est = XGBClassifier(n_estimators=256, device='gpu')\n",
    "est.fit(X_class, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/xgboost/core.py:158: UserWarning: [10:39:02] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# est.save_model('/work/users/d/d/ddinh/aaco/models/adni_different_masking.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2412, 12, 4), (2412, 12, 3))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ts_val = []\n",
    "y_ts_val = []\n",
    "\n",
    "for i in range(num_ts): \n",
    "    x_temp = np.zeros_like(x_val)\n",
    "    x_temp[:, :i+1, :] = x_val[:, :i+1, :]\n",
    "    x_ts_val.append(x_temp)    \n",
    "    y_ts_val.append(y_val)\n",
    "x_ts_val = np.concatenate(x_ts_val, axis=0)\n",
    "y_ts_val = np.concatenate(y_ts_val, axis=0)\n",
    "x_ts_val.shape, y_ts_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_val = []\n",
    "for item in x_ts_val:\n",
    "    x_all_val.append(item.flatten('F'))\n",
    "x_val = np.array(x_all_val)\n",
    "y_val = y_ts_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksper = 1\n",
    "d = x_val.shape[1]\n",
    "X_class_val = np.concatenate([x_val]*masksper, 0)\n",
    "Y_class_val = np.concatenate([y_val]*masksper, 0)\n",
    "B_val = np.concatenate(\n",
    "[np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class_val.shape[0])],\n",
    "1)\n",
    "B_val = np.float32(B_val.T)\n",
    "\n",
    "# B_val = np.ones_like(B_val)\n",
    "\n",
    "zero_mask_val = X_class_val == 0\n",
    "B_val[zero_mask_val] = 0\n",
    "\n",
    "# remove 0 mask\n",
    "mask_nonzero_val = np.sum(B_val, axis=1) != 0\n",
    "B_val = B_val[mask_nonzero_val]\n",
    "X_class_val = X_class_val[mask_nonzero_val]\n",
    "Y_class_val = Y_class_val[mask_nonzero_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_val = []\n",
    "for i in range(B_val.shape[0]):\n",
    "    b = B_val[i]\n",
    "    temp = -1\n",
    "    for j in range(4):\n",
    "        part = b[num_ts*j:num_ts*(j+1)]\n",
    "        max_index = np.where(part == 1)[0]\n",
    "        if len(max_index) > 0:\n",
    "            max_index = max_index[-1]\n",
    "            if max_index > temp:\n",
    "                temp = max_index\n",
    "    location_val.append(temp)\n",
    "    \n",
    "location_val = np.array(location_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp_val = []\n",
    "for i in range(Y_class_val.shape[0]):\n",
    "    y_location_val = Y_class_val[i, location_val[i]]\n",
    "    mask_nan_val = np.isnan(y_location_val)\n",
    "    y_location_val[mask_nan_val] = 0\n",
    "    y_temp_val.append(y_location_val)\n",
    "y_temp_val = np.array(y_temp_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove y with nan values for validation set\n",
    "mask_zero_val = np.sum(y_temp_val, axis=1) != 0\n",
    "X_class_val = X_class_val[mask_zero_val]\n",
    "Y_class_val = y_temp_val[mask_zero_val]\n",
    "B_val = B_val[mask_zero_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_zero_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class_val = np.concatenate((X_class_val*B_val, B_val), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = est.predict_proba(X_class_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5409617097061442\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.argmax(val_preds, 1)==np.argmax(Y_class_val, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5109105973299515\n",
      "0.7030657167468642\n"
     ]
    }
   ],
   "source": [
    "# this is temoprary evaluation, since it's not really correct.\n",
    "# it should be evaluated on the accumulated predictions, not the individual predictions like this rolling out \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "pr_auc_macro = average_precision_score(Y_class_val, val_preds, average=\"macro\")\n",
    "print(pr_auc_macro)\n",
    "\n",
    "roc_auc_macro = roc_auc_score(Y_class_val, val_preds, average=\"macro\", multi_class=\"ovr\")\n",
    "print(roc_auc_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.037846"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(B_val.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "# 0.5057228523811975\n",
    "# 0.6798018971723648\n",
    "\n",
    "# masking from a2mt paper\n",
    "# 0.5109105973299515\n",
    "# 0.7030657167468642"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
