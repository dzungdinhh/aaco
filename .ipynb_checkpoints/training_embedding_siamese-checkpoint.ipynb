{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow==2.17.0\n",
      "  Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.37.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.29.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.16.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.6.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from tensorflow==2.17.0) (4.7.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.26.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (18.1.1)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.12.1)\n",
      "Requirement already satisfied: packaging in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from tensorflow==2.17.0) (23.0)\n",
      "Requirement already satisfied: setuptools in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from tensorflow==2.17.0) (65.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.68.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.1.0)\n",
      "Collecting tensorboard<2.18,>=2.17\n",
      "  Using cached tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (4.25.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from tensorflow==2.17.0) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (24.12.23)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (2.5.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorflow==2.17.0) (3.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.38.4)\n",
      "Requirement already satisfied: namex in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
      "Requirement already satisfied: rich in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.9.4)\n",
      "Requirement already satisfied: optree in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.13.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.6.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.0.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "  Attempting uninstall: tensorflow\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tensorboard-2.17.1 tensorflow-2.17.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow==2.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==2.1.0\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from torch==2.1.0) (3.1.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from torch==2.1.0) (3.9.0)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from torch==2.1.0) (4.7.1)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /nas/longleaf/rhel8/apps/anaconda/2023.03.ood/lib/python3.10/site-packages (from jinja2->torch==2.1.0) (2.1.1)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.12.0 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 sympy-1.13.3 torch-2.1.0 triton-2.1.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/nas/longleaf/home/ddinh/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('/work/users/d/d/ddinh/aaco/src')\n",
    "# from load_dataset import load_adni_data\n",
    "# from cvar_sensing.utils import prepare_time_series, batch_interp_nd\n",
    "# import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_digits.npy')\n",
    "X_train_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_counter.npy')\n",
    "y_train = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/train_labels.npy')\n",
    "X_test_digits = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_digits.npy')\n",
    "X_test_counter = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_counter.npy')\n",
    "y_test = np.load('/work/users/d/d/ddinh/aaco/input_data/Synthetic_raw/test_labels.npy')\n",
    "\n",
    "train_y = np.eye(np.max(y_train) + 1)[y_train]\n",
    "y_test = np.eye(np.max(y_test) + 1)[y_test]\n",
    "\n",
    "train_x = np.concatenate([X_train_digits, X_train_counter], axis=1)\n",
    "X_test = np.concatenate([X_test_digits, X_test_counter], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(n_estimators=256, device='gpu')\n",
    "classifier.load_model('/work/users/d/d/ddinh/aaco/models/synthetic_raw.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: implement progressively acquire features version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BaseEmbeddingModel(tf.keras.Model):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.base_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.num_features,)),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "        ])\n",
    "        self.embedding_head = tf.keras.layers.Dense(32, activation=None)  # embedding head\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        embedding = self.embedding_head(x)\n",
    "        return embedding\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_features': self.num_features,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "def create_siamese_network(num_features, embedding_dim=32):\n",
    "    # Define the two inputs\n",
    "    input_a = layers.Input(shape=(num_features,), name='input_a')\n",
    "    input_b = layers.Input(shape=(num_features,), name='input_b')\n",
    "\n",
    "    # Initialize the base embedding model\n",
    "    base_embedding = BaseEmbeddingModel(num_features)\n",
    "\n",
    "    # Generate the embeddings for both inputs\n",
    "    embedding_a = base_embedding(input_a)\n",
    "    embedding_b = base_embedding(input_b)\n",
    "\n",
    "    # Compute the Euclidean distance between embeddings\n",
    "    distance = tf.keras.layers.Lambda(lambda tensors: tf.norm(tensors[0] - tensors[1], axis=1, keepdims=True))([embedding_a, embedding_b])\n",
    "\n",
    "    # Define the model\n",
    "    siamese_model = Model(inputs=[input_a, input_b], outputs=distance, name='siamese_network')\n",
    "\n",
    "    return siamese_model, base_embedding\n",
    "\n",
    "def x_masked(x, d=20, num_timestamps=10, num_modalities=2):\n",
    "    masks = np.concatenate(\n",
    "        [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(int(d*(3/4)))], 1, keepdims=True) for _ in range(x.shape[0])], 1\n",
    "        )\n",
    "    masks = np.float32(masks.T)\n",
    "\n",
    "    # ensure there is no 1 at the end of time step \n",
    "    for i in range(num_modalities):\n",
    "        masks[:, num_timestamps * i + num_timestamps - 1] = 0\n",
    "        \n",
    "    masks_zero = np.sum(masks, axis=1) == 0\n",
    "    \n",
    "    \n",
    "    while np.sum(masks_zero) > 0:\n",
    "        # masks[masks_zero] = np.float32(np.concatenate(\n",
    "        #     [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(np.sum(masks_zero))], 1\n",
    "        #     )).T\n",
    "        masks[masks_zero] = np.float32(np.concatenate(\n",
    "            [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(int(d*(3/4)))], 1, keepdims=True) for _ in range(np.sum(masks_zero))], 1\n",
    "            )).T\n",
    "        for i in range(num_modalities):\n",
    "            masks[:, num_timestamps * i + num_timestamps - 1] = 0\n",
    "            \n",
    "        masks_zero = np.sum(masks, axis=1) == 0\n",
    "        \n",
    "    return np.copy(x), tf.cast(masks, tf.float32)\n",
    "\n",
    "\n",
    "def compute_loss_timestep(y_true, x_data, mask, classifier, acquisition_cost, loss_function, num_timestamps=10, num_modalities=2):\n",
    "    y_pred = classifier.predict_proba(np.concatenate([x_data * mask, mask], axis=1))\n",
    "    y_pred = torch.Tensor(y_pred)\n",
    "    if isinstance(y_true, tf.Tensor):\n",
    "        y_true = torch.Tensor(y_true.numpy())\n",
    "    else:\n",
    "        y_true = torch.Tensor(y_true)\n",
    "    total_cost = torch.zeros(mask.shape[0])\n",
    "    total_cost += mask.sum(axis=1) * acquisition_cost\n",
    "    total_loss = loss_function(y_pred, y_true).numpy() + total_cost.numpy()\n",
    "    return total_loss\n",
    "\n",
    "def get_potential_features(x, y, classifier, prev_masks, acquisition_cost, d=20, num_masks=1500, topk=5, num_timestamps=10, num_modalities=2):\n",
    "    new_masks = np.concatenate(\n",
    "        [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(num_masks)], 1\n",
    "        )\n",
    "    new_masks = np.float32(new_masks.T)\n",
    "\n",
    "    # repeat for parallelization\n",
    "    x_rep = np.repeat(x, num_masks, axis=0)\n",
    "    y_rep = np.repeat(y, num_masks, axis=0)\n",
    "    new_masks = np.concatenate([new_masks for _ in range(x.shape[0])], 0)\n",
    "    prev_masks_rep = np.repeat(prev_masks, num_masks, axis=0)\n",
    "\n",
    "    # combine previous masks with new masks\n",
    "    N = x_rep.shape[0]\n",
    "    segments_previous = prev_masks_rep.reshape(N, num_modalities, num_timestamps)\n",
    "    segments_after = new_masks.reshape(N, num_modalities, num_timestamps)\n",
    "    last_indices = np.where(segments_previous == 1, np.arange(num_timestamps), -1)\n",
    "    last_indices_per_segment = np.max(last_indices, axis=2)  \n",
    "    final_last_index = np.max(last_indices_per_segment, axis=1)  \n",
    "    mask = np.arange(num_timestamps)[None, None, :] > final_last_index[:, None, None]\n",
    "    final_segments = np.where(mask, segments_after, segments_previous)\n",
    "    final = final_segments.reshape(N, num_timestamps * num_modalities)\n",
    "    \n",
    "    # compute loss for current and future masks\n",
    "    current_loss = compute_loss_timestep(y, x, prev_masks, classifier, acquisition_cost, nn.CrossEntropyLoss(reduction='none'))\n",
    "    future_loss = compute_loss_timestep(y_rep, x_rep, final, classifier, acquisition_cost, nn.CrossEntropyLoss(reduction='none'))\n",
    "    \n",
    "    # get the top k min loss\n",
    "    top_k_sets = []\n",
    "    distributions = []\n",
    "    terminations = []\n",
    "    percentage_unique = []\n",
    "    for i in range(x.shape[0]):\n",
    "        min_current_loss = current_loss[i]\n",
    "        single_sample_future_loss = np.copy(future_loss[i * num_masks: (i + 1) * num_masks])\n",
    "        single_sample_future_set = np.copy(final[i * num_masks: (i + 1) * num_masks])\n",
    "        \n",
    "        top_k_loss = np.argsort(single_sample_future_loss)\n",
    "        min_k_future_loss = single_sample_future_loss[top_k_loss]\n",
    "        min_k_subsets = single_sample_future_set[top_k_loss]\n",
    "        \n",
    "        unique_min_k_subsets, unique_indices = np.unique(min_k_subsets, axis=0, return_index=True)\n",
    "        unique_min_k_future_loss = min_k_future_loss[unique_indices]\n",
    "        \n",
    "        sorted_unique_indices = np.argsort(unique_min_k_future_loss)\n",
    "        unique_min_k_future_loss = unique_min_k_future_loss[sorted_unique_indices]\n",
    "        unique_min_k_subsets = unique_min_k_subsets[sorted_unique_indices]\n",
    "        \n",
    "        if unique_min_k_subsets.shape[0] <= 1:\n",
    "            percentage_unique.append(0)\n",
    "            topk_min = topk\n",
    "            if np.equal(unique_min_k_subsets[0], prev_masks[i]).all():\n",
    "                termination = np.ones(topk)\n",
    "                unique_min_k_subsets = np.repeat(prev_masks[i][None, :], topk, axis=0)\n",
    "                subset_losses = np.ones(topk) * min_current_loss\n",
    "            else:\n",
    "                if unique_min_k_future_loss[0] >= min_current_loss:\n",
    "                    termination = np.ones(topk)\n",
    "                    unique_min_k_subsets = np.repeat(prev_masks[i][None, :], topk, axis=0)\n",
    "                    subset_losses = np.ones(topk) * min_current_loss\n",
    "                else: \n",
    "                    termination = np.zeros(topk)\n",
    "                    unique_min_k_subsets = np.repeat(unique_min_k_subsets[0], topk, axis=0)\n",
    "                    subset_losses = np.repeat(unique_min_k_future_loss[0], topk)\n",
    "        else: \n",
    "            percentage_unique.append(1)\n",
    "            topk_min = min(topk, unique_min_k_subsets.shape[0])\n",
    "            termination = np.zeros(topk_min)\n",
    "            subset_losses = unique_min_k_future_loss[:topk_min]\n",
    "\n",
    "            for j in range(topk_min):\n",
    "                if unique_min_k_future_loss[j] >= min_current_loss:\n",
    "                    termination[j] = 1\n",
    "                    unique_min_k_subsets[j] = prev_masks[i]\n",
    "                    \n",
    "        unique_min_k_subsets = unique_min_k_subsets[:topk_min]\n",
    "        unique_min_k_subsets[:,:d] -= prev_masks[i]\n",
    "        # get the first 1 in each segment\n",
    "        # todo: consider changing this into a for loop, might be faster\n",
    "        n = unique_min_k_subsets.shape[0]\n",
    "        segment_size = num_timestamps\n",
    "        num_segments = num_modalities\n",
    "        segments = unique_min_k_subsets.reshape(n, num_segments, segment_size)\n",
    "        first_one_indices = np.where(segments == 1, np.arange(segment_size), np.inf)\n",
    "        first_one_per_segment = np.min(first_one_indices, axis=2)\n",
    "        first_one_per_segment[first_one_per_segment == np.inf] = -1\n",
    "        global_first_one = np.min(\n",
    "            np.where(first_one_per_segment != -1, first_one_per_segment, np.inf), axis=1\n",
    "        )\n",
    "        global_first_one = np.where(global_first_one == np.inf, -1, global_first_one)\n",
    "        keep_indices = (first_one_indices == global_first_one[:, None, None])\n",
    "        transformed_masks = np.zeros_like(unique_min_k_subsets)\n",
    "        transformed_masks[keep_indices.reshape(unique_min_k_subsets.shape)] = 1\n",
    "        unique_min_k_subsets = np.copy(transformed_masks)\n",
    "        unique_min_k_subsets = np.concatenate([unique_min_k_subsets, termination[:, None]], axis=1) # shape (topk, d+1)\n",
    "        \n",
    "        # turn into probabilities\n",
    "        unique_min_k_subsets = unique_min_k_subsets / np.sum(unique_min_k_subsets, axis=1)[:, None]\n",
    "        subset_losses = subset_losses[:topk_min]\n",
    "        weights = np.exp(-subset_losses[:topk_min])\n",
    "        weights /= np.sum(weights)\n",
    "        distribution = np.sum(unique_min_k_subsets * weights[:, None], axis=0) \n",
    "        \n",
    "        distributions.append(distribution)\n",
    "        terminations.append(termination)\n",
    "\n",
    "    distributions = np.array(distributions) # shape (batch_size, d)\n",
    "    distributions = tf.cast(distributions, tf.float32)\n",
    "    distributions = distributions / (tf.reduce_sum(distributions, axis=1, keepdims=True) + 1e-12)\n",
    "    # print(\"check sum of distributions: \", np.sum(distributions, axis=1))\n",
    "    \n",
    "    return distributions\n",
    "\n",
    "def pair_generator(x, y):\n",
    "    # given x, y, generate the corresponding pairs\n",
    "    # really simple, just shuffle the indices and make sure they are not the same index\n",
    "    n_samples = x.shape[0]\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    while np.any(indices == np.arange(n_samples)):\n",
    "        indices = np.random.permutation(n_samples)\n",
    "    pairs = x[indices].copy()\n",
    "    labels = y.numpy()[indices].copy()\n",
    "    return pairs, labels\n",
    "\n",
    "def mse_distance_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: Tensor of shape (batch_size, 1) representing distribution distances\n",
    "    y_pred: Tensor of shape (batch_size, 1) representing embedding distances\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# @tf.function\n",
    "def train_step(model, optimizer, x, y, classifier, acquisition_cost, alpha=1):\n",
    "    x, prev_masks = x_masked(x)\n",
    "    \n",
    "    x_rep, y_rep = pair_generator(x, y)\n",
    "    \n",
    "    distributions = get_potential_features(np.copy(x), np.copy(y), classifier, np.copy(prev_masks), acquisition_cost)\n",
    "    distributions_rep = get_potential_features(np.copy(x_rep), np.copy(y_rep), classifier, np.copy(prev_masks), acquisition_cost)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_distance = model([x * prev_masks, x_rep * prev_masks])\n",
    "        target_distance = tf.norm(distributions - distributions_rep, axis=1, keepdims=True)\n",
    "        \n",
    "        loss = mse_distance_loss(target_distance, pred_distance)\n",
    "\n",
    "    # Backprop\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, x, y, classifier, acquisition_cost, alpha=1):\n",
    "    x, prev_masks = x_masked(x)\n",
    "    x_rep, y_rep = pair_generator(x, y)\n",
    "    \n",
    "    distributions = get_potential_features(np.copy(x), np.copy(y), classifier, np.copy(prev_masks), acquisition_cost)\n",
    "    distributions_rep = get_potential_features(np.copy(x_rep), np.copy(y_rep), classifier, np.copy(prev_masks), acquisition_cost)\n",
    "    \n",
    "    pred_distance = model([x * prev_masks, x_rep * prev_masks])\n",
    "    target_distance = tf.norm(distributions - distributions_rep, axis=1, keepdims=True)\n",
    "    \n",
    "    loss = mse_distance_loss(target_distance, pred_distance)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "X, X_val, y, y_val = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.shuffle(10000).batch(512).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(512).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "num_features = 20\n",
    "siamese_model, base_embedding = create_siamese_network(num_features=num_features, embedding_dim=32)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "acquisition_cost = 0.015\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    step_count = 0\n",
    "\n",
    "    for batch_x, batch_y in dataset:\n",
    "        loss = train_step(siamese_model, optimizer, batch_x, batch_y, classifier, acquisition_cost)\n",
    "        epoch_loss += loss.numpy()\n",
    "        step_count += 1\n",
    "    train_loss = epoch_loss / step_count\n",
    "\n",
    "    val_loss_sum = 0.0\n",
    "    val_steps = 0\n",
    "    for val_x_batch, val_y_batch in val_dataset:\n",
    "        loss_val = validation_step(siamese_model, val_x_batch, val_y_batch, classifier, acquisition_cost)\n",
    "        val_loss_sum += loss_val.numpy()\n",
    "        val_steps += 1\n",
    "    val_loss = val_loss_sum / val_steps\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        siamese_model.save('/work/users/d/d/ddinh/aaco/models/siamese.h5')\n",
    "        siamese_model.save('/work/users/d/d/ddinh/aaco/models/siamese.keras')\n",
    "        siamese_model.save_weights('/work/users/d/d/ddinh/aaco/models/siamese.weights.h5')\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "# add the regularizer same as divdis to avoid always predic as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing masks...\n",
      "computing predictions...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m     mask_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(mask_input)\n\u001b[1;32m     45\u001b[0m     ts_rep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(ts, x_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_rep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     y_pred[:,ts,:] \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(y_pred)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:450\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ):\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:668\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[0;32m--> 668\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[1;32m    670\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[1;32m    671\u001b[0m         dataset\n\u001b[1;32m    672\u001b[0m     )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:677\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:140\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# performance.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprefetch(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m        A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2311\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2312\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    113\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1251\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1250\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1221\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1225\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:691\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;241m=\u001b[39m created_variables\n\u001b[0;32m--> 691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_scoped_tracing_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_capturing_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScopeType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVARIABLE_CREATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[1;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    698\u001b[0m )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:604\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options\u001b[0;34m(self, scope, scope_type)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    602\u001b[0m weak_wrapped_fn \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(wrapped_fn)\n\u001b[0;32m--> 604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_tracing_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_decorator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_decorator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapped_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:646\u001b[0m, in \u001b[0;36mFunction._generate_tracing_options\u001b[0;34m(self, fn, scope_type)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autograph:\n\u001b[1;32m    643\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph_util\u001b[38;5;241m.\u001b[39mpy_func_from_autograph(\n\u001b[1;32m    644\u001b[0m       fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_autograph_options)\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTracingOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolymorphic_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscope_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduce_retracing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce_retracing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experimental_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_captures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_captures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:16\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, python_function, name, polymorphic_type, default_values, scope_type, attributes, autograph, autograph_options, reduce_retracing, bind_graph_to_function, function_cache, function_captures, lock)\u001b[0m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:112\u001b[0m, in \u001b[0;36mTracingOptions.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolymorphic_type \u001b[38;5;241m=\u001b[39m function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType\u001b[38;5;241m.\u001b[39mfrom_callable(\n\u001b[1;32m    106\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_function\n\u001b[1;32m    107\u001b[0m   )\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_values \u001b[38;5;241m=\u001b[39m function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType\u001b[38;5;241m.\u001b[39mget_default_values(\n\u001b[1;32m    109\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_function\n\u001b[1;32m    110\u001b[0m   )\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_signature \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_input_signature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolymorphic_type\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:169\u001b[0m, in \u001b[0;36mto_input_signature\u001b[0;34m(function_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameter\u001b[38;5;241m.\u001b[39mtype_constraint:\n\u001b[1;32m    163\u001b[0m   \u001b[38;5;66;03m# Generate legacy constraint representation.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m   constraint \u001b[38;5;241m=\u001b[39m parameter\u001b[38;5;241m.\u001b[39mtype_constraint\u001b[38;5;241m.\u001b[39mplaceholder_value(\n\u001b[1;32m    165\u001b[0m       trace_type\u001b[38;5;241m.\u001b[39mInternalPlaceholderContext(unnest_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m   )\n\u001b[1;32m    167\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    168\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, tensor\u001b[38;5;241m.\u001b[39mTensorSpec)\n\u001b[0;32m--> 169\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m   ):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# input_signature only supports contiguous TensorSpec composites\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     is_auto_constrained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/nest.py:293\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.flatten\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten\u001b[39m(structure, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a flat list from a given structure.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/nest_util.py:712\u001b[0m, in \u001b[0;36mflatten\u001b[0;34m(modality, structure, expand_composites)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Flattens a nested structure.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m  TypeError: The nest is or contains a dict with non-sortable keys.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m--> 712\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m    714\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_flatten(structure)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/nest_util.py:726\u001b[0m, in \u001b[0;36m_tf_core_flatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    724\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    725\u001b[0m expand_composites \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(expand_composites)\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pywrap_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_MASKS_CURRENT = 100\n",
    "NUM_MASKS_FUTURE = 100\n",
    "NUM_TS = 12\n",
    "NUM_MODALITIES = 4\n",
    "acquisition_cost = 0.1\n",
    "d = NUM_TS * NUM_MODALITIES\n",
    "loss_function = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "print(\"computing masks...\")\n",
    "\n",
    "ts_current = np.random.randint(0, num_ts, NUM_MASKS_CURRENT)\n",
    "b_c = []\n",
    "for i in range(NUM_MASKS_CURRENT):\n",
    "    mask_temp = []\n",
    "    for j in range(NUM_MODALITIES): \n",
    "        mask = np.sum(np.random.permutation(np.eye(NUM_TS))[:,:ts_current[i]+1][:, :np.random.randint(ts_current[i]+1)], 1)\n",
    "        mask_temp.append(mask)\n",
    "    b_c.append(mask_temp)\n",
    "b_c = np.array(b_c)\n",
    "b_c = np.transpose(b_c, (0, 2, 1)).reshape(-1, d)\n",
    "b_empty = np.sum(b_c, 1) == 0\n",
    "b_c = b_c[~b_empty]\n",
    "b_c = np.unique(b_c, axis=0)\n",
    "num_mask = b_c.shape[0]\n",
    "\n",
    "\n",
    "x_current = np.repeat(train_x, num_mask, axis=0)\n",
    "y_current = np.repeat(train_y, num_mask, axis=0)\n",
    "\n",
    "b_current = np.concatenate([b_c for _ in range(train_x.shape[0])], axis=0)\n",
    "print(\"computing predictions...\")\n",
    "\n",
    "y_pred = np.zeros(y_current.shape)\n",
    "for ts in range(NUM_TS):\n",
    "    print(ts)\n",
    "    x_input = np.zeros(x_current.shape)\n",
    "    mask_input = np.zeros(b_current.shape)\n",
    "    \n",
    "    for k in range(NUM_MODALITIES):\n",
    "        x_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(x_current[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "        mask_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(b_current[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "    \n",
    "    x_input = torch.Tensor(x_input)\n",
    "    mask_input = torch.Tensor(mask_input.numpy())\n",
    "    ts_rep = np.repeat(ts, x_input.shape[0]).reshape(-1, 1)\n",
    "    pred = classifier.predict(np.concatenate([x_input * mask_input, ts_rep], axis=-1), verbose=0)\n",
    "    y_pred[:,ts,:] = pred\n",
    "y_pred = torch.Tensor(y_pred)\n",
    "y_current = torch.Tensor(y_current)\n",
    "print(\"computing loss...\")\n",
    "cost = torch.zeros(b_current.shape[0])\n",
    "for m in range(NUM_MODALITIES):\n",
    "    modality_cost = 1 if m in [0, 1] else 0.5\n",
    "    cost += b_current[:, m * NUM_TS: (m + 1) * NUM_TS].sum(1) * acquisition_cost * modality_cost\n",
    "loss_current = compute_accumulated_loss(y_pred, y_current, loss_function) + cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing best mask...\n",
      "x_future shape:  (6355800, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"computing best mask...\")\n",
    "# while loop here to make sure mask is not 0\n",
    "Bf = []\n",
    "for i in range(NUM_MASKS_CURRENT):\n",
    "    for j in range(NUM_MASKS_FUTURE): \n",
    "        mask_temp = []\n",
    "        for j in range(NUM_MODALITIES): \n",
    "            mask = np.sum(np.random.permutation(np.eye(NUM_TS))[:,ts_current[i]+1:][:, :np.random.randint(NUM_TS-ts_current[i]+1)], 1)\n",
    "            mask_temp.append(mask)\n",
    "        Bf.append(mask_temp)\n",
    "Bf = np.array(Bf)\n",
    "Bf = np.transpose(Bf, (0, 2, 1)).reshape(-1, d)\n",
    "    \n",
    "Bf[0] = np.zeros(d) # current mask \n",
    "Bf = np.unique(Bf, axis=0)\n",
    "\n",
    "num_mask_future = Bf.shape[0]\n",
    " \n",
    "x_future = np.repeat(x_current, num_mask_future, axis=0)\n",
    "y_future = np.repeat(y_current, num_mask_future, axis=0)\n",
    "print(\"x_future shape: \", x_future.shape)\n",
    "\n",
    "b_current_for_future = np.repeat(b_c, num_mask_future, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bforb = np.repeat(Bf, num_mask, axis=0)\n",
    "\n",
    "Bforb = np.minimum(b_current_for_future+Bforb, 1)\n",
    "Bforb = np.concatenate([Bforb for _ in range(train_x.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing predictions...\n",
      "0\n",
      "\u001b[1m198619/198619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 394us/step\n",
      "1\n",
      "\u001b[1m198619/198619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 389us/step\n",
      "2\n",
      "\u001b[1m198619/198619\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 394us/step\n",
      "3\n",
      "\u001b[1m 62848/198619\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 383us/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     mask_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(mask_input)\n\u001b[1;32m     14\u001b[0m     ts_rep \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(ts, x_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_rep\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     y_pred[:,ts,:] \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputing loss...\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:512\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    510\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    511\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[0;32m--> 512\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[1;32m    514\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"computing predictions...\")\n",
    "y_pred = np.zeros(y_future.shape)\n",
    "for ts in range(NUM_TS):\n",
    "    print(ts)\n",
    "    x_input = np.zeros(x_future.shape)\n",
    "    mask_input = np.zeros(Bforb.shape)\n",
    "    \n",
    "    for k in range(NUM_MODALITIES):\n",
    "        x_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(x_future[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "        mask_input[:,k * NUM_TS: k * NUM_TS + ts + 1] = np.copy(Bforb[:,k * NUM_TS: k * NUM_TS + ts + 1])\n",
    "    \n",
    "    x_input = torch.Tensor(x_input)\n",
    "    mask_input = torch.Tensor(mask_input)\n",
    "    ts_rep = np.repeat(ts, x_input.shape[0]).reshape(-1, 1)\n",
    "    pred = classifier.predict(np.concatenate([x_input * mask_input, ts_rep], axis=-1), verbose=1)\n",
    "    y_pred[:,ts,:] = pred\n",
    "\n",
    "print(\"computing loss...\")    \n",
    "y_pred = torch.Tensor(y_pred)\n",
    "y_future = torch.Tensor(y_future)\n",
    "cost = torch.zeros(Bforb.shape[0])\n",
    "for m in range(NUM_MODALITIES):\n",
    "    modality_cost = 1 if m in [0, 1] else 0.5\n",
    "    cost += Bforb[:, m * NUM_TS: (m + 1) * NUM_TS].sum(1) * acquisition_cost * modality_cost\n",
    "loss_future = compute_accumulated_loss(y_pred, y_future, loss_function) + cost \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[758316. 758987. 758008. 757953. 757620. 757955. 757972. 758435. 757102.\n",
      " 756993. 757385. 757279. 757917. 757722. 757728. 757501. 757812. 758526.\n",
      " 758420. 757669. 758158. 758848. 758452. 758176. 757722. 758034. 758743.\n",
      " 759001. 758049. 757567. 758648. 758536. 757489. 756505. 757903. 757405.\n",
      " 757466. 757965. 758633. 757916. 758311. 757459. 758096. 757114. 757111.\n",
      " 757801. 758337. 757861.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1418396, 49), (1418396, 3), (1418396, 48))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masksper = 256 \n",
    "d = train_x_ts.shape[1] - 1\n",
    "X_class = np.concatenate([train_x_ts]*masksper, 0)\n",
    "Y_class = np.concatenate([train_y_ts]*masksper, 0)\n",
    "B = np.concatenate(\n",
    "[np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class.shape[0])],\n",
    "1)\n",
    "B = np.float32(B.T)\n",
    "\n",
    "zero_mask = X_class[:,:d] == 0\n",
    "B[zero_mask] = 0\n",
    "# remove 0 mask\n",
    "mask_nonzero = np.sum(B, axis=1) != 0\n",
    "B = B[mask_nonzero]\n",
    "X_class = X_class[mask_nonzero]\n",
    "Y_class = Y_class[mask_nonzero]\n",
    "\n",
    "X_class[:,:d] = X_class[:,:d] * B\n",
    "X_class.shape, Y_class.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_current = np.random.randint(0, num_ts, NUM_MASKS_CURRENT)\n",
    "b_current = []\n",
    "for i in range(NUM_MASKS_CURRENT):\n",
    "    mask_temp = []\n",
    "    for j in range(NUM_MODALITIES): \n",
    "        mask = np.sum(np.random.permutation(np.eye(NUM_TS))[:,:ts_current[i]+1][:, :np.random.randint(ts_current[i]+1)], 1)\n",
    "        mask_temp.append(mask)\n",
    "    b_current.append(mask_temp)\n",
    "    \n",
    "b_current = np.array(b_current)\n",
    "b_current = np.transpose(b_current, (1, 0, 2)).reshape(-1, NUM_MODALITIES * NUM_TS)\n",
    "b_empty = np.sum(b_current, 1) == 0\n",
    "b_current = b_current[~b_empty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5595, 49), (5595, 3), (5595, 49))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masksper = 5\n",
    "d = val_x_ts.shape[1]\n",
    "X_class_val = np.concatenate([val_x_ts]*masksper, 0)\n",
    "Y_class_val = np.concatenate([val_y_ts]*masksper, 0)\n",
    "B_val = np.concatenate(\n",
    "[np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class_val.shape[0])],\n",
    "1)\n",
    "B_val = np.float32(B_val.T)\n",
    "\n",
    "# B_val = np.ones_like(B_val)\n",
    "\n",
    "zero_mask_val = X_class_val[:,:d] == 0\n",
    "B_val[zero_mask_val] = 0\n",
    "\n",
    "# remove 0 mask\n",
    "mask_nonzero_val = np.sum(B_val, axis=1) != 0\n",
    "B_val = B_val[mask_nonzero_val]\n",
    "X_class_val = X_class_val[mask_nonzero_val]\n",
    "Y_class_val = Y_class_val[mask_nonzero_val]\n",
    "\n",
    "X_class_val[:,:d] = X_class_val[:,:d] * B_val\n",
    "X_class_val.shape, Y_class_val.shape, B_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132588, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(logits, axis=1).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m35373/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 915us/step - accuracy: 0.6022 - loss: 0.7826 - prc: 0.6291 - roc_auc: 0.8070\n",
      "Epoch 1: val_roc_auc improved from -inf to 0.76294, saving model to /work/users/d/d/ddinh/aaco/models/mlp_test.keras\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1ms/step - accuracy: 0.6022 - loss: 0.7826 - prc: 0.6291 - roc_auc: 0.8070 - val_accuracy: 0.5645 - val_loss: 0.9315 - val_prc: 0.5431 - val_roc_auc: 0.7629\n",
      "Epoch 2/100\n",
      "\u001b[1m35363/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6201 - loss: 0.7467 - prc: 0.6643 - roc_auc: 0.8241\n",
      "Epoch 2: val_roc_auc improved from 0.76294 to 0.76604, saving model to /work/users/d/d/ddinh/aaco/models/mlp_test.keras\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1ms/step - accuracy: 0.6201 - loss: 0.7467 - prc: 0.6643 - roc_auc: 0.8241 - val_accuracy: 0.5657 - val_loss: 0.9197 - val_prc: 0.5514 - val_roc_auc: 0.7660\n",
      "Epoch 3/100\n",
      "\u001b[1m35340/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 936us/step - accuracy: 0.6223 - loss: 0.7426 - prc: 0.6686 - roc_auc: 0.8261\n",
      "Epoch 3: val_roc_auc improved from 0.76604 to 0.76734, saving model to /work/users/d/d/ddinh/aaco/models/mlp_test.keras\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 973us/step - accuracy: 0.6223 - loss: 0.7426 - prc: 0.6686 - roc_auc: 0.8261 - val_accuracy: 0.5675 - val_loss: 0.9245 - val_prc: 0.5521 - val_roc_auc: 0.7673\n",
      "Epoch 4/100\n",
      "\u001b[1m35324/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.6237 - loss: 0.7398 - prc: 0.6713 - roc_auc: 0.8275\n",
      "Epoch 4: val_roc_auc did not improve from 0.76734\n",
      "\u001b[1m35377/35377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 955us/step - accuracy: 0.6237 - loss: 0.7398 - prc: 0.6713 - roc_auc: 0.8275 - val_accuracy: 0.5539 - val_loss: 0.9287 - val_prc: 0.5460 - val_roc_auc: 0.7616\n",
      "Epoch 5/100\n",
      "\u001b[1m 4567/35377\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 939us/step - accuracy: 0.6242 - loss: 0.7391 - prc: 0.6721 - roc_auc: 0.8277"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, accuracy, roc_auc\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# history = train_model(model, train_x_ts, train_y_ts, val_x_ts, val_y_ts)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# evaluate_model(model, val_x_ts, val_y_ts)\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_class_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_class_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m evaluate_model(model, X_class_val, Y_class_val)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_history\u001b[39m(history):\n",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     42\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_roc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     43\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     44\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \n\u001b[1;32m     45\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/device:GPU:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "TOTAL_FEATURES = train_x_ts.shape[-1]\n",
    "HIDDEN_DIM = 10\n",
    "NUM_CLASSES = train_y_ts.shape[-1]\n",
    "DROPOUT_RATE = 0.75\n",
    "\n",
    "def create_mlp():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(TOTAL_FEATURES,)),\n",
    "        layers.Dense(HIDDEN_DIM, activation='relu'),\n",
    "        # layers.Dropout(DROPOUT_RATE),\n",
    "        layers.Dense(HIDDEN_DIM, activation='relu'),\n",
    "        # layers.Dropout(DROPOUT_RATE),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model = create_mlp()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', AUC(name='roc_auc'), AUC(name='prc', curve='PR')]\n",
    ")\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        '/work/users/d/d/ddinh/aaco/models/mlp_embedding.keras',  \n",
    "        monitor='val_roc_auc',\n",
    "        mode='max',  \n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_roc_auc', \n",
    "        mode='max', \n",
    "        patience=10,  \n",
    "        verbose=1\n",
    "    )\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[checkpoint, early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    loss, accuracy, roc_auc, prc = model.evaluate(X, y, verbose=0)\n",
    "    print(f\"Loss: {loss}, Accuracy: {accuracy}, ROC AUC: {roc_auc}, PRC: {prc}\")\n",
    "    return loss, accuracy, roc_auc\n",
    "\n",
    "# history = train_model(model, train_x_ts, train_y_ts, val_x_ts, val_y_ts)\n",
    "# evaluate_model(model, val_x_ts, val_y_ts)\n",
    "history = train_model(model, X_class, Y_class, X_class_val, Y_class_val)\n",
    "evaluate_model(model, X_class_val, Y_class_val)\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['roc_auc'])\n",
    "    plt.plot(history.history['val_roc_auc'])\n",
    "    plt.title('model roc_auc')\n",
    "    plt.ylabel('roc_auc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['prc'])\n",
    "    plt.plot(history.history['val_prc'])\n",
    "    plt.title('model prc')\n",
    "    plt.ylabel('prc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "plot_history(history)\n",
    "#  val_prc: 0.5998 - val_roc_auc: 0.7917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_adni_data()\n",
    "x = dataset.x\n",
    "y = dataset.y\n",
    "\n",
    "mask_nan = np.isnan(x)\n",
    "\n",
    "x[mask_nan] = 0\n",
    "\n",
    "num_ts = y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06387226 0.64271457 0.29341317]\n",
      "[0.08805668 0.61234818 0.29959514]\n",
      "[0.10761421 0.59187817 0.30050761]\n",
      "[0.125      0.58974359 0.28525641]\n",
      "[0.16613757 0.54179894 0.29206349]\n",
      "[0.13573883 0.56872852 0.29553265]\n",
      "[0.178125  0.5796875 0.2421875]\n",
      "[0.18283582 0.53731343 0.27985075]\n",
      "[0.17350746 0.46455224 0.3619403 ]\n",
      "[0.18235294 0.45294118 0.36470588]\n",
      "[0.18596491 0.49473684 0.31929825]\n",
      "[0.17098446 0.43005181 0.39896373]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_ts):\n",
    "    count = [0,0,0]\n",
    "    total = 0\n",
    "    for j in range(y.shape[0]):\n",
    "        for k in range(3):\n",
    "            if y[j,i,k] == 1:\n",
    "                count[k] += 1\n",
    "                total += 1\n",
    "    count = np.array(count)\n",
    "    print(count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13054449 0.56998672 0.29946879]\n"
     ]
    }
   ],
   "source": [
    "count = [0,0,0]\n",
    "total = 0\n",
    "for i in range(num_ts):\n",
    "    for j in range(y.shape[0]):\n",
    "        for k in range(3):\n",
    "            if y[j,i,k] == 1:\n",
    "                count[k] += 1\n",
    "                total += 1\n",
    "count = np.array(count)\n",
    "print(count/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40976380572188953"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_nan = x==0\n",
    "# get the number of nan values\n",
    "total = mask_nan.sum()\n",
    "probability = total / x.size\n",
    "probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((801, 12, 4), (201, 12, 4), (801, 12, 3), (201, 12, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into train and test sets 80/20 with random seed 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = prepare_time_series(torch.Tensor([range(0,12)] * x_train.shape[0]), torch.Tensor(x_train))[1]\n",
    "# y_train = prepare_time_series(torch.Tensor([range(0,12)] * y_train.shape[0]), torch.Tensor(y_train))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = torch.cat([x_train, x_train[:, [-1]]], dim=-2).numpy()\n",
    "# y_train = torch.cat([y_train, y_train[:, [-1]]], dim=-2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.round(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ts = []\n",
    "# y_ts = []\n",
    "\n",
    "# for i in range(num_ts): \n",
    "#     x_temp = np.zeros_like(x_train)\n",
    "#     x_temp[:, :i+1, :] = x_train[:, :i+1, :]\n",
    "#     x_ts.append(x_temp)    \n",
    "#     y_ts.append(y_train)\n",
    "# x_ts = np.concatenate(x_ts, axis=0)\n",
    "# y_ts = np.concatenate(y_ts, axis=0)\n",
    "# x_ts.shape, y_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"temp\n",
    "\"\"\"\n",
    "x_ts = x_train\n",
    "y_ts = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = []\n",
    "for item in x_ts:\n",
    "    x_all.append(item.flatten('F'))\n",
    "x_train = np.array(x_all)\n",
    "y_train = y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_masks(inputs, pm=0.2, pd_m=0.4):\n",
    "    inputs = np.copy(inputs).reshape(-1, 12, 4)\n",
    "    n, T, M = inputs.shape\n",
    "    masked_inputs = inputs.copy()\n",
    "\n",
    "    mask_m1 = np.random.binomial(1, pm, size=(n, T, M))\n",
    "    \n",
    "    t_max = np.random.randint(0, T, size=n)\n",
    "    mask_m2 = np.ones((n, T, M))\n",
    "    for i in range(n):\n",
    "        mask_m2[i, t_max[i]:] = 0\n",
    "    \n",
    "    modality_drop = np.random.binomial(1, pd_m, size=(n, M))\n",
    "    mask_m3 = np.ones((n, T, M))\n",
    "    for m in range(M):\n",
    "        mask_m3[:, :, m] *= modality_drop[:, m].reshape(-1, 1)\n",
    "\n",
    "    final_mask = mask_m1 * mask_m2 * mask_m3\n",
    "    masked_inputs *= final_mask\n",
    "\n",
    "    return masked_inputs, final_mask.reshape(n, -1)\n",
    "\n",
    "pm = 0.2\n",
    "pd_m = 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksper = 256 * 2\n",
    "d = x_train.shape[1]\n",
    "X_class = np.concatenate([x_train]*masksper, 0)\n",
    "Y_class = np.concatenate([y_train]*masksper, 0)\n",
    "# B = np.concatenate(\n",
    "# [np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class.shape[0])],\n",
    "# 1)\n",
    "# B = np.float32(B.T)\n",
    "\n",
    "_, B = apply_masks(X_class, pm, pd_m)\n",
    "    \n",
    "\"\"\"\n",
    "remove for interpolation\n",
    "\"\"\"\n",
    "zero_mask = X_class == 0\n",
    "B[zero_mask] = 0\n",
    "\n",
    "# # remove 0 mask\n",
    "mask_nonzero = np.sum(B, axis=1) != 0\n",
    "B = B[mask_nonzero]\n",
    "X_class = X_class[mask_nonzero]\n",
    "Y_class = Y_class[mask_nonzero]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_nan = X_class == 0\n",
    "# B[mask_nan] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class = np.concatenate((X_class*B, B), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((213901, 96), (213901, 12, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_class.shape, Y_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = []\n",
    "for i in range(B.shape[0]):\n",
    "    b = B[i]\n",
    "    temp = -1\n",
    "    for j in range(4):\n",
    "        part = b[num_ts*j:num_ts*(j+1)]\n",
    "        max_index = np.where(part == 1)[0]\n",
    "        if len(max_index) > 0:\n",
    "            max_index = max_index[-1]\n",
    "            if max_index > temp:\n",
    "                temp = max_index\n",
    "    location.append(temp)\n",
    "    \n",
    "location = np.array(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 16433]\n",
      " [    1 22512]\n",
      " [    2 24842]\n",
      " [    3 30042]\n",
      " [    4 27878]\n",
      " [    5 18594]\n",
      " [    6 21267]\n",
      " [    7  9046]\n",
      " [    8 19186]\n",
      " [    9  5576]\n",
      " [   10 10502]\n",
      " [   11  8023]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(location, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp = []\n",
    "for i in range(Y_class.shape[0]):\n",
    "    y_location = Y_class[i, location[i]]\n",
    "    mask_nan = np.isnan(y_location)\n",
    "    y_location[mask_nan] = 0\n",
    "    y_temp.append(y_location)\n",
    "y_temp = np.array(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove y with nan values\n",
    "mask_zero = np.sum(y_temp, axis=1) != 0\n",
    "X_class = X_class[mask_zero]\n",
    "Y_class = y_temp[mask_zero]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_class = np.argmax(Y_class, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/xgboost/core.py:158: UserWarning: [10:37:46] WARNING: /workspace/src/context.cc:43: No visible GPU is found, setting device to CPU.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/xgboost/core.py:158: UserWarning: [10:37:46] WARNING: /workspace/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;gpu&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=256, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;gpu&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=256, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device='gpu', early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=256, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train classifier\n",
    "est = XGBClassifier(n_estimators=256, device='gpu')\n",
    "est.fit(X_class, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/users/d/d/ddinh/.venv/lib64/python3.11/site-packages/xgboost/core.py:158: UserWarning: [10:39:02] WARNING: /workspace/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# est.save_model('/work/users/d/d/ddinh/aaco/models/adni_different_masking.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2412, 12, 4), (2412, 12, 3))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ts_val = []\n",
    "y_ts_val = []\n",
    "\n",
    "for i in range(num_ts): \n",
    "    x_temp = np.zeros_like(x_val)\n",
    "    x_temp[:, :i+1, :] = x_val[:, :i+1, :]\n",
    "    x_ts_val.append(x_temp)    \n",
    "    y_ts_val.append(y_val)\n",
    "x_ts_val = np.concatenate(x_ts_val, axis=0)\n",
    "y_ts_val = np.concatenate(y_ts_val, axis=0)\n",
    "x_ts_val.shape, y_ts_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_val = []\n",
    "for item in x_ts_val:\n",
    "    x_all_val.append(item.flatten('F'))\n",
    "x_val = np.array(x_all_val)\n",
    "y_val = y_ts_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "masksper = 1\n",
    "d = x_val.shape[1]\n",
    "X_class_val = np.concatenate([x_val]*masksper, 0)\n",
    "Y_class_val = np.concatenate([y_val]*masksper, 0)\n",
    "B_val = np.concatenate(\n",
    "[np.sum(np.random.permutation(np.eye(d))[:, :np.random.randint(d)], 1, keepdims=True) for _ in range(X_class_val.shape[0])],\n",
    "1)\n",
    "B_val = np.float32(B_val.T)\n",
    "\n",
    "# B_val = np.ones_like(B_val)\n",
    "\n",
    "zero_mask_val = X_class_val == 0\n",
    "B_val[zero_mask_val] = 0\n",
    "\n",
    "# remove 0 mask\n",
    "mask_nonzero_val = np.sum(B_val, axis=1) != 0\n",
    "B_val = B_val[mask_nonzero_val]\n",
    "X_class_val = X_class_val[mask_nonzero_val]\n",
    "Y_class_val = Y_class_val[mask_nonzero_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_val = []\n",
    "for i in range(B_val.shape[0]):\n",
    "    b = B_val[i]\n",
    "    temp = -1\n",
    "    for j in range(4):\n",
    "        part = b[num_ts*j:num_ts*(j+1)]\n",
    "        max_index = np.where(part == 1)[0]\n",
    "        if len(max_index) > 0:\n",
    "            max_index = max_index[-1]\n",
    "            if max_index > temp:\n",
    "                temp = max_index\n",
    "    location_val.append(temp)\n",
    "    \n",
    "location_val = np.array(location_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_temp_val = []\n",
    "for i in range(Y_class_val.shape[0]):\n",
    "    y_location_val = Y_class_val[i, location_val[i]]\n",
    "    mask_nan_val = np.isnan(y_location_val)\n",
    "    y_location_val[mask_nan_val] = 0\n",
    "    y_temp_val.append(y_location_val)\n",
    "y_temp_val = np.array(y_temp_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove y with nan values for validation set\n",
    "mask_zero_val = np.sum(y_temp_val, axis=1) != 0\n",
    "X_class_val = X_class_val[mask_zero_val]\n",
    "Y_class_val = y_temp_val[mask_zero_val]\n",
    "B_val = B_val[mask_zero_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_zero_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class_val = np.concatenate((X_class_val*B_val, B_val), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = est.predict_proba(X_class_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5409617097061442\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.argmax(val_preds, 1)==np.argmax(Y_class_val, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5109105973299515\n",
      "0.7030657167468642\n"
     ]
    }
   ],
   "source": [
    "# this is temoprary evaluation, since it's not really correct.\n",
    "# it should be evaluated on the accumulated predictions, not the individual predictions like this rolling out \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "pr_auc_macro = average_precision_score(Y_class_val, val_preds, average=\"macro\")\n",
    "print(pr_auc_macro)\n",
    "\n",
    "roc_auc_macro = roc_auc_score(Y_class_val, val_preds, average=\"macro\", multi_class=\"ovr\")\n",
    "print(roc_auc_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.037846"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(B_val.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "# 0.5057228523811975\n",
    "# 0.6798018971723648\n",
    "\n",
    "# masking from a2mt paper\n",
    "# 0.5109105973299515\n",
    "# 0.7030657167468642"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
